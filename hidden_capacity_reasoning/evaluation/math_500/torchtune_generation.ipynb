{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34445de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/torchtune/stable/generated/torchtune.models.qwen2_5.qwen2_5_1_5b_instruct.html#torchtune.models.qwen2_5.qwen2_5_1_5b_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24147bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/torchtune/0.6/deep_dives/checkpointer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738efe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: None\n",
      ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 22.0MB/s]\n",
      "LICENSE: 100%|█████████████████████████████| 1.06k/1.06k [00:00<00:00, 5.47MB/s]\n",
      "README.md: 100%|███████████████████████████| 16.0k/16.0k [00:00<00:00, 4.43MB/s]\n",
      "benchmark.jpg: 100%|█████████████████████████| 777k/777k [00:00<00:00, 3.90MB/s]\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/code/models/DeepSeek-R1-Distill-Qwen-1.5B/generation_config.json\n",
      "/code/models/DeepSeek-R1-Distill-Qwen-1.5B/README.md\n",
      "/code/models/DeepSeek-R1-Distill-Qwen-1.5B/config.json\n",
      "/code/models/DeepSeek-R1-Distill-Qwen-1.5B/model.safetensors\n",
      "/code/models/DeepSeek-R1-Distill-Qwen-1.5B/tokenizer_config.json\n",
      "/code/models/DeepSeek-R1-Distill-Qwen-1.5B/.gitattributes\n",
      "/code/models/DeepSeek-R1-Distill-Qwen-1.5B/.cache\n",
      "/code/models/DeepSeek-R1-Distill-Qwen-1.5B/original_repo_id.json\n",
      "/code/models/DeepSeek-R1-Distill-Qwen-1.5B/LICENSE\n",
      "/code/models/DeepSeek-R1-Distill-Qwen-1.5B/tokenizer.json\n",
      "/code/models/DeepSeek-R1-Distill-Qwen-1.5B/figures\n"
     ]
    }
   ],
   "source": [
    "!tune download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --output-dir models/DeepSeek-R1-Distill-Qwen-1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61a3ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERT QWEN2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtune.training import FullModelHFCheckpointer, ModelType\n",
    "\n",
    "checkpoint_dir = \"models/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "output_dir = \"models/DeepSeek-R1-Distill-Qwen-1.5B-torchtune\"\n",
    "pytorch_files = [\n",
    "    \"model.safetensors\",\n",
    "]\n",
    "\n",
    "# Set up the checkpointer and load state dict\n",
    "checkpointer = FullModelHFCheckpointer(\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    checkpoint_files=pytorch_files,\n",
    "    output_dir=output_dir,\n",
    "    model_type=ModelType.QWEN2,\n",
    ")\n",
    "torchtune_sd = checkpointer.load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb1fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.models.qwen2 import qwen2_1_5b, qwen2\n",
    "\n",
    "# model = qwen2_1_5b()\n",
    "model = qwen2(\n",
    "    vocab_size=151936,\n",
    "    num_layers=28,\n",
    "    num_heads=12,\n",
    "    num_kv_heads=2,\n",
    "    embed_dim=1536,\n",
    "    intermediate_dim=8960,\n",
    "    max_seq_len=32768,\n",
    "    attn_dropout=0.0,\n",
    "    norm_eps=1e-06,\n",
    "    rope_base=1000000.0,\n",
    "    # tie_word_embeddings=True,\n",
    "    tie_word_embeddings=False,\n",
    ")\n",
    "model.load_state_dict(\n",
    "    torchtune_sd[\"model\"],\n",
    "    # strict=False,\n",
    ")\n",
    "model = model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b8003e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (tok_embeddings): Embedding(151936, 1536)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x TransformerSelfAttentionLayer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (output_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        (pos_embeddings): Qwen2RotaryPositionalEmbeddings()\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (w1): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "        (w2): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "        (w3): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (mlp_norm): RMSNorm()\n",
       "      (sa_scale): Identity()\n",
       "      (mlp_scale): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a65612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.generation import generate, generate_next_token as sample_next\n",
    "from torchtune import generation\n",
    "\n",
    "# output, logits = generate(\n",
    "#     model, torch.tensor(prompt), max_generated_tokens=100, pad_id=0\n",
    "# )\n",
    "\n",
    "\n",
    "def generate_next_token(\n",
    "    model,\n",
    "    input_pos,\n",
    "    x,\n",
    "    q=None,\n",
    "    *,\n",
    "    mask=None,\n",
    "    temperature: float = 1.0,\n",
    "    top_k=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates the next tokens given a prompt, and also returns the corresponding logits.\n",
    "\n",
    "    Args:\n",
    "        model (TransformerDecoder): model used for generation\n",
    "        input_pos (torch.Tensor): tensor with the positional encodings associated with the given prompt,\n",
    "            with shape [bsz x seq_length].\n",
    "        x (torch.Tensor): tensor with the token IDs associated with the given prompt,\n",
    "            with shape [bsz x seq_length].\n",
    "        q (Optional[torch.Tensor]): randomly sampled tensor for softmax sampling trick.\n",
    "            See https://github.com/pytorch-labs/gpt-fast/blob/32971d3129541c5bfb4f715abc33d1c5f408d204/generate.py#L40\n",
    "        mask (Optional[torch.Tensor]): attention mask with shape [bsz x seq_length x seq_length],\n",
    "            default None.\n",
    "        temperature (float): value to scale the predicted logits by, default 1.0.\n",
    "        top_k (Optional[int]): Top-k value to use for sampling, default None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: tuple of two tensors:\n",
    "            - tokens (torch.Tensor): tensor with the generated tokens,\n",
    "                with shape [bsz x 1].\n",
    "            - logits (torch.Tensor): tensor with the logits associated with the generated tokens,\n",
    "                with shape [bsz x 1 x vocab_size].\n",
    "\n",
    "    \"\"\"\n",
    "    # model produces logits in [bsz, seq_length, vocab_size]\n",
    "    # we want to take the last token's logits as the input to the next model call\n",
    "    if temperature != 0:\n",
    "        return sample_next(\n",
    "            model,\n",
    "            input_pos,\n",
    "            x,\n",
    "            q,\n",
    "            mask,\n",
    "            temperature,\n",
    "            top_k,\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        logits = model(x, input_pos=input_pos, mask=mask)[:, -1]\n",
    "        # logits = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        return (\n",
    "            torch.argmax(logits, dim=-1, keepdim=True).to(dtype=torch.int),\n",
    "            logits.unsqueeze(1),\n",
    "        )\n",
    "\n",
    "\n",
    "generation.generate_next_token = generate_next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b7c047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77, 73, 0.948051948051948)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    # \"dim/hendrycks_math_train_12k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096\"\n",
    "    \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    ")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    # test_size=250,\n",
    "    # test_size=1,\n",
    "    seed=42,\n",
    ")\n",
    "dataset = dataset[\"test\"].filter(lambda x: x[\"model_answer\"].count(\"</think>\") == 1)\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "\n",
    "correct_dataset = []\n",
    "\n",
    "for pos, item in enumerate(dataset):\n",
    "    try:\n",
    "        answer = dataset_answer_filter(item[\"answer\"])\n",
    "        model_answer = model_answer_filter(item[\"model_answer\"])\n",
    "        # print(answer, model_answer)\n",
    "        # break\n",
    "        if is_equiv(answer, model_answer):\n",
    "            correct_dataset.append(item)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "len(dataset), len(correct_dataset), len(correct_dataset) / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0cc9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "264542e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fa747b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532e561d671c4c7a80cdb0807a094b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 42\u001b[0m\n\u001b[1;32m     28\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     29\u001b[0m     batch,\n\u001b[1;32m     30\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# generated_ids = model.generate(\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#     **model_inputs,\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     max_new_tokens=4096,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#     top_p=None,\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m generated_ids, logits \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_generated_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_generated_tokens=120,\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids) :]\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_inputs\u001b[38;5;241m.\u001b[39minput_ids, generated_ids)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m responses \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtune/generation/_generation.py:370\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, prompt, max_generated_tokens, pad_id, temperature, top_k, stop_tokens, rng, custom_generate_next_token)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rng \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    368\u001b[0m         (bsz, model\u001b[38;5;241m.\u001b[39mtok_embeddings\u001b[38;5;241m.\u001b[39mnum_embeddings), device\u001b[38;5;241m=\u001b[39mprompt\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    369\u001b[0m     )\u001b[38;5;241m.\u001b[39mexponential_(\u001b[38;5;241m1\u001b[39m, generator\u001b[38;5;241m=\u001b[39mrng)\n\u001b[0;32m--> 370\u001b[0m tokens, logits \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_generate_next_token\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_input_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([generated_tokens, tokens], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    380\u001b[0m generated_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([generated_logits, logits], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtune/generation/_generation.py:102\u001b[0m, in \u001b[0;36mgenerate_next_token\u001b[0;34m(model, input_pos, x, q, mask, temperature, top_k)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mGenerates the next tokens given a prompt, and also returns the corresponding logits.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# model produces logits in [bsz, seq_length, vocab_size]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# we want to take the last token's logits as the input to the next model call\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    104\u001b[0m     sample(logits\u001b[38;5;241m.\u001b[39mclone(), temperature\u001b[38;5;241m=\u001b[39mtemperature, top_k\u001b[38;5;241m=\u001b[39mtop_k, q\u001b[38;5;241m=\u001b[39mq),\n\u001b[1;32m    105\u001b[0m     logits\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    106\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtune/modules/transformer.py:635\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tokens, mask, encoder_input, encoder_mask, input_pos)\u001b[0m\n\u001b[1;32m    633\u001b[0m         hidden\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;66;03m# shape: [b, s, d]\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# shape: [b, seq_len, out_dim]\u001b[39;00m\n\u001b[1;32m    644\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munembed(h)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtune/modules/transformer.py:128\u001b[0m, in \u001b[0;36mTransformerSelfAttentionLayer.forward\u001b[0;34m(self, x, mask, input_pos, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_scale(attn_out) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Norm applied before the feedforward layer\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Residual connection; shape: [batch_size, seq_length, embed_dim]\u001b[39;00m\n\u001b[1;32m    131\u001b[0m out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_scale(mlp_out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtune/modules/feed_forward.py:53\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw3 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw3(x)\n\u001b[0;32m---> 53\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from more_itertools import chunked\n",
    "\n",
    "batch_size = 1\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "batches = []\n",
    "for batch in chunked(correct_dataset, batch_size):\n",
    "    batch = [item[\"problem\"] for item in batch]\n",
    "    batch = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": base_prompt.format(question=item)},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        for item in batch\n",
    "    ]\n",
    "    batches.append(batch)\n",
    "\n",
    "    generation_results = []\n",
    "\n",
    "    device = \"cuda\"\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(batches):\n",
    "        model_inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            truncation=False,\n",
    "            add_special_tokens=False,\n",
    "        ).to(device)\n",
    "        # generated_ids = model.generate(\n",
    "        #     **model_inputs,\n",
    "        #     max_new_tokens=4096,\n",
    "        #     do_sample=False,\n",
    "        #     temperature=None,\n",
    "        #     top_p=None,\n",
    "        # )\n",
    "        generated_ids, logits = generate(\n",
    "            model,\n",
    "            model_inputs[\"input_ids\"],\n",
    "            max_generated_tokens=4096,\n",
    "            # max_generated_tokens=120,\n",
    "            pad_id=tokenizer.eos_token_id,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids) :]\n",
    "            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        generation_results.extend(responses)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fec64989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this magic square problem here, and I need to find the value of n. Let me try to figure it out step by step. First, I should probably visualize the square based on the Asymptote code provided. It's a 3x3 grid, right? So, it's a 3x3 grid with some numbers filled in and some variables. Let me try to sketch it out in my mind. The Asymptote code draws a 3x3 grid with some numbers and expressions. Let me see: the center is labeled n-\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809720de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf sdpa batch 2 float32\n",
    "# \"Okay, so I have this problem here where Rick is thinking of a positive factor of 14, and Steve is thinking of a positive factor of 42. They both are thinking of the same number, and I need to figure out how many possible numbers they could be thinking of. Hmm, let me break this down step by step.\\n\\nFirst, let me recall what a factor is. A factor of a number is another number that divides into it without leaving a remainder. So, for example, the factors of 14 are numbers that can multiply together to give 14. Similarly, factors of 42 are numbers that can multiply together to give 42.\\n\\nAlright, so let me list out all the positive factors of 14 and 42. That should help me see which numbers are common to both lists.\\n\\nStarting with 14. The factors of 14 are the numbers that divide 14 exactly. Let's see:\\n\\n1 × 14 = 14, so 1 and 14 are factors.\\n\\n2 × 7 = 14, so 2 and 7 are factors.\\n\\nIs there any more? 3 doesn't divide 14 evenly because 14 divided by 3 is about 4.666, which isn't a whole number. Similarly, 4 doesn't divide 14 evenly because 14 divided by 4 is 3.5, which isn't a whole number. 5 doesn't divide 14 either, and 6 is already covered by 2 and 3. So, the factors of 14 are 1, 2, 7, and 14.\\n\\nNow, moving on to 42. Let's list all the factors of 42.\\n\\n1 × 42 = 42, so 1 and 42 are factors.\\n\\n2 × 21 = 42, so 2 and 21 are factors.\\n\\n3 × 14 = 42, so 3 and 14 are factors.\\n\\n6 × 7 = 42, so 6 and 7 are factors.\\n\\nWait, let me make sure I haven't missed any. After 1, 2, 3, 6, 7, 14, 21, 42. Yeah, that seems right. So the factors of 42 are 1, 2, 3, 6, 7, 14, 21, and 42.\\n\\nNow, the problem says that Rick is thinking of a factor of 14, and Steve is thinking of a factor of 42, and they are thinking of the same number. So, we need to find the numbers that are common to both lists of factors.\\n\\nLooking at the factors of 14: 1, 2, 7, 14.\\n\\nLooking at the factors of 42: 1, 2, 3, 6, 7, 14, 21, 42.\\n\\nSo, the common numbers between these two lists are 1, 2, 7, and 14. Let me count them: 1, 2, 7, 14. That's four numbers.\\n\\nTherefore, there are four possible numbers that Rick and Steve could be thinking of.\\n\\nWait, let me double-check to make sure I didn't miss any. For 14, the factors are definitely 1, 2, 7, 14. For 42, the factors are 1, 2, 3, 6, 7, 14, 21, 42. So, the intersection is indeed 1, 2, 7, 14. So, four numbers.\\n\\nIs there a possibility that I missed any factors? Let me think. For 14, 1, 2, 7, 14. For 42, 1, 2, 3, 6, 7, 14, 21, 42. So, yeah, the overlapping numbers are 1, 2, 7, 14. So, four numbers.\\n\\nTherefore, the number of possible numbers they could be thinking of is 4.\\n\\n**Final Answer**\\nThe number of possible numbers they could be thinking of is \\\\boxed{4}.\\n</think>\\n\\nRick is thinking of a positive factor of 14, and Steve is thinking of a positive factor of 42. We need to determine how many possible numbers they could be thinking of if they are the same.\\n\\nFirst, we list the factors of 14:\\n- The factors of 14 are 1, 2, 7, and 14.\\n\\nNext, we list the factors of 42:\\n- The factors of 42 are 1, 2, 3, 6, 7, 14, 21, and 42.\\n\\nWe then find the common factors of both lists:\\n- The common factors of 14 and 42 are 1, 2, 7, and 14.\\n\\nThus, there are four possible numbers that Rick and Steve could be thinking of.\\n\\nThe number of possible numbers they could be thinking of is \\\\boxed{4}.\"\n",
    "\n",
    "\n",
    "# torchtune\n",
    "# \"Okay, so I have this magic square problem here, and I need to find the value of n. Let me try to figure it out step by step. First, I should probably visualize the square based on the Asymptote code provided. It's a 3x3 grid, right? So, it's a 3x3 grid with some numbers filled in and some variables. Let me try to sketch it out in my mind. The Asymptote code draws a 3x3 grid with some numbers and expressions. Let me see: the center is labeled n-\"\n",
    "\n",
    "# \"Okay, so I have this magic square problem here, and I need to find the value of n. Let me try to figure it out step by step. First, I should probably visualize the square based on the Asymptote code provided. It's a 3x3 grid, right? So, it's a 3x3 grid with some numbers filled in and some variables. Let me try to sketch it out in my mind. The Asymptote code draws a 3x3 grid with some numbers and expressions. Let me see: the center is labeled n-\"\n",
    "\n",
    "# \"Okay, so I have this magic square problem here, and I need to find the value of n. Let me try to figure it out step by step. First, I should probably visualize the square based on the Asymptote code provided. It's a 3x3 grid, right? So, it's a 3x3 grid with some numbers filled in and some variables. Let me try to sketch it out in my mind. The Asymptote code draws a 3x3 grid with some numbers and expressions. Let me see: the center is labeled n-\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
