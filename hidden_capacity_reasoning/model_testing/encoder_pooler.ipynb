{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "953c0eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06318e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Okay, so I need to come up with a short introduction to a large language model. Hmm, where do I start? Well, I know that LLMs are these big machines that can understand and generate human language. They\\'re really good at processing information and generating responses. I think they\\'re used in various fields like AI, chatbots, and even creative industries. But I\\'m not exactly sure about all the specifics. Let me think about how to structure this.\\n\\nFirst, I should probably define what an LLM is. Maybe start with something like \"Large Language Models (LLMs)\" or \"Language Models.\" Then, mention that they are designed to understand and generate human language. That makes sense. But I should also highlight their capabilities and applications. Maybe something about their ability to handle diverse languages and contexts.\\n\\nI remember hearing that LLMs can act as chatbots, so that\\'s a good point. They can respond to questions and provide information. Also, they can generate creative content, which is interesting. I should include that. Maybe touch on their versatilityâ€”whether used in education, business, or entertainment.\\n\\nI wonder if there are any limitations. For example, are they good at understanding all languages, or are there some languages they struggle with? Also, how do they handle different types of inputs, like text, images, or even code? Maybe that\\'s a point to include as well.\\n\\nWait, I should make sure the introduction flows well. Maybe start with a definition, then talk about their capabilities, applications, and limitations. That way, it\\'s concise and covers the main aspects.\\n\\nLet me try to put this together. Start with \"Large Language Models (LLMs)\" and explain they\\'re designed to understand and generate human language. Then mention their applications in various fields like education, business, entertainment, and creative industries. Include that they can act as chatbots and generate responses and creative content. Also, note that they can handle diverse languages and contexts, but have limitations in specific areas.\\n\\nI think that\\'s a solid structure. Now, I\\'ll write it out in a clear and concise manner, making sure each sentence adds value and flows smoothly into the next.\\n</think>\\n\\nLarge Language Models (LLMs) are advanced artificial intelligence systems designed to understand and generate human language. These models are versatile, capable of handling a wide range of tasks including answering questions, providing information, and generating creative content. They are particularly useful in fields such as education, business, entertainment, and creative industries, where they can assist in'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model..1\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87598600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 23, 1536]), torch.bfloat16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    **model_inputs,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.hidden_states[-1].shape, model_output_1.hidden_states[-1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a732ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1536])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Qwen2ModelEmbedPooler(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.lm_head = None\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds):\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )[0]\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.sum(1) / torch.tensor(\n",
    "            input_embeds.shape[1],\n",
    "            device=input_embeds.device,\n",
    "            dtype=input_embeds.dtype,\n",
    "        )\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.unsqueeze(1)\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "embed_pooler = Qwen2ModelEmbedPooler.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", device_map={\"\": 0}\n",
    ")\n",
    "result = embed_pooler(\n",
    "    # model_output_1.hidden_states[-1],\n",
    "    torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    ")\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "model = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "model_pooler = RobertaModel(\n",
    "    RobertaConfig(\n",
    "        # num_hidden_layers=6,\n",
    "        hidden_size=1536,\n",
    "    )\n",
    ")\n",
    "model_pooler = model_pooler.to(\"cuda\")\n",
    "# RobertaConfig(\n",
    "#     num_hidden_layers=6,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4032b2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307055616"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "count_parameters(model_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b530fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.7319, -0.3607,  1.3480,  ..., -0.7462, -1.3530,  1.6311],\n",
       "         [-0.9783,  0.4005,  1.1449,  ..., -0.8091, -1.1307,  0.1278],\n",
       "         [-1.6309,  0.9927,  1.3285,  ..., -0.9963, -1.0119,  0.0880],\n",
       "         ...,\n",
       "         [-0.0877, -0.7833,  0.2703,  ..., -1.1880, -1.4520,  0.4867],\n",
       "         [-1.9345, -0.4809,  0.7215,  ..., -1.7561, -1.0042, -0.3902],\n",
       "         [-2.1255, -0.8651,  0.4842,  ..., -1.5525, -1.3648,  0.6962]],\n",
       "\n",
       "        [[-0.9212,  0.5121,  1.8180,  ..., -1.5539,  0.2467,  0.9719],\n",
       "         [-1.5249, -0.0152,  0.5992,  ..., -0.8235, -1.0264,  0.4108],\n",
       "         [-1.0407, -1.4339,  1.3941,  ..., -1.1372,  0.2327,  0.9719],\n",
       "         ...,\n",
       "         [-0.6508, -0.5177,  1.5293,  ..., -1.0896,  0.0824,  1.1235],\n",
       "         [-1.1410, -0.8605,  1.5728,  ..., -1.2504, -0.7460,  0.3516],\n",
       "         [-2.0835, -0.4642,  1.4805,  ..., -1.0204, -1.0545,  0.6524]]],\n",
       "       device='cuda:0'), pooler_output=tensor([[-0.4771,  0.7896, -0.9626,  ..., -0.9099, -0.0183, -0.6381],\n",
       "        [ 0.0185,  0.9661, -0.2579,  ..., -0.9888,  0.3235,  0.1862]],\n",
       "       device='cuda:0'), hidden_states=(tensor([[[ 0.9573, -1.2190,  0.9172,  ...,  0.1121,  1.1610,  0.5136],\n",
       "         [ 0.9483, -0.0000,  0.9000,  ...,  0.1140,  1.1648,  0.5274],\n",
       "         [-0.2510, -0.1850,  0.4608,  ..., -0.5592,  0.7559, -0.0625],\n",
       "         ...,\n",
       "         [ 1.6727, -1.3529,  0.0037,  ..., -0.1614, -0.2289,  0.3240],\n",
       "         [ 0.6403, -2.1075,  1.1530,  ...,  0.4067,  0.8833, -0.0000],\n",
       "         [-0.1490, -0.2068, -0.2333,  ..., -0.0102, -0.0000,  0.7064]],\n",
       "\n",
       "        [[ 0.9573, -1.2190,  0.9172,  ...,  0.1121,  1.1610,  0.5136],\n",
       "         [ 0.9483, -1.2311,  0.9000,  ...,  0.1140,  1.1648,  0.5274],\n",
       "         [-0.2510, -0.1850,  0.4608,  ..., -0.0000,  0.7559, -0.0625],\n",
       "         ...,\n",
       "         [ 1.6727, -1.3529,  0.0037,  ..., -0.1614, -0.2289,  0.3240],\n",
       "         [ 0.6403, -2.1075,  0.0000,  ...,  0.4067,  0.8833, -0.0000],\n",
       "         [-0.1490, -0.2068, -0.0000,  ..., -0.0000, -0.6028,  0.7064]]],\n",
       "       device='cuda:0'), tensor([[[-0.4841, -0.4378,  1.1071,  ..., -0.3365,  2.1510,  0.2248],\n",
       "         [-0.1080,  0.3208,  0.8172,  ..., -0.5734,  1.9933,  0.2340],\n",
       "         [-0.4315,  0.5389,  0.1721,  ..., -1.7127,  0.2019, -0.6716],\n",
       "         ...,\n",
       "         [ 0.7093, -0.3437, -0.2138,  ..., -0.5026, -0.3676,  0.5353],\n",
       "         [ 0.5051, -2.0401,  0.9057,  ..., -0.1921,  0.6239, -0.6794],\n",
       "         [-1.6584,  0.4418, -0.2425,  ..., -1.2903,  0.4914,  0.4851]],\n",
       "\n",
       "        [[ 0.6000, -0.5587,  0.5856,  ..., -0.4587,  1.9306,  0.1405],\n",
       "         [-0.3366, -0.8195,  0.4622,  ..., -0.6692,  2.0630, -0.0711],\n",
       "         [-0.2972,  0.7595,  0.5125,  ..., -1.1325,  0.4038, -0.6376],\n",
       "         ...,\n",
       "         [ 0.6041, -0.3102,  0.6410,  ..., -0.3605, -0.4732,  0.1635],\n",
       "         [ 0.2385, -1.5810,  0.1089,  ..., -0.4549,  0.5656, -0.7484],\n",
       "         [-1.5332,  0.5122, -0.1214,  ..., -0.3777, -0.2541,  0.4811]]],\n",
       "       device='cuda:0'), tensor([[[-0.0813, -1.0008,  0.9705,  ..., -0.1809,  1.9859,  0.2320],\n",
       "         [ 0.1996, -0.4090,  0.7325,  ..., -0.1045,  1.7613,  0.6462],\n",
       "         [-0.2393,  0.2173,  0.5104,  ..., -1.4285,  0.2659,  0.3250],\n",
       "         ...,\n",
       "         [ 0.3547, -0.2487, -0.0274,  ..., -0.7335, -0.5398,  1.0179],\n",
       "         [ 0.6404, -0.9116,  1.2448,  ..., -0.7278,  0.1551,  0.6294],\n",
       "         [-1.0319,  0.4157, -0.1975,  ..., -0.8836, -0.0165,  0.7645]],\n",
       "\n",
       "        [[ 0.4870, -1.1489,  0.4582,  ..., -0.4201,  1.1951,  0.5719],\n",
       "         [ 0.1308, -1.3182,  0.8646,  ..., -0.3660,  1.2455,  0.0493],\n",
       "         [-0.5936,  1.0214,  0.9606,  ..., -1.0027,  0.0525,  0.2644],\n",
       "         ...,\n",
       "         [ 0.3626, -0.0607,  0.9306,  ..., -0.6940, -0.9070,  0.7022],\n",
       "         [ 0.6835, -0.8978,  0.4995,  ..., -0.6281,  0.0077,  0.2917],\n",
       "         [-1.4119,  0.8143, -0.2307,  ...,  0.0170, -0.3434,  0.4024]]],\n",
       "       device='cuda:0'), tensor([[[-0.9418, -0.1208,  0.0328,  ...,  0.3619,  1.3324, -0.7526],\n",
       "         [-0.3990,  0.0872, -0.1786,  ...,  0.2824,  1.2271, -0.0697],\n",
       "         [-1.1416,  0.6242,  0.3216,  ..., -0.2808, -0.4757, -1.4343],\n",
       "         ...,\n",
       "         [-0.0776,  0.1276, -0.4480,  ..., -0.4445, -0.4069,  0.0056],\n",
       "         [-0.2412, -0.4597,  1.0753,  ..., -0.7935, -0.0872, -0.0351],\n",
       "         [-1.8740,  0.8855, -0.6041,  ..., -0.9927, -0.2330, -0.2156]],\n",
       "\n",
       "        [[-0.5994, -0.4456, -0.5645,  ...,  0.4070,  0.9625, -0.4753],\n",
       "         [-0.9418, -0.4845,  0.6300,  ...,  0.4163,  1.0811, -1.4836],\n",
       "         [-1.1809,  1.5703,  0.7494,  ...,  0.2118, -0.3286, -1.2881],\n",
       "         ...,\n",
       "         [-0.7875,  0.2005,  0.4244,  ..., -0.4967, -0.8370, -0.3376],\n",
       "         [ 0.4135, -0.9627,  0.0037,  ..., -0.2862, -0.5444, -0.4097],\n",
       "         [-2.2924,  0.8568, -0.1683,  ...,  0.1757,  0.1222, -0.7922]]],\n",
       "       device='cuda:0'), tensor([[[-0.0148,  0.2425,  0.1185,  ..., -0.1386,  2.0096,  0.5851],\n",
       "         [ 0.2161,  0.7802, -0.1902,  ...,  0.0339,  1.8333,  0.0041],\n",
       "         [ 0.0971,  0.6155,  0.2853,  ...,  0.1555,  0.2755, -0.5310],\n",
       "         ...,\n",
       "         [-0.3414,  0.1607, -0.2194,  ..., -0.5674,  0.1217, -0.1082],\n",
       "         [ 0.3143, -0.2282,  0.8036,  ..., -0.4537,  0.7910,  0.9517],\n",
       "         [-0.5444,  0.6876, -0.0663,  ..., -1.0875,  0.3497, -0.6525]],\n",
       "\n",
       "        [[ 0.2270, -0.2461, -0.4297,  ..., -0.3856,  1.7382,  0.2778],\n",
       "         [ 0.2505,  0.1103,  0.3684,  ...,  0.0554,  2.1770, -0.0590],\n",
       "         [-0.0601,  1.3214,  0.1482,  ..., -0.0758,  0.6426,  0.6860],\n",
       "         ...,\n",
       "         [ 0.2931,  0.1285,  0.5010,  ..., -0.6269,  0.0506,  0.3341],\n",
       "         [ 0.4622, -0.6346, -0.2778,  ..., -0.1850,  0.4016,  0.3929],\n",
       "         [-2.1299,  0.9289, -0.1256,  ..., -0.1596,  0.7607, -0.0200]]],\n",
       "       device='cuda:0'), tensor([[[-1.2326,  0.3326, -0.5365,  ..., -0.0604,  1.4507,  0.4813],\n",
       "         [-0.8463,  0.3572, -0.2340,  ..., -0.3862,  1.1499, -0.2062],\n",
       "         [-0.7245,  0.6410,  0.3845,  ..., -0.2603,  0.0606, -1.0208],\n",
       "         ...,\n",
       "         [-0.7443, -0.0798,  0.1579,  ..., -0.8341, -0.0071, -0.5719],\n",
       "         [ 0.0472, -0.2762,  0.5163,  ..., -0.8041,  0.1353,  0.3828],\n",
       "         [-1.0449,  0.6850,  0.1168,  ..., -1.2660,  1.0523, -0.7299]],\n",
       "\n",
       "        [[-1.0246, -0.1301, -0.4056,  ..., -0.8398,  1.7411, -0.1714],\n",
       "         [-1.3433, -0.2720,  0.5121,  ..., -0.3298,  1.7109, -0.4100],\n",
       "         [-1.3187,  1.1919,  0.3778,  ...,  0.3015,  0.3708,  0.5487],\n",
       "         ...,\n",
       "         [-0.5625,  0.2825,  0.4455,  ..., -0.9366,  0.1601,  0.4692],\n",
       "         [-0.3659, -0.4803,  0.1987,  ...,  0.2613, -0.1144,  0.5429],\n",
       "         [-2.1798,  0.9076, -0.1733,  ..., -0.4285,  0.9024, -0.3081]]],\n",
       "       device='cuda:0'), tensor([[[-1.1605,  0.6340,  0.2042,  ...,  0.7516,  0.5523,  0.4172],\n",
       "         [-0.8496,  1.4102,  1.0525,  ...,  0.8519, -0.1413, -0.1541],\n",
       "         [-0.8367,  1.0824,  1.6970,  ...,  0.8115, -0.7009, -1.0675],\n",
       "         ...,\n",
       "         [-1.2615,  0.4588,  0.8889,  ..., -0.2092,  0.1090, -1.1226],\n",
       "         [-0.6722,  0.1296,  1.2377,  ..., -0.5772, -0.7632,  0.3933],\n",
       "         [-0.5353,  0.8705,  0.9212,  ..., -0.5567,  0.3841, -0.6626]],\n",
       "\n",
       "        [[-1.2504,  0.7145,  0.7209,  ...,  0.4796,  0.8246, -0.4316],\n",
       "         [-0.8913,  0.8714,  1.8667,  ...,  0.6064,  0.4652, -0.4352],\n",
       "         [-1.3194,  1.4792,  1.6494,  ...,  1.2148, -0.6197, -0.0120],\n",
       "         ...,\n",
       "         [-0.8457,  0.6293,  1.4826,  ..., -0.3998, -0.1356,  0.3194],\n",
       "         [-0.7811,  0.7162,  1.8387,  ...,  0.8236, -0.5678,  0.6231],\n",
       "         [-1.7892,  1.8864,  0.4951,  ...,  0.5816,  0.6080, -0.5469]]],\n",
       "       device='cuda:0'), tensor([[[-0.9160,  1.1051,  0.4584,  ...,  0.2508,  0.4200,  0.6308],\n",
       "         [-0.1725,  1.1036,  0.5149,  ...,  0.9045, -0.2233,  0.1955],\n",
       "         [-1.0856,  1.5706,  1.1109,  ...,  0.5106, -0.5635, -0.3906],\n",
       "         ...,\n",
       "         [-0.7280,  0.9508,  0.0758,  ..., -0.2146, -0.3712, -0.6124],\n",
       "         [-1.4059,  0.7156,  0.8345,  ..., -1.0118, -0.2341,  0.7368],\n",
       "         [-0.6546,  1.4451,  0.4741,  ..., -0.4282,  0.3076, -0.1613]],\n",
       "\n",
       "        [[-1.4939,  1.0809,  0.4392,  ..., -0.0367,  0.6309,  0.2111],\n",
       "         [-1.3384,  1.5130,  1.6704,  ...,  0.1962,  0.3183,  0.1256],\n",
       "         [-0.8195,  0.9777,  1.1372,  ...,  0.2464, -0.3194,  0.1799],\n",
       "         ...,\n",
       "         [-0.6840,  0.7105,  1.0870,  ..., -1.0183, -0.9584,  0.8036],\n",
       "         [-1.4832,  0.6728,  1.5764,  ...,  0.1251, -0.9637,  0.9083],\n",
       "         [-1.8904,  2.0092,  0.2558,  ...,  0.2057,  0.3166, -0.4191]]],\n",
       "       device='cuda:0'), tensor([[[-1.2489,  1.4853, -0.0170,  ...,  0.1389,  0.8166,  0.8585],\n",
       "         [-0.2429,  0.8723, -0.2258,  ...,  0.8686,  0.8126,  0.1961],\n",
       "         [-1.7689,  1.0815,  0.8779,  ..., -0.5305,  0.2339, -0.2755],\n",
       "         ...,\n",
       "         [-0.5775,  0.2792, -0.3768,  ..., -0.1696,  0.2333, -0.5203],\n",
       "         [-1.4547,  0.4284,  0.2340,  ..., -0.4419,  0.4927,  0.8864],\n",
       "         [-0.8421,  0.9109, -0.2092,  ..., -0.1758,  0.9135, -0.4740]],\n",
       "\n",
       "        [[-1.1482,  0.9945,  0.3487,  ..., -0.8117,  1.6159,  0.7352],\n",
       "         [-1.3463,  2.2067,  1.0986,  ..., -0.5326,  0.8113,  0.4523],\n",
       "         [-1.2120,  0.6248,  0.7232,  ..., -0.3706,  0.5788,  0.7347],\n",
       "         ...,\n",
       "         [-0.5221,  0.2924,  0.5575,  ..., -1.2392,  0.0102,  0.6976],\n",
       "         [-1.2267,  0.0778,  1.6003,  ..., -0.1314,  0.2479,  1.2558],\n",
       "         [-1.5441,  1.2759, -0.1164,  ...,  0.2891,  1.3848,  0.4370]]],\n",
       "       device='cuda:0'), tensor([[[-1.3238e+00,  1.5932e+00,  7.8748e-01,  ..., -3.5445e-01,\n",
       "           3.4183e-01,  8.6216e-01],\n",
       "         [-2.5180e-01,  8.6788e-01,  5.1634e-01,  ...,  3.2082e-01,\n",
       "           1.4445e-01, -1.8339e-01],\n",
       "         [-1.2370e+00,  1.4013e+00,  9.1827e-01,  ..., -6.5410e-01,\n",
       "           4.8645e-01, -4.0561e-02],\n",
       "         ...,\n",
       "         [-3.8154e-01,  1.7758e-01, -8.4079e-01,  ..., -9.0423e-01,\n",
       "          -3.0226e-01, -4.5119e-01],\n",
       "         [-1.0367e+00,  8.1546e-01,  7.2038e-01,  ..., -4.2163e-01,\n",
       "           2.8182e-01,  3.3869e-01],\n",
       "         [-8.6578e-01,  1.0580e+00, -2.9839e-01,  ..., -1.2486e+00,\n",
       "           2.0267e-02, -2.7191e-01]],\n",
       "\n",
       "        [[-1.2265e+00,  1.3946e+00,  1.0601e+00,  ..., -1.2715e+00,\n",
       "           8.1752e-01, -2.3028e-03],\n",
       "         [-1.2112e+00,  2.3593e+00,  1.1406e+00,  ..., -1.2452e+00,\n",
       "           2.2213e-01,  2.3002e-01],\n",
       "         [-1.0289e+00,  8.8550e-01,  5.6534e-01,  ..., -1.4191e+00,\n",
       "          -1.3458e-01,  7.5686e-01],\n",
       "         ...,\n",
       "         [-1.6829e-01,  7.1541e-01,  4.9588e-01,  ..., -1.6654e+00,\n",
       "           1.0091e-01,  1.0336e+00],\n",
       "         [-7.2825e-01,  2.2722e-01,  1.5001e+00,  ..., -6.9204e-01,\n",
       "          -2.6985e-01,  9.2524e-01],\n",
       "         [-1.3895e+00,  1.9967e+00,  2.7140e-01,  ..., -2.2494e-01,\n",
       "           4.9962e-01,  3.9580e-01]]], device='cuda:0'), tensor([[[-1.6895,  1.2309,  1.0137,  ..., -1.7890,  0.2262,  1.7574],\n",
       "         [-0.5941,  0.6945,  0.6456,  ..., -1.3473,  0.0878,  0.7680],\n",
       "         [-1.5936,  1.1201,  0.5015,  ..., -2.4289,  0.9336,  1.1952],\n",
       "         ...,\n",
       "         [-0.4249, -0.1506, -0.3065,  ..., -2.1055,  0.2011,  0.8497],\n",
       "         [-1.3491,  0.4738,  1.2559,  ..., -1.8780,  0.5163,  0.7568],\n",
       "         [-0.8032,  0.4542,  0.2810,  ..., -1.9515,  0.3062,  0.8090]],\n",
       "\n",
       "        [[-1.1161,  1.0706,  1.4319,  ..., -2.4880,  1.4762,  0.8219],\n",
       "         [-1.1999,  1.8461,  0.9018,  ..., -1.3214,  0.6989,  1.6937],\n",
       "         [-1.7924, -0.1586,  0.8716,  ..., -2.5540,  0.8892,  1.3087],\n",
       "         ...,\n",
       "         [-0.4649,  0.2804,  1.2094,  ..., -2.8605,  0.8958,  2.1266],\n",
       "         [-0.9491, -0.2069,  1.6907,  ..., -2.0822,  0.2493,  1.6729],\n",
       "         [-1.9137,  1.7817,  0.7796,  ..., -1.3092,  1.1291,  1.4186]]],\n",
       "       device='cuda:0'), tensor([[[-0.8534,  1.4078,  1.5029,  ..., -1.5384, -0.3072,  1.8419],\n",
       "         [-0.1453,  1.3409,  1.2176,  ..., -1.5572, -0.3369,  0.5667],\n",
       "         [-1.3465,  1.3324,  0.7204,  ..., -2.0276,  0.3626,  0.4154],\n",
       "         ...,\n",
       "         [ 0.1305,  0.2084,  0.5957,  ..., -1.5271, -0.5675,  0.9119],\n",
       "         [-1.0697,  0.5844,  1.7046,  ..., -2.1102,  0.2590,  0.1880],\n",
       "         [-0.9296,  0.0617,  1.0089,  ..., -1.8176, -0.0566,  0.9580]],\n",
       "\n",
       "        [[-0.8388,  1.1236,  1.5691,  ..., -2.0318,  0.9048,  0.7671],\n",
       "         [-0.4472,  1.1850,  1.2800,  ..., -1.0919,  0.4425,  0.7264],\n",
       "         [-1.3442, -0.2095,  1.8395,  ..., -1.9772,  1.3983,  1.4950],\n",
       "         ...,\n",
       "         [ 0.0310,  0.3658,  1.7610,  ..., -2.2245,  0.8506,  1.7211],\n",
       "         [-0.0275, -0.0150,  1.2764,  ..., -1.8893,  0.5400,  1.0182],\n",
       "         [-1.2772,  1.1281,  2.1405,  ..., -1.1817,  0.7422,  1.0470]]],\n",
       "       device='cuda:0'), tensor([[[-1.7319, -0.3607,  1.3480,  ..., -0.7462, -1.3530,  1.6311],\n",
       "         [-0.9783,  0.4005,  1.1449,  ..., -0.8091, -1.1307,  0.1278],\n",
       "         [-1.6309,  0.9927,  1.3285,  ..., -0.9963, -1.0119,  0.0880],\n",
       "         ...,\n",
       "         [-0.0877, -0.7833,  0.2703,  ..., -1.1880, -1.4520,  0.4867],\n",
       "         [-1.9345, -0.4809,  0.7215,  ..., -1.7561, -1.0042, -0.3902],\n",
       "         [-2.1255, -0.8651,  0.4842,  ..., -1.5525, -1.3648,  0.6962]],\n",
       "\n",
       "        [[-0.9212,  0.5121,  1.8180,  ..., -1.5539,  0.2467,  0.9719],\n",
       "         [-1.5249, -0.0152,  0.5992,  ..., -0.8235, -1.0264,  0.4108],\n",
       "         [-1.0407, -1.4339,  1.3941,  ..., -1.1372,  0.2327,  0.9719],\n",
       "         ...,\n",
       "         [-0.6508, -0.5177,  1.5293,  ..., -1.0896,  0.0824,  1.1235],\n",
       "         [-1.1410, -0.8605,  1.5728,  ..., -1.2504, -0.7460,  0.3516],\n",
       "         [-2.0835, -0.4642,  1.4805,  ..., -1.0204, -1.0545,  0.6524]]],\n",
       "       device='cuda:0')), past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model_pooler(\n",
    "    # model_output_1.hidden_states[-1],\n",
    "    inputs_embeds=torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d100646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1536])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeds = result.last_hidden_state  # .shape\n",
    "\n",
    "input_embeds = input_embeds.sum(1) / torch.tensor(\n",
    "    input_embeds.shape[1],\n",
    "    device=input_embeds.device,\n",
    "    dtype=input_embeds.dtype,\n",
    ")\n",
    "# print(input_embeds.dtype)\n",
    "input_embeds = input_embeds.unsqueeze(1)\n",
    "input_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774ecef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from hidden_capacity_reasoning.models import RobertaModelEmbedPoolerV1\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "roberta_model = RobertaModelEmbedPoolerV1(\n",
    "    RobertaConfig(\n",
    "        # num_hidden_layers=6,\n",
    "        hidden_size=1536,\n",
    "    )\n",
    ")\n",
    "roberta_model = roberta_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cda1e360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2004,  1.7836,  0.4762,  ..., -0.2210, -1.0527,  2.2582]],\n",
       "\n",
       "        [[-0.2585,  1.3164,  0.5259,  ..., -0.1286, -0.8434,  1.8991]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = roberta_model(\n",
    "    # model_output_1.hidden_states[-1],\n",
    "    input_embeds=torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    # output_hidden_states=True,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5357d70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1536])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fb86e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 1536, padding_idx=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd4d9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2RotaryEmbedding, Qwen2Config\n",
    "from torch import nn\n",
    "\n",
    "config = Qwen2Config()\n",
    "emb = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id).cuda()\n",
    "\n",
    "new_embeds = emb(model_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15d937d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 4096])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a5d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 22016,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"max_window_layers\": 28,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.49.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 151936\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Qwen2Config, RobertaConfig, PretrainedConfig\n",
    "\n",
    "\n",
    "# PretrainedConfig(new_config)\n",
    "class EmdeddingCompressorConfig(PretrainedConfig):\n",
    "    roberta: RobertaConfig = RobertaConfig(\n",
    "        hidden_size=1568,\n",
    "    )\n",
    "    qwen: Qwen2Config = Qwen2Config()\n",
    "\n",
    "\n",
    "config = EmdeddingCompressorConfig()\n",
    "config.roberta\n",
    "config.qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90e6a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.save_pretrained(\"EmdeddingCompressorConfig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35a69625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1568,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.49.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmdeddingCompressorConfig.from_pretrained(\"EmdeddingCompressorConfig\").roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "036fd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hidden_capacity_reasoning.models import (\n",
    "    Qwen2ForCausalLMCompressionV3,\n",
    "    EmdeddingCompressorConfigV1,\n",
    ")\n",
    "from transformers import Qwen2Config, RobertaConfig\n",
    "\n",
    "# config = EmdeddingCompressorConfigV1()\n",
    "config = Qwen2Config.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')\n",
    "config.roberta = RobertaConfig(\n",
    "    hidden_size=1536,\n",
    ")\n",
    "# new_model = Qwen2ForCausalLMCompressionV3.from_pretrained(\n",
    "new_model = Qwen2ForCausalLMCompressionV3(\n",
    "    config=config,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    # **config,\n",
    "    # \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e64bd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Model(\n",
       "  (embed_tokens): Embedding(151936, 1536)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x Qwen2DecoderLayer(\n",
       "      (self_attn): Qwen2Attention(\n",
       "        (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "      )\n",
       "      (mlp): Qwen2MLP(\n",
       "        (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "        (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "        (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "  (rotary_emb): Qwen2RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d1e44ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLMCompressionV3(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=22016, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=22016, bias=False)\n",
       "          (down_proj): Linear(in_features=22016, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       "  (embed_pooler): RobertaModelEmbedPoolerV1(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1536, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 1536, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 1536)\n",
       "      (LayerNorm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (key): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (value): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (LayerNorm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (LayerNorm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (model): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 1536, padding_idx=1)\n",
       "        (position_embeddings): Embedding(512, 1536, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(2, 1536)\n",
       "        (LayerNorm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSdpaSelfAttention(\n",
       "                (query): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (key): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (value): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (LayerNorm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (LayerNorm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): RobertaPooler(\n",
       "        (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d492557f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['embed_pooler.embeddings.word_embeddings.weight', 'embed_pooler.embeddings.position_embeddings.weight', 'embed_pooler.embeddings.token_type_embeddings.weight', 'embed_pooler.embeddings.LayerNorm.weight', 'embed_pooler.embeddings.LayerNorm.bias', 'embed_pooler.encoder.layer.0.attention.self.query.weight', 'embed_pooler.encoder.layer.0.attention.self.query.bias', 'embed_pooler.encoder.layer.0.attention.self.key.weight', 'embed_pooler.encoder.layer.0.attention.self.key.bias', 'embed_pooler.encoder.layer.0.attention.self.value.weight', 'embed_pooler.encoder.layer.0.attention.self.value.bias', 'embed_pooler.encoder.layer.0.attention.output.dense.weight', 'embed_pooler.encoder.layer.0.attention.output.dense.bias', 'embed_pooler.encoder.layer.0.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.0.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.0.intermediate.dense.weight', 'embed_pooler.encoder.layer.0.intermediate.dense.bias', 'embed_pooler.encoder.layer.0.output.dense.weight', 'embed_pooler.encoder.layer.0.output.dense.bias', 'embed_pooler.encoder.layer.0.output.LayerNorm.weight', 'embed_pooler.encoder.layer.0.output.LayerNorm.bias', 'embed_pooler.encoder.layer.1.attention.self.query.weight', 'embed_pooler.encoder.layer.1.attention.self.query.bias', 'embed_pooler.encoder.layer.1.attention.self.key.weight', 'embed_pooler.encoder.layer.1.attention.self.key.bias', 'embed_pooler.encoder.layer.1.attention.self.value.weight', 'embed_pooler.encoder.layer.1.attention.self.value.bias', 'embed_pooler.encoder.layer.1.attention.output.dense.weight', 'embed_pooler.encoder.layer.1.attention.output.dense.bias', 'embed_pooler.encoder.layer.1.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.1.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.1.intermediate.dense.weight', 'embed_pooler.encoder.layer.1.intermediate.dense.bias', 'embed_pooler.encoder.layer.1.output.dense.weight', 'embed_pooler.encoder.layer.1.output.dense.bias', 'embed_pooler.encoder.layer.1.output.LayerNorm.weight', 'embed_pooler.encoder.layer.1.output.LayerNorm.bias', 'embed_pooler.encoder.layer.2.attention.self.query.weight', 'embed_pooler.encoder.layer.2.attention.self.query.bias', 'embed_pooler.encoder.layer.2.attention.self.key.weight', 'embed_pooler.encoder.layer.2.attention.self.key.bias', 'embed_pooler.encoder.layer.2.attention.self.value.weight', 'embed_pooler.encoder.layer.2.attention.self.value.bias', 'embed_pooler.encoder.layer.2.attention.output.dense.weight', 'embed_pooler.encoder.layer.2.attention.output.dense.bias', 'embed_pooler.encoder.layer.2.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.2.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.2.intermediate.dense.weight', 'embed_pooler.encoder.layer.2.intermediate.dense.bias', 'embed_pooler.encoder.layer.2.output.dense.weight', 'embed_pooler.encoder.layer.2.output.dense.bias', 'embed_pooler.encoder.layer.2.output.LayerNorm.weight', 'embed_pooler.encoder.layer.2.output.LayerNorm.bias', 'embed_pooler.encoder.layer.3.attention.self.query.weight', 'embed_pooler.encoder.layer.3.attention.self.query.bias', 'embed_pooler.encoder.layer.3.attention.self.key.weight', 'embed_pooler.encoder.layer.3.attention.self.key.bias', 'embed_pooler.encoder.layer.3.attention.self.value.weight', 'embed_pooler.encoder.layer.3.attention.self.value.bias', 'embed_pooler.encoder.layer.3.attention.output.dense.weight', 'embed_pooler.encoder.layer.3.attention.output.dense.bias', 'embed_pooler.encoder.layer.3.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.3.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.3.intermediate.dense.weight', 'embed_pooler.encoder.layer.3.intermediate.dense.bias', 'embed_pooler.encoder.layer.3.output.dense.weight', 'embed_pooler.encoder.layer.3.output.dense.bias', 'embed_pooler.encoder.layer.3.output.LayerNorm.weight', 'embed_pooler.encoder.layer.3.output.LayerNorm.bias', 'embed_pooler.encoder.layer.4.attention.self.query.weight', 'embed_pooler.encoder.layer.4.attention.self.query.bias', 'embed_pooler.encoder.layer.4.attention.self.key.weight', 'embed_pooler.encoder.layer.4.attention.self.key.bias', 'embed_pooler.encoder.layer.4.attention.self.value.weight', 'embed_pooler.encoder.layer.4.attention.self.value.bias', 'embed_pooler.encoder.layer.4.attention.output.dense.weight', 'embed_pooler.encoder.layer.4.attention.output.dense.bias', 'embed_pooler.encoder.layer.4.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.4.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.4.intermediate.dense.weight', 'embed_pooler.encoder.layer.4.intermediate.dense.bias', 'embed_pooler.encoder.layer.4.output.dense.weight', 'embed_pooler.encoder.layer.4.output.dense.bias', 'embed_pooler.encoder.layer.4.output.LayerNorm.weight', 'embed_pooler.encoder.layer.4.output.LayerNorm.bias', 'embed_pooler.encoder.layer.5.attention.self.query.weight', 'embed_pooler.encoder.layer.5.attention.self.query.bias', 'embed_pooler.encoder.layer.5.attention.self.key.weight', 'embed_pooler.encoder.layer.5.attention.self.key.bias', 'embed_pooler.encoder.layer.5.attention.self.value.weight', 'embed_pooler.encoder.layer.5.attention.self.value.bias', 'embed_pooler.encoder.layer.5.attention.output.dense.weight', 'embed_pooler.encoder.layer.5.attention.output.dense.bias', 'embed_pooler.encoder.layer.5.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.5.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.5.intermediate.dense.weight', 'embed_pooler.encoder.layer.5.intermediate.dense.bias', 'embed_pooler.encoder.layer.5.output.dense.weight', 'embed_pooler.encoder.layer.5.output.dense.bias', 'embed_pooler.encoder.layer.5.output.LayerNorm.weight', 'embed_pooler.encoder.layer.5.output.LayerNorm.bias', 'embed_pooler.encoder.layer.6.attention.self.query.weight', 'embed_pooler.encoder.layer.6.attention.self.query.bias', 'embed_pooler.encoder.layer.6.attention.self.key.weight', 'embed_pooler.encoder.layer.6.attention.self.key.bias', 'embed_pooler.encoder.layer.6.attention.self.value.weight', 'embed_pooler.encoder.layer.6.attention.self.value.bias', 'embed_pooler.encoder.layer.6.attention.output.dense.weight', 'embed_pooler.encoder.layer.6.attention.output.dense.bias', 'embed_pooler.encoder.layer.6.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.6.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.6.intermediate.dense.weight', 'embed_pooler.encoder.layer.6.intermediate.dense.bias', 'embed_pooler.encoder.layer.6.output.dense.weight', 'embed_pooler.encoder.layer.6.output.dense.bias', 'embed_pooler.encoder.layer.6.output.LayerNorm.weight', 'embed_pooler.encoder.layer.6.output.LayerNorm.bias', 'embed_pooler.encoder.layer.7.attention.self.query.weight', 'embed_pooler.encoder.layer.7.attention.self.query.bias', 'embed_pooler.encoder.layer.7.attention.self.key.weight', 'embed_pooler.encoder.layer.7.attention.self.key.bias', 'embed_pooler.encoder.layer.7.attention.self.value.weight', 'embed_pooler.encoder.layer.7.attention.self.value.bias', 'embed_pooler.encoder.layer.7.attention.output.dense.weight', 'embed_pooler.encoder.layer.7.attention.output.dense.bias', 'embed_pooler.encoder.layer.7.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.7.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.7.intermediate.dense.weight', 'embed_pooler.encoder.layer.7.intermediate.dense.bias', 'embed_pooler.encoder.layer.7.output.dense.weight', 'embed_pooler.encoder.layer.7.output.dense.bias', 'embed_pooler.encoder.layer.7.output.LayerNorm.weight', 'embed_pooler.encoder.layer.7.output.LayerNorm.bias', 'embed_pooler.encoder.layer.8.attention.self.query.weight', 'embed_pooler.encoder.layer.8.attention.self.query.bias', 'embed_pooler.encoder.layer.8.attention.self.key.weight', 'embed_pooler.encoder.layer.8.attention.self.key.bias', 'embed_pooler.encoder.layer.8.attention.self.value.weight', 'embed_pooler.encoder.layer.8.attention.self.value.bias', 'embed_pooler.encoder.layer.8.attention.output.dense.weight', 'embed_pooler.encoder.layer.8.attention.output.dense.bias', 'embed_pooler.encoder.layer.8.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.8.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.8.intermediate.dense.weight', 'embed_pooler.encoder.layer.8.intermediate.dense.bias', 'embed_pooler.encoder.layer.8.output.dense.weight', 'embed_pooler.encoder.layer.8.output.dense.bias', 'embed_pooler.encoder.layer.8.output.LayerNorm.weight', 'embed_pooler.encoder.layer.8.output.LayerNorm.bias', 'embed_pooler.encoder.layer.9.attention.self.query.weight', 'embed_pooler.encoder.layer.9.attention.self.query.bias', 'embed_pooler.encoder.layer.9.attention.self.key.weight', 'embed_pooler.encoder.layer.9.attention.self.key.bias', 'embed_pooler.encoder.layer.9.attention.self.value.weight', 'embed_pooler.encoder.layer.9.attention.self.value.bias', 'embed_pooler.encoder.layer.9.attention.output.dense.weight', 'embed_pooler.encoder.layer.9.attention.output.dense.bias', 'embed_pooler.encoder.layer.9.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.9.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.9.intermediate.dense.weight', 'embed_pooler.encoder.layer.9.intermediate.dense.bias', 'embed_pooler.encoder.layer.9.output.dense.weight', 'embed_pooler.encoder.layer.9.output.dense.bias', 'embed_pooler.encoder.layer.9.output.LayerNorm.weight', 'embed_pooler.encoder.layer.9.output.LayerNorm.bias', 'embed_pooler.encoder.layer.10.attention.self.query.weight', 'embed_pooler.encoder.layer.10.attention.self.query.bias', 'embed_pooler.encoder.layer.10.attention.self.key.weight', 'embed_pooler.encoder.layer.10.attention.self.key.bias', 'embed_pooler.encoder.layer.10.attention.self.value.weight', 'embed_pooler.encoder.layer.10.attention.self.value.bias', 'embed_pooler.encoder.layer.10.attention.output.dense.weight', 'embed_pooler.encoder.layer.10.attention.output.dense.bias', 'embed_pooler.encoder.layer.10.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.10.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.10.intermediate.dense.weight', 'embed_pooler.encoder.layer.10.intermediate.dense.bias', 'embed_pooler.encoder.layer.10.output.dense.weight', 'embed_pooler.encoder.layer.10.output.dense.bias', 'embed_pooler.encoder.layer.10.output.LayerNorm.weight', 'embed_pooler.encoder.layer.10.output.LayerNorm.bias', 'embed_pooler.encoder.layer.11.attention.self.query.weight', 'embed_pooler.encoder.layer.11.attention.self.query.bias', 'embed_pooler.encoder.layer.11.attention.self.key.weight', 'embed_pooler.encoder.layer.11.attention.self.key.bias', 'embed_pooler.encoder.layer.11.attention.self.value.weight', 'embed_pooler.encoder.layer.11.attention.self.value.bias', 'embed_pooler.encoder.layer.11.attention.output.dense.weight', 'embed_pooler.encoder.layer.11.attention.output.dense.bias', 'embed_pooler.encoder.layer.11.attention.output.LayerNorm.weight', 'embed_pooler.encoder.layer.11.attention.output.LayerNorm.bias', 'embed_pooler.encoder.layer.11.intermediate.dense.weight', 'embed_pooler.encoder.layer.11.intermediate.dense.bias', 'embed_pooler.encoder.layer.11.output.dense.weight', 'embed_pooler.encoder.layer.11.output.dense.bias', 'embed_pooler.encoder.layer.11.output.LayerNorm.weight', 'embed_pooler.encoder.layer.11.output.LayerNorm.bias', 'embed_pooler.pooler.dense.weight', 'embed_pooler.pooler.dense.bias', 'embed_pooler.model.embeddings.word_embeddings.weight', 'embed_pooler.model.embeddings.position_embeddings.weight', 'embed_pooler.model.embeddings.token_type_embeddings.weight', 'embed_pooler.model.embeddings.LayerNorm.weight', 'embed_pooler.model.embeddings.LayerNorm.bias', 'embed_pooler.model.encoder.layer.0.attention.self.query.weight', 'embed_pooler.model.encoder.layer.0.attention.self.query.bias', 'embed_pooler.model.encoder.layer.0.attention.self.key.weight', 'embed_pooler.model.encoder.layer.0.attention.self.key.bias', 'embed_pooler.model.encoder.layer.0.attention.self.value.weight', 'embed_pooler.model.encoder.layer.0.attention.self.value.bias', 'embed_pooler.model.encoder.layer.0.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.0.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.0.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.0.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.0.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.0.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.0.output.dense.weight', 'embed_pooler.model.encoder.layer.0.output.dense.bias', 'embed_pooler.model.encoder.layer.0.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.0.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.1.attention.self.query.weight', 'embed_pooler.model.encoder.layer.1.attention.self.query.bias', 'embed_pooler.model.encoder.layer.1.attention.self.key.weight', 'embed_pooler.model.encoder.layer.1.attention.self.key.bias', 'embed_pooler.model.encoder.layer.1.attention.self.value.weight', 'embed_pooler.model.encoder.layer.1.attention.self.value.bias', 'embed_pooler.model.encoder.layer.1.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.1.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.1.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.1.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.1.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.1.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.1.output.dense.weight', 'embed_pooler.model.encoder.layer.1.output.dense.bias', 'embed_pooler.model.encoder.layer.1.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.1.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.2.attention.self.query.weight', 'embed_pooler.model.encoder.layer.2.attention.self.query.bias', 'embed_pooler.model.encoder.layer.2.attention.self.key.weight', 'embed_pooler.model.encoder.layer.2.attention.self.key.bias', 'embed_pooler.model.encoder.layer.2.attention.self.value.weight', 'embed_pooler.model.encoder.layer.2.attention.self.value.bias', 'embed_pooler.model.encoder.layer.2.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.2.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.2.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.2.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.2.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.2.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.2.output.dense.weight', 'embed_pooler.model.encoder.layer.2.output.dense.bias', 'embed_pooler.model.encoder.layer.2.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.2.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.3.attention.self.query.weight', 'embed_pooler.model.encoder.layer.3.attention.self.query.bias', 'embed_pooler.model.encoder.layer.3.attention.self.key.weight', 'embed_pooler.model.encoder.layer.3.attention.self.key.bias', 'embed_pooler.model.encoder.layer.3.attention.self.value.weight', 'embed_pooler.model.encoder.layer.3.attention.self.value.bias', 'embed_pooler.model.encoder.layer.3.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.3.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.3.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.3.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.3.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.3.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.3.output.dense.weight', 'embed_pooler.model.encoder.layer.3.output.dense.bias', 'embed_pooler.model.encoder.layer.3.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.3.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.4.attention.self.query.weight', 'embed_pooler.model.encoder.layer.4.attention.self.query.bias', 'embed_pooler.model.encoder.layer.4.attention.self.key.weight', 'embed_pooler.model.encoder.layer.4.attention.self.key.bias', 'embed_pooler.model.encoder.layer.4.attention.self.value.weight', 'embed_pooler.model.encoder.layer.4.attention.self.value.bias', 'embed_pooler.model.encoder.layer.4.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.4.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.4.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.4.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.4.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.4.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.4.output.dense.weight', 'embed_pooler.model.encoder.layer.4.output.dense.bias', 'embed_pooler.model.encoder.layer.4.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.4.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.5.attention.self.query.weight', 'embed_pooler.model.encoder.layer.5.attention.self.query.bias', 'embed_pooler.model.encoder.layer.5.attention.self.key.weight', 'embed_pooler.model.encoder.layer.5.attention.self.key.bias', 'embed_pooler.model.encoder.layer.5.attention.self.value.weight', 'embed_pooler.model.encoder.layer.5.attention.self.value.bias', 'embed_pooler.model.encoder.layer.5.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.5.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.5.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.5.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.5.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.5.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.5.output.dense.weight', 'embed_pooler.model.encoder.layer.5.output.dense.bias', 'embed_pooler.model.encoder.layer.5.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.5.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.6.attention.self.query.weight', 'embed_pooler.model.encoder.layer.6.attention.self.query.bias', 'embed_pooler.model.encoder.layer.6.attention.self.key.weight', 'embed_pooler.model.encoder.layer.6.attention.self.key.bias', 'embed_pooler.model.encoder.layer.6.attention.self.value.weight', 'embed_pooler.model.encoder.layer.6.attention.self.value.bias', 'embed_pooler.model.encoder.layer.6.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.6.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.6.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.6.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.6.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.6.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.6.output.dense.weight', 'embed_pooler.model.encoder.layer.6.output.dense.bias', 'embed_pooler.model.encoder.layer.6.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.6.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.7.attention.self.query.weight', 'embed_pooler.model.encoder.layer.7.attention.self.query.bias', 'embed_pooler.model.encoder.layer.7.attention.self.key.weight', 'embed_pooler.model.encoder.layer.7.attention.self.key.bias', 'embed_pooler.model.encoder.layer.7.attention.self.value.weight', 'embed_pooler.model.encoder.layer.7.attention.self.value.bias', 'embed_pooler.model.encoder.layer.7.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.7.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.7.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.7.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.7.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.7.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.7.output.dense.weight', 'embed_pooler.model.encoder.layer.7.output.dense.bias', 'embed_pooler.model.encoder.layer.7.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.7.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.8.attention.self.query.weight', 'embed_pooler.model.encoder.layer.8.attention.self.query.bias', 'embed_pooler.model.encoder.layer.8.attention.self.key.weight', 'embed_pooler.model.encoder.layer.8.attention.self.key.bias', 'embed_pooler.model.encoder.layer.8.attention.self.value.weight', 'embed_pooler.model.encoder.layer.8.attention.self.value.bias', 'embed_pooler.model.encoder.layer.8.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.8.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.8.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.8.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.8.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.8.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.8.output.dense.weight', 'embed_pooler.model.encoder.layer.8.output.dense.bias', 'embed_pooler.model.encoder.layer.8.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.8.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.9.attention.self.query.weight', 'embed_pooler.model.encoder.layer.9.attention.self.query.bias', 'embed_pooler.model.encoder.layer.9.attention.self.key.weight', 'embed_pooler.model.encoder.layer.9.attention.self.key.bias', 'embed_pooler.model.encoder.layer.9.attention.self.value.weight', 'embed_pooler.model.encoder.layer.9.attention.self.value.bias', 'embed_pooler.model.encoder.layer.9.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.9.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.9.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.9.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.9.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.9.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.9.output.dense.weight', 'embed_pooler.model.encoder.layer.9.output.dense.bias', 'embed_pooler.model.encoder.layer.9.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.9.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.10.attention.self.query.weight', 'embed_pooler.model.encoder.layer.10.attention.self.query.bias', 'embed_pooler.model.encoder.layer.10.attention.self.key.weight', 'embed_pooler.model.encoder.layer.10.attention.self.key.bias', 'embed_pooler.model.encoder.layer.10.attention.self.value.weight', 'embed_pooler.model.encoder.layer.10.attention.self.value.bias', 'embed_pooler.model.encoder.layer.10.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.10.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.10.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.10.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.10.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.10.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.10.output.dense.weight', 'embed_pooler.model.encoder.layer.10.output.dense.bias', 'embed_pooler.model.encoder.layer.10.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.10.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.11.attention.self.query.weight', 'embed_pooler.model.encoder.layer.11.attention.self.query.bias', 'embed_pooler.model.encoder.layer.11.attention.self.key.weight', 'embed_pooler.model.encoder.layer.11.attention.self.key.bias', 'embed_pooler.model.encoder.layer.11.attention.self.value.weight', 'embed_pooler.model.encoder.layer.11.attention.self.value.bias', 'embed_pooler.model.encoder.layer.11.attention.output.dense.weight', 'embed_pooler.model.encoder.layer.11.attention.output.dense.bias', 'embed_pooler.model.encoder.layer.11.attention.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.11.attention.output.LayerNorm.bias', 'embed_pooler.model.encoder.layer.11.intermediate.dense.weight', 'embed_pooler.model.encoder.layer.11.intermediate.dense.bias', 'embed_pooler.model.encoder.layer.11.output.dense.weight', 'embed_pooler.model.encoder.layer.11.output.dense.bias', 'embed_pooler.model.encoder.layer.11.output.LayerNorm.weight', 'embed_pooler.model.encoder.layer.11.output.LayerNorm.bias', 'embed_pooler.model.pooler.dense.weight', 'embed_pooler.model.pooler.dense.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.load_state_dict(model.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd32a3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "new_model.save_pretrained('r1_compressor_v4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
