{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953c0eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06318e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Okay, so I need to come up with a short introduction to a large language model. The user provided an example, which is pretty concise and covers the basics. Let me think about how I can approach this.\\n\\nFirst, I should consider what a large language model (LLM) is. It\\'s a type of AI that can understand and generate human language, right? So, it\\'s designed to do things like text generation, translation, summarization, and more. It\\'s widely used in various fields like education, healthcare, and even entertainment.\\n\\nI should make sure the introduction is informative but not too technical. It should give a good overview without getting bogged down in details. Maybe start with the definition, then mention its capabilities, and perhaps touch on its applications.\\n\\nWait, the example also mentions the user\\'s role and the purpose of the assistant. So, I should keep it friendly and approachable, maybe a bit enthusiastic to attract the user.\\n\\nI should avoid making it too long. The example is around 300 words, so I need to keep mine concise. Let me structure it: start with an introduction to LLM, mention its unique features, and then give a brief overview of its applications and the impact it has.\\n\\nI should also ensure that the language is engaging, maybe using some adjectives like \"powerful,\" \"innovative,\" and \"transformative.\" That might make it more appealing.\\n\\nLet me think about the flow: start with what an LLM is, then what it can do, and then how it\\'s useful. Maybe end with a sentence about its potential to change the future.\\n\\nI should also make sure to highlight the versatility and adaptability of LLMs, as that shows their value beyond just one area.\\n\\nOkay, putting it all together, I can draft something like:\\n\\n\"Large Language Models (LLMs) are advanced AI systems designed to understand and generate human language with remarkable accuracy and creativity. Unlike traditional language models, LLMs are capable of processing vast amounts of data and learning from it, making them highly adaptable and versatile. These models can perform a wide range of tasks, from reading comprehension and writing essays to translation and summarization. Their ability to understand complex patterns and nuances makes them invaluable in fields such as education, healthcare, and even entertainment. By leveraging their intelligence, LLMs have the potential to transform industries and improve how people interact with language. As technology continues to evolve, LLMs hold the promise of being the future of communication and creativity.\"\\n\\nHmm'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model..1\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87598600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 23, 1536]), torch.bfloat16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    **model_inputs,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.hidden_states[-1].shape, model_output_1.hidden_states[-1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a732ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2Config\n",
    "\n",
    "\n",
    "class Qwen2ModelEmbedPooler(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.lm_head = None\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds):\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )[0]\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.sum(1) / torch.tensor(\n",
    "            input_embeds.shape[1],\n",
    "            device=input_embeds.device,\n",
    "            dtype=input_embeds.dtype,\n",
    "        )\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.unsqueeze(1)\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "config = Qwen2Config.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "config.num_hidden_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6620a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hidden_capacity_reasoning.models import (\n",
    "    Qwen2ModelEmbedPoolerV2,\n",
    "    Qwen2ForCausalLMCompressionV5,\n",
    ")\n",
    "\n",
    "\n",
    "# embed_pooler = Qwen2ModelEmbedPooler.from_pretrained(\n",
    "\n",
    "# embed_pooler = Qwen2ModelEmbedPooler(\n",
    "embed_pooler = Qwen2ModelEmbedPoolerV2(\n",
    "    config=config,\n",
    "    # \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    # device_map={\"\":0},\n",
    ")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(count_parameters(embed_pooler))\n",
    "\n",
    "# result = embed_pooler(\n",
    "#     # model_output_1.hidden_states[-1],\n",
    "#     torch.cat(\n",
    "#         [\n",
    "#             model_output_1.hidden_states[-1],\n",
    "#             model_output_1.hidden_states[-1],\n",
    "#         ],\n",
    "#         dim=0,\n",
    "#     ),\n",
    "# )\n",
    "# result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ed4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f06799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some weights of Qwen2ForCausalLMCompressionV5 were not initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B and are newly initialized: ['embed_pooler.model.embed_tokens.weight', 'embed_pooler.model.layers.0.input_layernorm.weight', 'embed_pooler.model.layers.0.mlp.down_proj.weight', 'embed_pooler.model.layers.0.mlp.gate_proj.weight', 'embed_pooler.model.layers.0.mlp.up_proj.weight', 'embed_pooler.model.layers.0.post_attention_layernorm.weight', 'embed_pooler.model.layers.0.self_attn.k_proj.bias', 'embed_pooler.model.layers.0.self_attn.k_proj.weight', 'embed_pooler.model.layers.0.self_attn.o_proj.weight', 'embed_pooler.model.layers.0.self_attn.q_proj.bias', 'embed_pooler.model.layers.0.self_attn.q_proj.weight', 'embed_pooler.model.layers.0.self_attn.v_proj.bias', 'embed_pooler.model.layers.0.self_attn.v_proj.weight', 'embed_pooler.model.layers.1.input_layernorm.weight', 'embed_pooler.model.layers.1.mlp.down_proj.weight', 'embed_pooler.model.layers.1.mlp.gate_proj.weight', 'embed_pooler.model.layers.1.mlp.up_proj.weight', 'embed_pooler.model.layers.1.post_attention_layernorm.weight', 'embed_pooler.model.layers.1.self_attn.k_proj.bias', 'embed_pooler.model.layers.1.self_attn.k_proj.weight', 'embed_pooler.model.layers.1.self_attn.o_proj.weight', 'embed_pooler.model.layers.1.self_attn.q_proj.bias', 'embed_pooler.model.layers.1.self_attn.q_proj.weight', 'embed_pooler.model.layers.1.self_attn.v_proj.bias', 'embed_pooler.model.layers.1.self_attn.v_proj.weight', 'embed_pooler.model.layers.2.input_layernorm.weight', 'embed_pooler.model.layers.2.mlp.down_proj.weight', 'embed_pooler.model.layers.2.mlp.gate_proj.weight', 'embed_pooler.model.layers.2.mlp.up_proj.weight', 'embed_pooler.model.layers.2.post_attention_layernorm.weight', 'embed_pooler.model.layers.2.self_attn.k_proj.bias', 'embed_pooler.model.layers.2.self_attn.k_proj.weight', 'embed_pooler.model.layers.2.self_attn.o_proj.weight', 'embed_pooler.model.layers.2.self_attn.q_proj.bias', 'embed_pooler.model.layers.2.self_attn.q_proj.weight', 'embed_pooler.model.layers.2.self_attn.v_proj.bias', 'embed_pooler.model.layers.2.self_attn.v_proj.weight', 'embed_pooler.model.layers.3.input_layernorm.weight', 'embed_pooler.model.layers.3.mlp.down_proj.weight', 'embed_pooler.model.layers.3.mlp.gate_proj.weight', 'embed_pooler.model.layers.3.mlp.up_proj.weight', 'embed_pooler.model.layers.3.post_attention_layernorm.weight', 'embed_pooler.model.layers.3.self_attn.k_proj.bias', 'embed_pooler.model.layers.3.self_attn.k_proj.weight', 'embed_pooler.model.layers.3.self_attn.o_proj.weight', 'embed_pooler.model.layers.3.self_attn.q_proj.bias', 'embed_pooler.model.layers.3.self_attn.q_proj.weight', 'embed_pooler.model.layers.3.self_attn.v_proj.bias', 'embed_pooler.model.layers.3.self_attn.v_proj.weight', 'embed_pooler.model.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from hidden_capacity_reasoning.models import (\n",
    "    Qwen2ModelEmbedPoolerV2,\n",
    "    Qwen2ForCausalLMCompressionV5,\n",
    "    Qwen2PoolerConfig,\n",
    ")\n",
    "from transformers import Qwen2Config, PretrainedConfig\n",
    "\n",
    "\n",
    "# class Qwen2PoolerConfig(Qwen2Config):\n",
    "#     pooler_config: Qwen2Config = None\n",
    "\n",
    "\n",
    "config = Qwen2Config.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "pooler_config = Qwen2Config.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "pooler_config.num_hidden_layers = 4\n",
    "config.pooler_config = pooler_config\n",
    "\n",
    "\n",
    "new_model = Qwen2ForCausalLMCompressionV5.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0d74890",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save_pretrained(\"r1_compressor_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bef6cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('r1_compressor_v5/tokenizer_config.json',\n",
       " 'r1_compressor_v5/special_tokens_map.json',\n",
       " 'r1_compressor_v5/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "tokenizer.save_pretrained(\"r1_compressor_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f0fb950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7cece291e1f4c4db166a42a8ff6b37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hidden_capacity_reasoning.models import (\n",
    "    Qwen2ModelEmbedPoolerV2,\n",
    "    Qwen2ForCausalLMCompressionV5,\n",
    ")\n",
    "\n",
    "new_model = Qwen2ForCausalLMCompressionV5.from_pretrained(\n",
    "    \"r1_compressor_v5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fada556e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 22016,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"max_window_layers\": 28,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.49.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": {\n",
       "    \"_attn_implementation_autoset\": true,\n",
       "    \"_name_or_path\": \"\",\n",
       "    \"add_cross_attention\": false,\n",
       "    \"architectures\": [\n",
       "      \"Qwen2ForCausalLM\"\n",
       "    ],\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": 151643,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 151643,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"hidden_act\": \"silu\",\n",
       "    \"hidden_size\": 1536,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 8960,\n",
       "    \"is_decoder\": false,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"max_position_embeddings\": 131072,\n",
       "    \"max_window_layers\": 21,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"qwen2\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_hidden_layers\": 4,\n",
       "    \"num_key_value_heads\": 2,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": null,\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"rms_norm_eps\": 1e-06,\n",
       "    \"rope_scaling\": null,\n",
       "    \"rope_theta\": 10000,\n",
       "    \"sep_token_id\": null,\n",
       "    \"sliding_window\": 4096,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": false,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": \"bfloat16\",\n",
       "    \"torchscript\": false,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"use_mrope\": false,\n",
       "    \"use_sliding_window\": false,\n",
       "    \"vocab_size\": 151936\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hidden_capacity_reasoning.models import (\n",
    "    Qwen2ModelEmbedPoolerV2,\n",
    "    Qwen2ForCausalLMCompressionV5,\n",
    "    Qwen2PoolerConfig,\n",
    ")\n",
    "from transformers import Qwen2Config\n",
    "\n",
    "config = Qwen2PoolerConfig.from_pretrained(\"r1_compressor_v5\")\n",
    "Qwen2Config(config.pooler_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bcc5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "39 % 40 == 0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
