{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "953c0eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06318e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Okay, so I need to write a short introduction for a large language model. Let me start by thinking about what a large language model is. From what I know, it\\'s a type of AI that can understand and generate human-level language. It\\'s really good at learning patterns and rules in text, so it can generate coherent and meaningful responses.\\n\\nI should make sure the introduction is concise and covers the key points. Maybe start by explaining what it is, then mention its capabilities. I should also touch on its applications because that shows it\\'s useful beyond just generating text. I remember seeing that LLMs are used in various fields like writing, creative arts, education, and even in fields like law and finance. That\\'s a good point to include.\\n\\nI should also highlight its strengths, like being able to understand different languages and adapt to various contexts. That makes the introduction more comprehensive. Maybe end with something about the future, like its potential to shape the future of technology and creativity.\\n\\nWait, I should make sure the language is clear and flows well. I don\\'t want it to be too technical, but still informative. Let me structure it: start with an overview, then talk about capabilities, applications, strengths, and conclude with its future potential. That should make it cover all the necessary points in a short and engaging way.\\n\\nI also need to avoid any jargon that\\'s too obscure. Instead, use everyday language to make it accessible. Let me think about each sentence to ensure it\\'s clear and concise. Maybe something like:\\n\\n\"A large language model is an advanced AI that can understand and generate human-level language. It\\'s powerful in its ability to comprehend patterns and rules in text, making it useful in various applications such as creative arts, education, and beyond. Its versatility allows it to adapt to different contexts and languages, making it a valuable tool in the digital age. As technology continues to evolve, these models are poised to shape the future of creativity and problem-solving.\"\\n\\nDoes that cover all the points? It starts by defining it, then moves on to capabilities, applications, strengths, and future potential. I think that works well. I should review it to make sure it\\'s not too long and flows smoothly.\\n</think>\\n\\nA large language model is an advanced AI designed to understand and generate human-level language. Its capabilities include comprehending patterns and rules in text, making it highly versatile in applications such as creative arts, education, and beyond. The model\\'s adaptability to various contexts and languages positions it as a valuable tool in the'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model..1\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87598600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 23, 1536]), torch.bfloat16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    **model_inputs,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.hidden_states[-1].shape, model_output_1.hidden_states[-1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a732ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1536])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Qwen2ModelEmbedPooler(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.lm_head = None\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds):\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )[0]\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.sum(1) / torch.tensor(\n",
    "            input_embeds.shape[1],\n",
    "            device=input_embeds.device,\n",
    "            dtype=input_embeds.dtype,\n",
    "        )\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.unsqueeze(1)\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "embed_pooler = Qwen2ModelEmbedPooler.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "result = embed_pooler(\n",
    "    # model_output_1.hidden_states[-1],\n",
    "    torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    ")\n",
    "result.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
