{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffdb004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "224 202 0.9017857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "# model = model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    # \"dim/hendrycks_math_train_12k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096\"\n",
    "    # \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    # \"dim/hendrycks_math_train_1k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    ")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    # test_size=250,\n",
    "    test_size=350,\n",
    "    # test_size=999,\n",
    "    # test_size=1,\n",
    "    seed=42,\n",
    ")\n",
    "dataset = dataset[\"test\"].filter(lambda x: x[\"model_answer\"].count(\"</think>\") == 1)\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "\n",
    "correct_dataset = []\n",
    "\n",
    "for pos, item in enumerate(dataset):\n",
    "    try:\n",
    "        answer = dataset_answer_filter(item[\"answer\"])\n",
    "        model_answer = model_answer_filter(item[\"model_answer\"])\n",
    "        # print(answer, model_answer)\n",
    "        # break\n",
    "        if is_equiv(answer, model_answer):\n",
    "            correct_dataset.append(item)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(dataset), len(correct_dataset), len(correct_dataset) / len(dataset))\n",
    "\n",
    "correct_dataset = correct_dataset[:30]\n",
    "len(correct_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8bdd8",
   "metadata": {},
   "source": [
    "## Обучение по чанкам в цикле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc0804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b504f7a6924e408da1ae2b033914ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.4852, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4737, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4533, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4440, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4254, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5454, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 5/21 [00:53<02:51, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRONG 7\n",
      "Okay, so I have this magic square problem here, and I need to find the value of \\( n \\). Let me try to figure this out step by step. \n",
      "\n",
      "First, I remember that a magic square is a grid where the sums of numbers in each row, each column, and both main diagonals are equal. That common sum is called the magic constant. So, my goal is to find \\( n \\) such that all these sums are equal.\n",
      "\n",
      "Looking at the Asymptote code provided, it seems like the magic square is a 3x3 grid. Let me visualize it based on the labels given:\n",
      "\n",
      "- The top row has three cells: \\( n-3 \\), \\( 3 \\), and \\( n+1 \\).\n",
      "- The middle row has \\( n+2 \\), \\( 2n-9 \\), and \\( 1 \\).\n",
      "- The bottom row has \\( 2 \\), \\( n \\), and \\( n-1 \\).\n",
      "\n",
      "First, I'll write down the magic square structure:\n",
      "\n",
      "\\[\n",
      "\\begin{array}{|c|c|c|}\n",
      "\\hline\n",
      "n-3 & 3 & n+1 \\\\\n",
      "\\hline\n",
      "n+2 & 2n-9 & 1 \\\\\n",
      "\\hline\n",
      "2 & n & n-1 \\\\\n",
      "\\hline\n",
      "\\end{array}\n",
      "\\]\n",
      "\n",
      "In a magic square, all rows, columns, and the two main diagonals must sum to the same value, called the magic constant. Let's denote this constant as \\( S \\).\n",
      "\n",
      "I'll start by calculating the sum of the first row:\n",
      "\n",
      "\\[\n",
      "(n - 3) + 3 + (n + 1) = 2n + 1\n",
      "\\]\n",
      "\n",
      "So, the magic constant \\( S \\) is \\( 2n + 1 \\).\n",
      "\n",
      "Next, I'll check the second row:\n",
      "\n",
      "\\[\n",
      "(n + 2) + (2n - 9) + 1 = 3n - 6\n",
      "\\]\n",
      "\n",
      "Since this must also equal \\( S \\), I set up the equation:\n",
      "\n",
      "\\[\n",
      "3n - 6 = 2n + 1\n",
      "\\]\n",
      "\n",
      "Solving for \\( n \\):\n",
      "\n",
      "\\[\n",
      "n = 7\n",
      "\\]\n",
      "\n",
      "To ensure consistency, I'll verify the third row:\n",
      "\n",
      "\\[\n",
      "2 + n + (n - 1) = 2n + 1\n",
      "\\]\n",
      "\n",
      "Substituting \\( n = 7 \\):\n",
      "\n",
      "\\[\n",
      "2 + 7 + 6 = 15\n",
      "\\]\n",
      "\n",
      "Which matches the magic constant \\( S = 15 \\).\n",
      "\n",
      "Finally, I'll confirm the columns and diagonals:\n",
      "\n",
      "- **First column:**\n",
      "  \\[\n",
      "  (n - 3) + (n + 2) + 2 = 2n + 1\n",
      "  \\]\n",
      "  Substituting \\( n = 7 \\):\n",
      "  \\[\n",
      "  4 + 9 + 2 = 15\n",
      "  \\]\n",
      "  \n",
      "- **Second column:**\n",
      "  \\[\n",
      "  3 + (2n - 9) + n = 3n - 6\n",
      "  \\]\n",
      "  Substituting \\( n = 7 \\):\n",
      "  \\[\n",
      "  21 - 6 = 15\n",
      "  \\]\n",
      "  \n",
      "- **Third column:**\n",
      "  \\[\n",
      "  (n + 1) + (n - 1) + (n - 1) = 3n - 1\n",
      "  \\]\n",
      "  Substituting \\( n = 7 \\):\n",
      "  \\[\n",
      "  21 - 1 = 20\n",
      "  \\]\n",
      "  Wait, that doesn't add up to 15. Did I make a mistake?\n",
      "\n",
      "Let me check the third column again:\n",
      "\\[\n",
      "(n + 1) + (n - 1) + (n - 1) = 3n - 1\n",
      "\\]\n",
      "Yes, that's correct. Hmm, maybe I chose the wrong value for \\( n \\). Let me try \\( n = 5 \\):\n",
      "\\[\n",
      "First row: (5 - 3) + 3 + (n + 1) = 2 + 3 + 6 = 11\n",
      "\\]\n",
      "That's not 15 either. Maybe \\( n = 6 \\):\n",
      "\\[\n",
      "First row: (6 - 3) + 3 + (6 + 1) = 3 + 3 + 7 = 13\n",
      "\\]\n",
      "Still not 15. Let me try \\( n = 4 \\):\n",
      "\\[\n",
      "First row: (4 - 3) + 3 + (4 + 1) = 1 + 3 + 5 = 9\n",
      "\\]\n",
      "Nope, that's too low. Maybe \\( n = 8 \\):\n",
      "\\[\n",
      "First row: (8 - 3) + 3 + (8 + 1) = 5 + 3 + 9 = 17\n",
      "\\]\n",
      "Still not 15. Hmm, maybe I made a mistake with the first row. Let me check again:\n",
      "\\[\n",
      "First row: (n - 3) + 3 + (n + 1) = 2n - 1\n",
      "\\]\n",
      "Setting \\( 2n - 1 = 15 \\) gives \\( n = 8 \\). Wait, I think I added incorrectly before. So, \\( n = 8 \\) is correct.\n",
      "</think>\n",
      "\n",
      "To determine the value of \\( n \\) in the magic square, we can use the property that the sum of each row, column, and the two main diagonals must be equal. Let's denote the magic constant (the common sum) as \\( S \\).\n",
      "\n",
      "Given the magic square:\n",
      "\n",
      "\\[\n",
      "\\begin{array}{|c|c|c|}\n",
      "\\hline\n",
      "n - 3 & 3 & n + 1 \\\\\n",
      "\\hline\n",
      "n + 2 & 2n - 9 & 1 \\\\\n",
      "\\hline\n",
      "2 & n & n - 1 \\\\\n",
      "\\hline\n",
      "\\end{array}\n",
      "\\]\n",
      "\n",
      "Let's calculate the sum of the first row:\n",
      "\n",
      "\\[\n",
      "(n - 3) + 3 + (n + 1) = 2n - 1\n",
      "\\]\n",
      "\n",
      "Since this is a magic square, the sum of the first row equals the magic constant \\( S \\):\n",
      "\n",
      "\\[\n",
      "2n - 1 = S \\quad \\text{(1)}\n",
      "\\]\n",
      "\n",
      "Next, let's calculate the sum of the first column:\n",
      "\n",
      "\\[\n",
      "(n - 3) + (n + 2) + 2 = 2n + 1\n",
      "\\]\n",
      "\n",
      "Setting this equal to \\( S \\):\n",
      "\n",
      "\\[\n",
      "2n + 1 = S \\quad \\text{(2)}\n",
      "\\]\n",
      "\n",
      "Now, we have two expressions for \\( S \\):\n",
      "\n",
      "\\[\n",
      "2n - 1 = 2n + 1\n",
      "\\]\n",
      "\n",
      "Subtracting \\( 2n \\) from both sides:\n",
      "\n",
      "\\[\n",
      "-1 = 1\n",
      "\\]\n",
      "\n",
      "This is a contradiction, indicating an error in our approach. Let's instead calculate the sum of the main diagonal:\n",
      "\n",
      "\\[\n",
      "(n - 3) + (2n - 9) + (n - 1) = 4n - 13\n",
      "\\]\n",
      "\n",
      "Setting this equal to \\( S \\):\n",
      "\n",
      "\\[\n",
      "4n - 13 = S \\quad \\text{(3)}\n",
      "\\]\n",
      "\n",
      "Now, we have three expressions for \\( S \\):\n",
      "\n",
      "1. \\( 2n - 1 = S \\) (from the first row)\n",
      "2. \\( 2n + 1 = S \\) (from the first column)\n",
      "3. \\( 4n - 13 = S \\) (from the main diagonal)\n",
      "\n",
      "Setting equations (1) and (2) equal:\n",
      "\n",
      "\\[\n",
      "2n - 1 = 2n + 1\n",
      "\\]\n",
      "\n",
      "Subtracting \\( 2n \\) from both sides:\n",
      "\n",
      "\\[\n",
      "-1 = 1\n",
      "\\]\n",
      "\n",
      "This is still a contradiction. Let's instead use the second row to find another equation:\n",
      "\n",
      "\\[\n",
      "(n + 2) + (2n - 9) + 1 = 3n - 6\n",
      "\\]\n",
      "\n",
      "Setting this equal to \\( S \\):\n",
      "\n",
      "\\[\n",
      "3n - 6 = S \\quad \\text{(4)}\n",
      "\\]\n",
      "\n",
      "Now, we have:\n",
      "\n",
      "1. \\( 2n - 1 = S \\) (from the first row)\n",
      "2. \\( 3n - 6 = S \\) (from the second row)\n",
      "3. \\( 4n - 13 = S \\) (from the main diagonal)\n",
      "\n",
      "Setting equations (1) and (4) equal:\n",
      "\n",
      "\\[\n",
      "2n - 1 = 3n - 6\n",
      "\\]\n",
      "\n",
      "Subtracting \\( 2n \\) from both sides:\n",
      "\n",
      "\\[\n",
      "-1 = n - 6\n",
      "\\]\n",
      "\n",
      "Adding 6 to both sides:\n",
      "\n",
      "\\[\n",
      "n = 5\n",
      "\\]\n",
      "\n",
      "Let's verify this value by checking all rows, columns, and diagonals:\n",
      "\n",
      "1. **First Row:**\n",
      "   \\[\n",
      "   (5 - 3) + 3 + (5 + 1) = 2 + 3 + 6 = 11\n",
      "   \\]\n",
      "   \n",
      "2. **Second Row:**\n",
      "   \\[\n",
      "   (5 + 2) + (2 \\times 5 - 9) + 1 = 7 + 1 + 1 = 9\n",
      "   \\]\n",
      "   \n",
      "3. **Third Row:**\n",
      "   \\[\n",
      "   2 + 5 + (5 - 1) = 2 + 5 + 4 = 11\n",
      "   \\]\n",
      "   \n",
      "4. **First Column:**\n",
      "   \\[\n",
      "   (5 - 3) + (5 + 2) + 2 = 2 + 7 + 2 = 11\n",
      "   \\]\n",
      "   \n",
      "5. **Second Column:**\n",
      "   \\[\n",
      "   3 + (2 \\times 5 - 9) + 5 = 3 + 1 + 5 = 9\n",
      "   \\]\n",
      "   \n",
      "6. **Third Column:**\n",
      "   \\[\n",
      "   (5 + 1) + 1 + (5 - 1) = 6 + 1 + 4 = 11\n",
      "   \\]\n",
      "   \n",
      "7. **Main Diagonal (Top-left to Bottom-right):**\n",
      "   \\[\n",
      "   (5 - 3) + (2 \\times 5 - 9) + (5 - 1) = 2 + 1 + 4 = 7\n",
      "   \\]\n",
      "   \n",
      "   **Wait a minute!** The main diagonal sums to 7, but all other rows, columns, and the other diagonal should sum to 11. There's an inconsistency here. Let's check the other diagonal.\n",
      "\n",
      "8. **Other Diagonal (Top-right to Bottom-left):**\n",
      "   \\[\n",
      "   (5 + 1) + (2n - 9) + 2 = 6 + 1 + 2 = 9\n",
      "   \\]\n",
      "   \n",
      "   Again, this sums to 9, not 11. This indicates an error in our calculation of \\( n \\). Let's revisit the earlier steps.\n",
      "\n",
      "Upon reviewing, we realize that the main diagonal calculation was incorrect. The correct main diagonal should be:\n",
      "\n",
      "\\[\n",
      "(n - 3) + (2n - 9) + (n - 1) = 4n - 13\n",
      "\\]\n",
      "\n",
      "Setting this equal to \\( S \\):\n",
      "\n",
      "\\[\n",
      "4n - 13 = S\n",
      "\\]\n",
      "\n",
      "From the first row:\n",
      "\n",
      "\\[\n",
      "2n - 1 = S\n",
      "\\]\n",
      "\n",
      "Setting equal:\n",
      "\n",
      "\\[\n",
      "4n - 13 = 2n - 1\n",
      "\\]\n",
      "\n",
      "Subtracting \\( 2n \\) from both sides:\n",
      "\n",
      "\\[\n",
      "2n - 13 = -1\n",
      "\\]\n",
      "\n",
      "Adding 13 to both sides:\n",
      "\n",
      "\\[\n",
      "2n = 12\n",
      "\\]\n",
      "\n",
      "Dividing by 2:\n",
      "\n",
      "\\[\n",
      "n = 6\n",
      "\\]\n",
      "\n",
      "Now, let's verify with \\( n = 6 \\):\n",
      "\n",
      "1. **First Row:**\n",
      "   \\[\n",
      "   (6 - 3) + 3 + (6 + 1) = 3 + 3 + 7 = 13\n",
      "   \\]\n",
      "   \n",
      "2. **Second Row:**\n",
      "   \\[\n",
      "   (6 + 2) + (2 \\times 6 - 9) + 1 = 8 + 3 + 1 = 12\n",
      "   \\]\n",
      "   \n",
      "3. **Third Row:**\n",
      "   \\[\n",
      "   2 + 6 + (6 - 1) = 2 + 6 + 5 = 13\n",
      "   \\]\n",
      "   \n",
      "4. **First Column:**\n",
      "   \\[\n",
      "   (6 - 3) + (6 + 2) + 2 = 3 + 8 + 2 = 13\n",
      "   \\]\n",
      "   \n",
      "5. **Second Column:**\n",
      "   \\[\n",
      "   3 + (2 \\times 6 - 9) + 6 = 3 + 3 + 6 = 12\n",
      "   \\]\n",
      "   \n",
      "6. **Third Column:**\n",
      "   \\[\n",
      "   (6 + 1) + 1 + (6 - 1) = 7 + 1 + 5 = 13\n",
      "   \\]\n",
      "   \n",
      "7. **Main Diagonal (Top-left to Bottom-right):**\n",
      "   \\[\n",
      "   (6 - 3) + (2 \\times 6 - 9) + (6 - 1) = 3 + 3 + 5 = 11\n",
      "   \\]\n",
      "   \n",
      "   **Wait, this still doesn't add up to 13.** There's a mistake here. Let's recalculate the main diagonal:\n",
      "\n",
      "\\[\n",
      "(6 - 3) + (2 \\times 6 - 9) + (6 - 1) = 3 + 3 + 5 = 11\n",
      "\\]\n",
      "\n",
      "This still doesn't match the magic constant of 13. Let's check the other diagonal:\n",
      "\n",
      "8. **Other Diagonal (Top-right to Bottom-left):**\n",
      "   \\[\n",
      "   (6 + 1) + (2 \\times 6 - 9) + 2 = 7 + 3 + 2 = 12\n",
      "   \\]\n",
      "\n",
      "This also doesn't match. It seems there's an inconsistency in the magic square setup or an error in the given values. However, based on the corrected value of \\( n = 6 \\), the magic constant should be 13, but the diagonals are not matching. This suggests that either the given values are incorrect or there's a miscalculation.\n",
      "\n",
      "Given the complexity and the potential for multiple errors, it's clear that \\( n = 6 \\) doesn't satisfy all conditions perfectly. Therefore, it's possible that there's a mistake in the problem statement or the given values. However, based on the first row and column calculations, \\( n = 6 \\) is the most consistent value, even though the diagonals don't add up correctly.\n",
      "\n",
      "**Final Answer:**\n",
      "\\boxed{6}<｜end▁of▁sentence｜>\n",
      "вопрос+сжатые+сгенерированные=2528, всего_сгенерированно_токенов=3164 оригинальная_генерация=1959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.6115, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.6084, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1/21 [00:07<02:34,  7.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 89\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;66;03m# first time generation\u001b[39;00m\n\u001b[1;32m     83\u001b[0m         inputs_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m     84\u001b[0m             [\n\u001b[1;32m     85\u001b[0m                 input_ids_embeds,\n\u001b[1;32m     86\u001b[0m             ],\n\u001b[1;32m     87\u001b[0m             dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     88\u001b[0m         )\n\u001b[0;32m---> 89\u001b[0m     generated_ids_new \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[1;32m    100\u001b[0m generated_result \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids_new[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3218\u001b[0m     outputs,\n\u001b[1;32m   3219\u001b[0m     model_kwargs,\n\u001b[1;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3221\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:856\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:579\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    568\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    569\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m         position_embeddings,\n\u001b[1;32m    577\u001b[0m     )\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:260\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:192\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m--> 192\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# main diff with Llama\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    205\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/integrations/sdpa_attention.py:30\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msdpa_attention_forward\u001b[39m(\n\u001b[1;32m     19\u001b[0m     module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m     20\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     28\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_key_value_groups\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         value \u001b[38;5;241m=\u001b[39m repeat_kv(value, module\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m     33\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m attention_mask\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/integrations/sdpa_attention.py:14\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_rep \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[0;32m---> 14\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mreshape(batch, num_key_value_heads \u001b[38;5;241m*\u001b[39m n_rep, slen, head_dim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as text_tqdm\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    tokenize_single_turn,\n",
    "    EOS_TOKEN_ID,\n",
    "    END_THINK_ID,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "max_new_tokens = 400\n",
    "compression_tokens_amount = 2\n",
    "max_total_tokens = 4096\n",
    "max_total_steps = max_total_tokens // max_new_tokens + 1\n",
    "\n",
    "evaluation_dataset = []\n",
    "correct_items = 0\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for dataset_pos in tqdm(range(len(correct_dataset))):\n",
    "    # for dataset_pos in tqdm(range(1, len(correct_dataset))):\n",
    "    tokenized_turn = tokenize_single_turn(\n",
    "        question=base_prompt.format(question=correct_dataset[dataset_pos][\"problem\"]),\n",
    "        answer=correct_dataset[dataset_pos][\"model_answer\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    for key in tokenized_turn.keys():\n",
    "        tokenized_turn[key] = torch.tensor(tokenized_turn[key])\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    content_compression_mask = tokenized_turn[\"content_compression_mask\"]\n",
    "\n",
    "    input_part_end = (content_compression_mask == 0).nonzero()[-3][0]\n",
    "    # get only question part\n",
    "    question_input_ids = (\n",
    "        tokenized_turn[\"input_ids\"][: int(input_part_end) + 1].unsqueeze(0).cuda()\n",
    "    )\n",
    "    # print(tokenizer.decode(question_input_ids[-1]))\n",
    "\n",
    "    ######## start loop generation\n",
    "    ########\n",
    "    compression_loop = True\n",
    "    input_ids_embeds = model.get_input_embeddings()(question_input_ids)\n",
    "    compression_part = torch.tensor([[0]])\n",
    "    generated_ids_new = None\n",
    "    generated_embeds = None\n",
    "    generated_embeds_prev = None\n",
    "    generated_ids_new_prev = None\n",
    "    end_of_think = False\n",
    "    total_generated_text = \"\"\n",
    "\n",
    "    for compression_step in text_tqdm(range(max_total_steps)):\n",
    "        ######## generate new tokens\n",
    "        ########\n",
    "        inputs_embeds = None\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if compression_part.shape[1] >= compression_tokens_amount:\n",
    "                generated_embeds_prev = generated_embeds[\n",
    "                    :, -(max_new_tokens // 2) :, :\n",
    "                ].clone()\n",
    "                inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds,\n",
    "                        compression_part,\n",
    "                        generated_embeds_prev,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            else:\n",
    "                # first time generation\n",
    "                inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            generated_ids_new = model.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=torch.ones(\n",
    "                    inputs_embeds.shape[:2],\n",
    "                    device=\"cuda\",\n",
    "                ).long(),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                use_cache=compression_step > 0,\n",
    "            )\n",
    "            # break\n",
    "        generated_result = tokenizer.decode(generated_ids_new[-1])\n",
    "        # print(generated_result)\n",
    "        total_generated_text += generated_result\n",
    "        print(\"=\" * 50)\n",
    "        generated_embeds = model.get_input_embeddings()(generated_ids_new)\n",
    "        if END_THINK_ID in generated_ids_new[-1].tolist():\n",
    "            end_of_think = True\n",
    "            break\n",
    "\n",
    "        ########\n",
    "        ######## get original language loss\n",
    "        ########\n",
    "        labels = None\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            labels = torch.cat(\n",
    "                [\n",
    "                    question_input_ids.cuda(),\n",
    "                    ((torch.ones(compression_part.shape[:2]) * -100).long()).cuda(),\n",
    "                    (\n",
    "                        (torch.ones(generated_embeds_prev.shape[:2]) * -100).long()\n",
    "                    ).cuda(),\n",
    "                    generated_ids_new.cuda(),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            # first time generation\n",
    "            labels = torch.cat(\n",
    "                [\n",
    "                    question_input_ids.cuda(),\n",
    "                    generated_ids_new.cuda(),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        question_content_mask = content_compression_mask[\n",
    "            : int(input_part_end) + 1\n",
    "        ].clone()\n",
    "        question_content_mask[question_content_mask == 0] = 4\n",
    "        question_content_mask[question_content_mask == 1] = 0\n",
    "        question_content_mask[question_content_mask == 4] = 1\n",
    "\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            train_content_mask_new = torch.cat(\n",
    "                [\n",
    "                    question_content_mask,\n",
    "                    torch.zeros(compression_part.shape[1]),\n",
    "                    torch.zeros(generated_embeds_prev.shape[1]),\n",
    "                    torch.zeros(generated_ids_new.shape[1] // 2),\n",
    "                    torch.ones(generated_ids_new.shape[1] // 2),\n",
    "                ]\n",
    "            ).long()\n",
    "        else:\n",
    "            train_content_mask_new = torch.cat(\n",
    "                [\n",
    "                    question_content_mask,\n",
    "                    torch.ones(generated_ids_new.shape[1] // 2) * 0,\n",
    "                    torch.ones(generated_ids_new.shape[1] // 2),\n",
    "                ]\n",
    "            ).long()\n",
    "\n",
    "        generated_ids_new_prev = generated_ids_new.clone()\n",
    "        # generated_embeds = model.get_input_embeddings()(generated_ids_new)\n",
    "\n",
    "        new_input_embeds = None\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            new_input_embeds = torch.cat(\n",
    "                [\n",
    "                    input_ids_embeds.cuda(),\n",
    "                    compression_part.cuda(),\n",
    "                    generated_embeds_prev.cuda(),\n",
    "                    generated_embeds,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            new_input_embeds = torch.cat(\n",
    "                [\n",
    "                    input_ids_embeds,\n",
    "                    generated_embeds,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        labels[:, train_content_mask_new == 0] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            original_loss = model(\n",
    "                inputs_embeds=new_input_embeds,\n",
    "                labels=labels,\n",
    "            ).loss\n",
    "        print(\"original_loss\", original_loss)\n",
    "        ########\n",
    "        ######## generate compress embeddings\n",
    "        ########\n",
    "        compression_tensor = torch.nn.Parameter(\n",
    "            torch.rand_like(\n",
    "                new_input_embeds[:, :compression_tokens_amount, :],\n",
    "            )\n",
    "            * model.get_input_embeddings().weight.data.std(),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "        question_labels = question_input_ids.clone().cuda()\n",
    "        question_labels[0][question_content_mask == 0] = -100\n",
    "        compression_tensor_labels = (\n",
    "            (torch.ones(compression_tensor.shape[:2]) * -100).long().cuda()\n",
    "        )\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            compression_part_labels = (\n",
    "                (torch.ones(compression_part.shape[:2]) * -100).long().cuda()\n",
    "            )\n",
    "            compressed_labels = torch.cat(\n",
    "                [\n",
    "                    question_labels,\n",
    "                    compression_part_labels,\n",
    "                    compression_tensor_labels,\n",
    "                    generated_ids_new[:, -(max_new_tokens // 2) :],\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "        else:\n",
    "            compressed_labels = torch.cat(\n",
    "                [\n",
    "                    question_labels,\n",
    "                    compression_tensor_labels,\n",
    "                    generated_ids_new[:, -(max_new_tokens // 2) :],\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "        ########\n",
    "        ######## train\n",
    "        ########\n",
    "        epoch_amount = 100\n",
    "\n",
    "        optimizer = torch.optim.Adam([compression_tensor], lr=0.1)\n",
    "        acclumulation_steps = 1\n",
    "        for epoch in range(epoch_amount):\n",
    "            if compression_part.shape[1] >= compression_tokens_amount:\n",
    "                compressed_inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds.detach(),\n",
    "                        compression_part.detach(),\n",
    "                        compression_tensor,\n",
    "                        generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            else:\n",
    "                compressed_inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds.detach(),\n",
    "                        compression_tensor,\n",
    "                        generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            compression_loss = model(\n",
    "                inputs_embeds=compressed_inputs_embeds,\n",
    "                labels=compressed_labels,\n",
    "            ).loss\n",
    "            compression_loss.backward()\n",
    "            if (epoch + 1) % acclumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if compression_loss.item() <= original_loss.item():\n",
    "                break\n",
    "            # if compression_loss.item() <= (original_loss.item() + 0.01):\n",
    "            #     break\n",
    "            # if (compression_loss.item() + 0.05) <= original_loss.item():\n",
    "            #     break\n",
    "        print(\"compression_loss\", compression_loss)\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            compression_part = torch.cat(\n",
    "                [\n",
    "                    compression_part,\n",
    "                    compression_tensor.detach(),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            compression_part = compression_tensor.detach()\n",
    "    # if end_of_think:\n",
    "    inputs_embeds = torch.cat(\n",
    "        [\n",
    "            inputs_embeds,\n",
    "            generated_embeds,\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    final_response = model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=torch.ones(\n",
    "            inputs_embeds.shape[:2],\n",
    "            device=\"cuda\",\n",
    "        ).long(),\n",
    "        max_new_tokens=(max_total_steps - compression_step) * max_new_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    final_answer = tokenizer.decode(final_response[-1])\n",
    "    total_generated_text += final_answer\n",
    "    # print(\"FINAL ANSWER\", final_answer)\n",
    "\n",
    "    gold_answer = correct_dataset[dataset_pos][\"answer\"]\n",
    "    answer = dataset_answer_filter(gold_answer)\n",
    "    # print(\"GOLD ANSWER\", answer)\n",
    "    model_answer = model_answer_filter(total_generated_text)\n",
    "    if is_equiv(answer, model_answer):\n",
    "        correct_items += 1\n",
    "        print(\"CORRECT\")\n",
    "    else:\n",
    "        print(\"WRONG\", gold_answer)\n",
    "        print(total_generated_text)\n",
    "\n",
    "    compressed_total_len = inputs_embeds.shape[1] + final_response.shape[1]\n",
    "    total_generated_tokens = final_response.shape[1] + max_new_tokens * (\n",
    "        compression_step + 1\n",
    "    )\n",
    "    original_total_len = len(\n",
    "        tokenizer.encode(\n",
    "            correct_dataset[dataset_pos][\"model_answer\"],\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"вопрос+сжатые+сгенерированные={compressed_total_len}, всего_сгенерированно_токенов={total_generated_tokens} оригинальная_генерация={original_total_len}\"\n",
    "    )\n",
    "    evaluation_dataset.append(\n",
    "        {\n",
    "            \"original_total_len\": original_total_len,\n",
    "            \"compressed_total_len\": compressed_total_len,\n",
    "        }\n",
    "    )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f082d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13392857142857142, 0.11607142857142858, 0.8666666666666667)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_dataset) / len(dataset), correct_items / len(dataset), correct_items / len(\n",
    "    correct_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52c547f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56056, 30087, 0.5367311260168403)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_total_len = 0\n",
    "compressed_total_len = 0\n",
    "for item in evaluation_dataset:\n",
    "    original_total_len += item[\"original_total_len\"]\n",
    "    compressed_total_len += item[\"compressed_total_len\"]\n",
    "original_total_len, compressed_total_len, compressed_total_len / original_total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0401256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (56056, 21924, 0.39110889110889113) - 0.8333333333333334, 200 токенов, 4 сжимающих\n",
    "# (56056, 29497, 0.5262059369202227) - 0.9666666666666667, 400 токенов, 16 сжимающих\n",
    "# (56056, 27994, 0.499393463679178) - 0.9, 400 токенов, 8 сжимающих\n",
    "# (56056, 23062, 0.4114100185528757) - 0.9, 200 токенов, 16 сжимающих\n",
    "# (56056, 30087, 0.5367311260168403) - 0.8666666666666667, 400, 32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
