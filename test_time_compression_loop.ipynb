{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffdb004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "224 202 0.9017857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "# model = model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    # \"dim/hendrycks_math_train_12k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096\"\n",
    "    # \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    # \"dim/hendrycks_math_train_1k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    ")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    # test_size=250,\n",
    "    test_size=350,\n",
    "    # test_size=999,\n",
    "    # test_size=1,\n",
    "    seed=42,\n",
    ")\n",
    "dataset = dataset[\"test\"].filter(lambda x: x[\"model_answer\"].count(\"</think>\") == 1)\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "\n",
    "correct_dataset = []\n",
    "\n",
    "for pos, item in enumerate(dataset):\n",
    "    try:\n",
    "        answer = dataset_answer_filter(item[\"answer\"])\n",
    "        model_answer = model_answer_filter(item[\"model_answer\"])\n",
    "        # print(answer, model_answer)\n",
    "        # break\n",
    "        if is_equiv(answer, model_answer):\n",
    "            correct_dataset.append(item)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(dataset), len(correct_dataset), len(correct_dataset) / len(dataset))\n",
    "\n",
    "correct_dataset = correct_dataset[:30]\n",
    "len(correct_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f058fd3",
   "metadata": {},
   "source": [
    "## test time train generation (single train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "303e6e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e664c45096f94299b7ca356cecab643b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_loss tensor(0.2270, device='cuda:0')\n",
      "compression_loss tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1737, вопрос+сжатые+сгенерированные=1953 оригинальная генерация=1959\n",
      "original_loss tensor(0.2759, device='cuda:0')\n",
      "compression_loss tensor(0.2916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=625, вопрос+сжатые+сгенерированные=841 оригинальная генерация=1125\n",
      "original_loss tensor(0.2294, device='cuda:0')\n",
      "compression_loss tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1173, вопрос+сжатые+сгенерированные=1389 оригинальная генерация=1548\n",
      "original_loss tensor(0.2595, device='cuda:0')\n",
      "compression_loss tensor(0.2987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1459, вопрос+сжатые+сгенерированные=1675 оригинальная генерация=3960\n",
      "original_loss tensor(0.2939, device='cuda:0')\n",
      "compression_loss tensor(0.3127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=204, вопрос+сжатые+сгенерированные=420 оригинальная генерация=748\n",
      "original_loss tensor(0.2882, device='cuda:0')\n",
      "compression_loss tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=517, вопрос+сжатые+сгенерированные=733 оригинальная генерация=1080\n",
      "original_loss tensor(0.2947, device='cuda:0')\n",
      "compression_loss tensor(0.3125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=986, вопрос+сжатые+сгенерированные=1202 оригинальная генерация=1639\n",
      "original_loss tensor(0.2449, device='cuda:0')\n",
      "compression_loss tensor(0.2649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1360, вопрос+сжатые+сгенерированные=1576 оригинальная генерация=1732\n",
      "original_loss tensor(0.2826, device='cuda:0')\n",
      "compression_loss tensor(0.2975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1189, вопрос+сжатые+сгенерированные=1405 оригинальная генерация=1198\n",
      "original_loss tensor(0.2778, device='cuda:0')\n",
      "compression_loss tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=612, вопрос+сжатые+сгенерированные=828 оригинальная генерация=2314\n",
      "original_loss tensor(0.2608, device='cuda:0')\n",
      "compression_loss tensor(0.2761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=317, вопрос+сжатые+сгенерированные=533 оригинальная генерация=1262\n",
      "original_loss tensor(0.2694, device='cuda:0')\n",
      "compression_loss tensor(0.2874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=824, вопрос+сжатые+сгенерированные=1040 оригинальная генерация=1176\n",
      "original_loss tensor(0.2898, device='cuda:0')\n",
      "compression_loss tensor(0.3092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=407, вопрос+сжатые+сгенерированные=623 оригинальная генерация=1482\n",
      "original_loss tensor(0.3246, device='cuda:0')\n",
      "compression_loss tensor(0.3306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=468, вопрос+сжатые+сгенерированные=684 оригинальная генерация=814\n",
      "original_loss tensor(0.2591, device='cuda:0')\n",
      "compression_loss tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "WRONG x^5 - x^4 + x^3 - x^2 + x - 1\n",
      ", there are no more terms to bring down. So, the remainder is \\( -x^5 - 3 \\). \n",
      "\n",
      "Since the degree of the remainder \\( (-x^5 - 3) \\) is 5, which is greater than the degree of the divisor \\( (x + 1) \\), which is 1, I can't continue the division further. \n",
      "\n",
      "Therefore, the quotient is \\( x^5 \\) and the remainder is \\( -x^5 - 3 \\). \n",
      "\n",
      "Wait, that doesn't seem right. If I have a remainder of the same degree as the divisor, I should have a negative sign. Let me check my steps again. \n",
      "\n",
      "When I subtracted \\( x^6 + x^5 \\) from \\( x^6 - 3 \\), I got \\( -x^5 - 3 \\). That seems correct. So, the remainder is indeed \\( -x^5 - 3 \\). \n",
      "\n",
      "But the problem asks for the quotient when \\( x^6 - 3 \\) is divided by \\( x + 1 \\). So, the quotient is \\( x^5 \\) and the remainder is \\( -x^5 - 3 \\). \n",
      "\n",
      "Wait, but if I write the division as \\( x^6 - 3 = (x + 1)(x^5) + (-x^5 - 3) \\), that seems correct. \n",
      "\n",
      "Alternatively, maybe I can factor the remainder. Let me see: \\( -x^5 - 3 = - (x^5 + 3) \\). Hmm, not sure if that helps. \n",
      "\n",
      "Alternatively, perhaps I made a mistake in the initial step. Let me try another approach. \n",
      "\n",
      "Using polynomial long division, I can write \\( x^6 - 3 \\) divided by \\( x + 1 \\). \n",
      "\n",
      "First, divide \\( x^6 \\) by \\( x \\) to get \\( x^5 \\). Multiply \\( x + 1 \\) by \\( x^5 \\) to get \\( x^6 + x^5 \\). Subtract that from \\( x^6 - 3 \\) to get \\( -x^5 - 3 \\). \n",
      "\n",
      "Now, divide \\( -x^5 \\) by \\( x \\) to get \\( -x^4 \\). Multiply \\( x + 1 \\) by \\( -x^4 \\) to get \\( -x^5 - x^4 \\). Subtract that from \\( -x^5 - 3 \\) to get \\( x^4 - 3 \\). \n",
      "\n",
      "Next, divide \\( x^4 \\) by \\( x \\) to get \\( x^3 \\). Multiply \\( x + 1 \\) by \\( x^3 \\) to get \\( x^4 + x^3 \\). Subtract that from \\( x^4 - 3 \\) to get \\( -x^3 - 3 \\). \n",
      "\n",
      "Divide \\( -x^3 \\) by \\( x \\) to get \\( -x^2 \\). Multiply \\( x + 1 \\) by \\( -x^2 \\) to get \\( -x^3 - x^2 \\). Subtract that from \\( -x^3 - 3 \\) to get \\( x^2 - 3 \\). \n",
      "\n",
      "Divide \\( x^2 \\) by \\( x \\) to get \\( x \\). Multiply \\( x + 1 \\) by \\( x \\) to get \\( x^2 + x \\). Subtract that from \\( x^2 - 3 \\) to get \\( -x - 3 \\). \n",
      "\n",
      "Divide \\( -x \\) by \\( x \\) to get \\( -1 \\). Multiply \\( x + 1 \\) by \\( -1 \\) to get \\( -x - 1 \\). Subtract that from \\( -x - 3 \\) to get \\( -2 \\). \n",
      "\n",
      "So, the quotient is \\( x^5 - x^4 + x^3 - x^2 + x - 1 \\) and the remainder is \\( -2 \\). \n",
      "\n",
      "Wait, that's different from my previous result. So, which one is correct? \n",
      "\n",
      "Let me verify by multiplying the divisor \\( x + 1 \\) by the quotient \\( x^5 - x^4 + x^3 - x^2 + x - 1 \\) and adding the remainder \\( -2 \\). \n",
      "\n",
      "Compute \\( (x + 1)(x^5 - x^4 + x^3 - x^2 + x - 1) \\). \n",
      "\n",
      "First, multiply \\( x \\) by each term: \\( x^6 - x^5 + x^4 - x^3 + x^2 - x \\). \n",
      "\n",
      "Then, multiply \\( 1 \\) by each term: \\( x^5 - x^4 + x^3 - x^2 + x - 1 \\). \n",
      "\n",
      "Add them together: \n",
      "\n",
      "\\( x^6 - x^5 + x^4 - x^3 + x^2 - x + x^5 - x^4 + x^3 - x^2 + x - 1 \\). \n",
      "\n",
      "Combine like terms: \n",
      "\n",
      "- \\( x^6 \\)\n",
      "- \\( (-x^5 + x^5) = 0 \\)\n",
      "- \\( (x^4 - x^4) = 0 \\)\n",
      "- \\( (-x^3 + x^3) = 0 \\)\n",
      "- \\( (x^2 - x^2) = 0 \\)\n",
      "- \\( (-x + x) = 0 \\)\n",
      "- \\( -1 \\)\n",
      "\n",
      "So, the result is \\( x^6 - 1 \\). \n",
      "\n",
      "But the original dividend is \\( x^6 - 3 \\). So, the remainder is \\( -1 \\), not \\( -2 \\). \n",
      "\n",
      "Wait, so my previous step had a mistake. Let me go back. \n",
      "\n",
      "After dividing \\( -x^3 \\) by \\( x \\), I got \\( -x^2 \\). Multiply \\( x + 1 \\) by \\( -x^2 \\) to get \\( -x^3 - x^2 \\). Subtract that from \\( -x^3 - 3 \\) to get \\( x^2 - 3 \\). \n",
      "\n",
      "Then, divide \\( x^2 \\) by \\( x \\) to get \\( x \\). Multiply \\( x + 1 \\) by \\( x \\) to get \\( x^2 + x \\). Subtract that from \\( x^2 - 3 \\) to get \\( -x - 3 \\). \n",
      "\n",
      "Divide \\( -x \\) by \\( x \\) to get \\( -1 \\). Multiply \\( x + 1 \\) by \\( -1 \\) to get \\( -x - 1 \\). Subtract that from \\( -x - 3 \\) to get \\( -2 \\). \n",
      "\n",
      "So, the remainder is \\( -2 \\). \n",
      "\n",
      "But when I multiplied back, I got \\( x^6 - 1 \\), which is not equal to \\( x^6 - 3 \\). So, that suggests that my quotient is incorrect. \n",
      "\n",
      "Wait, perhaps I made a mistake in the signs. Let me check the subtraction steps again. \n",
      "\n",
      "After getting \\( -x^3 - 3 \\), I divide \\( -x^3 \\) by \\( x \\) to get \\( -x^2 \\). Multiply \\( x + 1 \\) by \\( -x^2 \\) to get \\( -x^3 - x^2 \\). Subtract that from \\( -x^3 - 3 \\): \n",
      "\n",
      "\\( (-x^3 - 3) - (-x^3 - x^2) = (-x^3 + x^3) + x^2 - 3 = x^2 - 3 \\). \n",
      "\n",
      "That seems correct. \n",
      "\n",
      "Then, divide \\( x^2 \\) by \\( x \\) to get \\( x \\). Multiply \\( x + 1 \\) by \\( x \\) to get \\( x^2 + x \\). Subtract that from \\( x^2 - 3 \\): \n",
      "\n",
      "\\( (x^2 - 3) - (x^2 + x) = -x - 3 \\). \n",
      "\n",
      "That also seems correct. \n",
      "\n",
      "Divide \\( -x \\) by \\( x \\) to get \\( -1 \\). Multiply \\( x + 1 \\) by \\( -1 \\) to get \\( -x - 1 \\). Subtract that from \\( -x - 3 \\): \n",
      "\n",
      "\\( (-x - 3) - (-x - 1) = (-x + x) + (-3 + 1) = -2 \\). \n",
      "\n",
      "So, the remainder is indeed \\( -2 \\). \n",
      "\n",
      "But when I multiply back, I get \\( x^6 - 1 \\), which is not equal to \\( x^6 - 3 \\). So, that suggests that my quotient is incorrect. \n",
      "\n",
      "Wait, perhaps I made a mistake in the initial step. Let me try a different approach. \n",
      "\n",
      "Alternatively, maybe I can use the Remainder Theorem. \n",
      "\n",
      "The Remainder Theorem states that the remainder of a polynomial \\( f(x) \\) divided by \\( x - a \\) is \\( f(a) \\). \n",
      "\n",
      "But in this case, the divisor is \\( x + 1 \\), which is \\( x - (-1) \\). So, the remainder should be \\( f(-1) \\). \n",
      "\n",
      "Let me compute \\( f(-1) \\): \n",
      "\n",
      "\\( f(-1) = (-1)^6 - 3 = 1 - 3 = -2 \\). \n",
      "\n",
      "So, the remainder is \\( -2 \\). \n",
      "\n",
      "Therefore, the remainder is \\( -2 \\), and the quotient is \\( x^5 - x^4 + x^3 - x^2 + x - 1 \\). \n",
      "\n",
      "But when I multiply back, I get \\( x^6 - 1 \\), which is not equal to \\( x^6 - 3 \\). So, that suggests that my quotient is incorrect. \n",
      "\n",
      "Wait, perhaps I made a mistake in the polynomial long division. Let me try again. \n",
      "\n",
      "Divide \\( x^6 - 3 \\) by \\( x + 1 \\). \n",
      "\n",
      "First term: \\( x^6 / x = x^5 \\). Multiply \\( x + 1 \\) by \\( x^5 \\) to get \\( x^6 + x^5 \\). Subtract that from \\( x^6 - 3 \\): \n",
      "\n",
      "\\( (x^6 - 3) - (x^6 + x^5) = -x^5 - 3 \\). \n",
      "\n",
      "Next term: \\( -x^5 / x = -x^4 \\). Multiply \\( x + 1 \\) by \\( -x^4 \\) to get \\( -x^5 - x^4 \\). Subtract that from \\( -x^5 - 3 \\): \n",
      "\n",
      "\\( (-x^5 - 3) - (-x^5 - x^4) = x^4 - 3 \\). \n",
      "\n",
      "Next term: \\( x^4 / x = x^3 \\). Multiply \\( x + 1 \\) by \\( x^3 \\) to get \\( x^4 + x^3 \\). Subtract that from \\( x^4 - 3 \\): \n",
      "\n",
      "\\( (x^4 - 3) - (x^4 + x^3) = -x^3 - 3 \\). \n",
      "\n",
      "Next term: \\( -x^3 / x = -x^2 \\). Multiply \\( x + 1 \\) by \\( -x^2 \\) to get \\( -x^3 - x^2 \\). Subtract that from \\( -x^3 - 3 \\): \n",
      "\n",
      "\\( (-x^3 - 3) - (-x^3 - x^2) = x^2 - 3 \\). \n",
      "\n",
      "Next term: \\( x^2 / x = x \\). Multiply \\( x + 1 \\) by \\( x \\) to get \\( x^2 + x \\). Subtract that from \\( x^2 - 3 \\): \n",
      "\n",
      "\\( (x^2 - 3) - (x^2 + x) = -x - 3 \\). \n",
      "\n",
      "Next term: \\( -x / x = -1 \\). Multiply \\( x + 1 \\) by \\( -1 \\) to get \\( -x - 1 \\). Subtract that from \\( -x - 3 \\): \n",
      "\n",
      "\\( (-x - 3) - (-x - 1) = -2 \\). \n",
      "\n",
      "So, the quotient is \\( x^5 - x^4 + x^3 - x^2 + x - 1 \\) and the remainder is \\( -2 \\). \n",
      "\n",
      "But when I multiply back, I get \\( x^6 - 1 \\), which is not equal to \\( x^6 - 3 \\). So, that suggests that my quotient is incorrect. \n",
      "\n",
      "Wait, perhaps I made a mistake in the multiplication. Let me verify. \n",
      "\n",
      "Multiply \\( x + 1 \\) by \\( x^5 - x^4 + x^3 - x^2 + x - 1 \\). \n",
      "\n",
      "First, multiply \\( x \\) by each term: \n",
      "\n",
      "\\( x^6 - x^5 + x^4 - x^3 + x^2 - x \\). \n",
      "\n",
      "Then, multiply \\( 1 \\) by each term: \n",
      "\n",
      "\\( x^5 - x^4 + x^3 - x^2 + x - 1 \\). \n",
      "\n",
      "Add them together: \n",
      "\n",
      "\\( x^6 - x^5 + x^4 - x^3 + x^2 - x + x^5 - x^4 + x^3 - x^2 + x - 1 \\). \n",
      "\n",
      "Combine like terms: \n",
      "\n",
      "- \\( x^6 \\)\n",
      "- \\( (-x^5 + x^5) = 0 \\)\n",
      "- \\( (x^4 - x^4) = 0 \\)\n",
      "- \\( (-x^3 + x^3) = 0 \\)\n",
      "- \\( (x^2 - x^2) = 0 \\)\n",
      "- \\( (-x + x) = 0 \\)\n",
      "- \\( -1 \\)\n",
      "\n",
      "So, the result is \\( x^6 - 1 \\). \n",
      "\n",
      "But the original dividend is \\( x^6 - 3 \\). So, the remainder is \\( -1 \\), not \\( -2 \\). \n",
      "\n",
      "Wait, so my previous step had a mistake. Let me go back. \n",
      "\n",
      "After getting \\( -x^3 - 3 \\), I divide \\( -x^3 \\) by \\( x \\) to get \\( -x^2 \\). Multiply \\( x + 1 \\) by \\( -x^2 \\) to get \\( -x^3 - x^2 \\). Subtract that from \\( -x^3 - 3 \\): \n",
      "\n",
      "\\( (-x^3 - 3) - (-x^3 - x^2) = x^2 - 3 \\). \n",
      "\n",
      "That seems correct. \n",
      "\n",
      "Then, divide \\( x^2 \\) by \\( x \\) to get \\( x \\). Multiply \\( x + 1 \\) by \\( x \\) to get \\( x^2 + x \\). Subtract that from \\( x^2 - 3 \\): \n",
      "\n",
      "\\( (x^2 - 3) - (x^2 + x) = -x - 3 \\). \n",
      "\n",
      "That also seems correct. \n",
      "\n",
      "Divide \\( -x \\) by \\( x \\) to get \\( -1 \\). Multiply \\( x + 1 \\) by \\( -1 \\) to get \\( -x - 1 \\). Subtract that from \\( -x - 3 \\): \n",
      "\n",
      "\\( (-x - 3) - (-x - 1) = -2 \\). \n",
      "\n",
      "So, the remainder is \\( -2 \\). \n",
      "\n",
      "But when I multiply back, I get \\( x^6 - 1 \\), which is not equal to \\( x^6 - 3 \\). So, that suggests that my quotient is incorrect. \n",
      "\n",
      "Wait, perhaps I made a mistake in the initial step. Let me try a different approach. \n",
      "\n",
      "Alternatively, maybe I can factor the polynomial. \n",
      "\n",
      "Let me consider \\( x^6 - 3 \\). \n",
      "\n",
      "I know that \\( x^6 = (x^3)^2 \\), so maybe I can write it as \\( (x^3)^2 - (\\sqrt{3})^2 \\), which factors as \\( (x^3 - \\sqrt{3})(x^3 + \\sqrt{3}) \\). \n",
      "\n",
      "But I'm not sure if that helps with division by \\( x + 1 \\). \n",
      "\n",
      "Alternatively, perhaps I can use synthetic division. \n",
      "\n",
      "Synthetic division is a shortcut for polynomial division when dividing by a binomial of the form \\( x - c \\). In this case, the divisor is \\( x + 1 \\), which is \\( x - (-1) \\). So, \\( c = -1 \\). \n",
      "\n",
      "Let me set up synthetic division with \\( c = -1 \\) and the coefficients of \\( x^6 - 3 \\). \n",
      "\n",
      "The polynomial is \\( x^6 + 0x^5 + 0x^4 + 0x^3 + 0x^2 + 0x - 3 \\). \n",
      "\n",
      "So, the coefficients are: 1 (for \\( x^6 \\)), 0 (for \\( x^5 \\)), 0 (for \\( x^4 \\)), 0 (for \\( x^3 \\)), 0 (for \\( x^2 \\)), 0 (for \\( x \\)), and -3 (constant term). \n",
      "\n",
      "Set up synthetic division:\n",
      "\n",
      "-1 | 1   0   0   0   0   0   -3\n",
      "\n",
      "Bring down the 1.\n",
      "\n",
      "Multiply 1 by -1: -1. Add to next coefficient: 0 + (-1) = -1.\n",
      "\n",
      "Multiply -1 by -1: 1. Add to next coefficient: 0 + 1 = 1.\n",
      "\n",
      "Multiply 1 by -1: -1. Add to next coefficient: 0 + (-1) = -1.\n",
      "\n",
      "Multiply -1 by -1: 1. Add to next coefficient: 0 + 1 = 1.\n",
      "\n",
      "Multiply 1 by -1: -1. Add to next coefficient: 0 + (-1) = -1.\n",
      "\n",
      "Multiply -1 by -1: 1. Add to last coefficient: -3 + 1 = -2.\n",
      "\n",
      "So, the coefficients of the quotient are: 1, -1, 1, -1, 1, -1, and the remainder is -2. \n",
      "\n",
      "Therefore, the quotient is \\( x^5 - x^4 + x^3 - x^2 + x - 1 \\) and the remainder is -2. \n",
      "\n",
      "But when I multiply back, I get \\( x^6 - 1 \\), which is not equal to \\( x^6 - 3 \\). So, that suggests that my quotient is incorrect. \n",
      "\n",
      "Wait, perhaps I made a mistake in the synthetic division. Let me check again. \n",
      "\n",
      "-1 | 1   0   0   0   0   0   -3\n",
      "\n",
      "Bring down the 1.\n",
      "\n",
      "Multiply 1 by -1: -1. Add to next coefficient: 0 + (-1) = -1.\n",
      "\n",
      "Multiply -1 by -1: 1. Add to next coefficient: 0 + 1 = 1\n",
      "сгенерированно=4096, вопрос+сжатые+сгенерированные=4312 оригинальная генерация=2318\n",
      "original_loss tensor(0.2492, device='cuda:0')\n",
      "compression_loss tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1020, вопрос+сжатые+сгенерированные=1236 оригинальная генерация=2469\n",
      "original_loss tensor(0.2055, device='cuda:0')\n",
      "compression_loss tensor(0.2572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=946, вопрос+сжатые+сгенерированные=1162 оригинальная генерация=1907\n",
      "original_loss tensor(0.3752, device='cuda:0')\n",
      "compression_loss tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=2622, вопрос+сжатые+сгенерированные=2838 оригинальная генерация=2678\n",
      "original_loss tensor(0.2832, device='cuda:0')\n",
      "compression_loss tensor(0.2949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=777, вопрос+сжатые+сгенерированные=993 оригинальная генерация=2664\n",
      "original_loss tensor(0.3187, device='cuda:0')\n",
      "compression_loss tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=518, вопрос+сжатые+сгенерированные=734 оригинальная генерация=1143\n",
      "original_loss tensor(0.3054, device='cuda:0')\n",
      "compression_loss tensor(0.3254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1020, вопрос+сжатые+сгенерированные=1236 оригинальная генерация=1759\n",
      "original_loss tensor(0.2425, device='cuda:0')\n",
      "compression_loss tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=3426, вопрос+сжатые+сгенерированные=3642 оригинальная генерация=3001\n",
      "original_loss tensor(0.2341, device='cuda:0')\n",
      "compression_loss tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=238, вопрос+сжатые+сгенерированные=454 оригинальная генерация=1554\n",
      "original_loss tensor(0.2270, device='cuda:0')\n",
      "compression_loss tensor(0.2624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1178, вопрос+сжатые+сгенерированные=1394 оригинальная генерация=2260\n",
      "original_loss tensor(0.2278, device='cuda:0')\n",
      "compression_loss tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=824, вопрос+сжатые+сгенерированные=1040 оригинальная генерация=1478\n",
      "original_loss tensor(0.2346, device='cuda:0')\n",
      "compression_loss tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=4004, вопрос+сжатые+сгенерированные=4220 оригинальная генерация=3321\n",
      "original_loss tensor(0.3020, device='cuda:0')\n",
      "compression_loss tensor(0.3208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=552, вопрос+сжатые+сгенерированные=768 оригинальная генерация=1269\n",
      "original_loss tensor(0.2668, device='cuda:0')\n",
      "compression_loss tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=834, вопрос+сжатые+сгенерированные=1050 оригинальная генерация=2693\n",
      "original_loss tensor(0.2632, device='cuda:0')\n",
      "compression_loss tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=610, вопрос+сжатые+сгенерированные=826 оригинальная генерация=1346\n",
      "original_loss tensor(0.3102, device='cuda:0')\n",
      "compression_loss tensor(0.3251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=215, вопрос+сжатые+сгенерированные=431 оригинальная генерация=2159\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as text_tqdm\n",
    "from hidden_capacity_reasoning.utils import tokenize_single_turn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "max_new_tokens = 400\n",
    "compression_tokens = 8 * 2\n",
    "\n",
    "evaluation_dataset = []\n",
    "correct_items = 0\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for dataset_pos in tqdm(range(len(correct_dataset))):\n",
    "    tokenized_turn = tokenize_single_turn(\n",
    "        question=base_prompt.format(question=correct_dataset[dataset_pos][\"problem\"]),\n",
    "        answer=correct_dataset[dataset_pos][\"model_answer\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    for key in tokenized_turn.keys():\n",
    "        tokenized_turn[key] = torch.tensor(tokenized_turn[key])\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    content_compression_mask = tokenized_turn[\"content_compression_mask\"]\n",
    "\n",
    "    input_part_end = (content_compression_mask == 0).nonzero()[-3][0]\n",
    "    # get only question part\n",
    "    question_input_ids = (\n",
    "        tokenized_turn[\"input_ids\"][: int(input_part_end) + 1].unsqueeze(0).cuda()\n",
    "    )\n",
    "    # print(tokenizer.decode(question_input_ids[-1]))\n",
    "\n",
    "    ########\n",
    "    ######## generate first part of tokens\n",
    "    ########\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # input_ids = torch.tensor(question_input_ids).cuda()\n",
    "        input_ids_embeds = model.get_input_embeddings()(question_input_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat(\n",
    "            [\n",
    "                input_ids_embeds,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        generated_ids_new = model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=torch.ones(\n",
    "                inputs_embeds.shape[:2],\n",
    "                device=\"cuda\",\n",
    "            ).long(),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        # break\n",
    "    generated_result = tokenizer.decode(generated_ids_new[-1])\n",
    "    # print(generated_result)\n",
    "\n",
    "    ########\n",
    "    ######## get original language loss\n",
    "    ########\n",
    "    labels = torch.cat(\n",
    "        [\n",
    "            question_input_ids.cuda(),\n",
    "            generated_ids_new.cuda(),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    # print(tokenizer.decode(labels[-1]))\n",
    "\n",
    "    question_content_mask = content_compression_mask[: int(input_part_end) + 1].clone()\n",
    "    question_content_mask[question_content_mask == 0] = 4\n",
    "    question_content_mask[question_content_mask == 1] = 0\n",
    "    question_content_mask[question_content_mask == 4] = 1\n",
    "    train_content_mask_new = torch.cat(\n",
    "        [\n",
    "            question_content_mask,\n",
    "            torch.ones(\n",
    "                generated_ids_new.shape[1],\n",
    "            ),\n",
    "        ]\n",
    "    ).long()\n",
    "    # print(question_content_mask)\n",
    "\n",
    "    generated_embeds = model.get_input_embeddings()(generated_ids_new)\n",
    "    new_input_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds,\n",
    "            generated_embeds,\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    labels[:, train_content_mask_new == 0] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        original_loss = model(\n",
    "            inputs_embeds=new_input_embeds,\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "    print(\"original_loss\", original_loss)\n",
    "    ########\n",
    "    ######## generate compress embeddings\n",
    "    ########\n",
    "\n",
    "    compression_tensor = torch.nn.Parameter(\n",
    "        torch.rand_like(\n",
    "            new_input_embeds[:, :compression_tokens, :],\n",
    "        )\n",
    "        * model.get_input_embeddings().weight.data.std(),\n",
    "        requires_grad=True,\n",
    "    )\n",
    "    compressed_inputs_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds.detach(),\n",
    "            compression_tensor,\n",
    "            generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    question_labels = question_input_ids.clone()\n",
    "    question_labels[0][question_content_mask == 0] = -100\n",
    "    question_labels = question_labels.cuda()\n",
    "    compressed_part = torch.ones(compression_tensor.shape[:2]).long().cuda()\n",
    "\n",
    "    compressed_labels = torch.cat(\n",
    "        [\n",
    "            question_labels,\n",
    "            compressed_part,\n",
    "            generated_ids_new[:, -(max_new_tokens // 2) :],\n",
    "        ],\n",
    "        dim=-1,\n",
    "    )\n",
    "\n",
    "    ########\n",
    "    ######## train\n",
    "    ########\n",
    "    epoch_amount = 100\n",
    "\n",
    "    optimizer = torch.optim.Adam([compression_tensor], lr=0.1)\n",
    "    acclumulation_steps = 1\n",
    "    for epoch in range(epoch_amount):\n",
    "        compressed_inputs_embeds = torch.cat(\n",
    "            [\n",
    "                input_ids_embeds.detach(),\n",
    "                compression_tensor,\n",
    "                generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        compression_loss = model(\n",
    "            inputs_embeds=compressed_inputs_embeds,\n",
    "            labels=compressed_labels,\n",
    "        ).loss\n",
    "        compression_loss.backward()\n",
    "        if (epoch + 1) % acclumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # print(epoch, compression_loss)\n",
    "        # if compression_loss.item() <= (original_loss.item() - 0.01):\n",
    "        #     break\n",
    "        if (compression_loss.item() - 0.02) <= original_loss.item():\n",
    "            break\n",
    "        # if compression_loss.item() <= original_loss.item():\n",
    "        #     break\n",
    "    print(\"compression_loss\", compression_loss)\n",
    "    ########\n",
    "    ######## evaluate\n",
    "    ########\n",
    "    with torch.no_grad():\n",
    "\n",
    "        compressed_inputs_embeds = torch.cat(\n",
    "            [\n",
    "                input_ids_embeds.detach(),\n",
    "                # compression_tensor,\n",
    "                torch.rand_like(compression_tensor).cuda(),\n",
    "                generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        generated_ids_compressed = model.generate(\n",
    "            inputs_embeds=compressed_inputs_embeds,\n",
    "            attention_mask=torch.ones(\n",
    "                compressed_inputs_embeds.shape[:2],\n",
    "                device=\"cuda\",\n",
    "            ).long(),\n",
    "            max_new_tokens=4096,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        # break\n",
    "    generated_result = tokenizer.decode(generated_ids_compressed[-1])\n",
    "    gold_answer = correct_dataset[dataset_pos][\"answer\"]\n",
    "    answer = dataset_answer_filter(gold_answer)\n",
    "    model_answer = model_answer_filter(generated_result)\n",
    "    if is_equiv(answer, model_answer):\n",
    "        correct_items += 1\n",
    "        print(\"CORRECT\")\n",
    "    else:\n",
    "        print(\"WRONG\", gold_answer)\n",
    "        print(generated_result)\n",
    "    compressed_total_len = (\n",
    "        compression_tensor.shape[1]\n",
    "        + generated_embeds[:, -(max_new_tokens // 2) :, :].shape[1]\n",
    "        + generated_ids_compressed.shape[1]\n",
    "    )\n",
    "    original_total_len = len(\n",
    "        tokenizer.encode(\n",
    "            correct_dataset[dataset_pos][\"model_answer\"],\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"сгенерированно={generated_ids_compressed.shape[1]}, вопрос+сжатые+сгенерированные={compressed_total_len} оригинальная генерация={original_total_len}\"\n",
    "    )\n",
    "    evaluation_dataset.append(\n",
    "        {\n",
    "            \"original_total_len\": original_total_len,\n",
    "            \"compressed_total_len\": compressed_total_len,\n",
    "        }\n",
    "    )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c623f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13392857142857142, 0.12946428571428573, 0.9666666666666667)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_dataset) / len(dataset), correct_items / len(dataset), correct_items / len(\n",
    "    correct_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ceadd571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56056, 41238, 0.7356571999429142)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_total_len = 0\n",
    "compressed_total_len = 0\n",
    "for item in evaluation_dataset:\n",
    "    original_total_len += item[\"original_total_len\"]\n",
    "    compressed_total_len += item[\"compressed_total_len\"]\n",
    "original_total_len, compressed_total_len, compressed_total_len / original_total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (56056, 38735, 0.6910054231482803) - обучение с 8 токенами, до того как лосс не станет оригинальным, все ответы правильные (0.9666666666666667) просто ошибка парсинга\n",
    "# (56056, 37788, 0.6741116026830313) - - обучение с 8 токенами, до того как лосс не станет оригинальным - 0.01, все ответы правильные (0.9666666666666667) просто ошибка парсинга\n",
    "# (56056, 41238, 0.7356571999429142) - никакого обучения, просто вставка рандомных 8 векторов, точность 0.9666666666666667"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
