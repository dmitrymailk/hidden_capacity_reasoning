{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffdb004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "224 202 0.9017857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "# model = model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    # \"dim/hendrycks_math_train_12k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096\"\n",
    "    # \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    # \"dim/hendrycks_math_train_1k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    ")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    # test_size=250,\n",
    "    test_size=350,\n",
    "    # test_size=999,\n",
    "    # test_size=1,\n",
    "    seed=42,\n",
    ")\n",
    "dataset = dataset[\"test\"].filter(lambda x: x[\"model_answer\"].count(\"</think>\") == 1)\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "\n",
    "correct_dataset = []\n",
    "\n",
    "for pos, item in enumerate(dataset):\n",
    "    try:\n",
    "        answer = dataset_answer_filter(item[\"answer\"])\n",
    "        model_answer = model_answer_filter(item[\"model_answer\"])\n",
    "        # print(answer, model_answer)\n",
    "        # break\n",
    "        if is_equiv(answer, model_answer):\n",
    "            correct_dataset.append(item)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(dataset), len(correct_dataset), len(correct_dataset) / len(dataset))\n",
    "\n",
    "correct_dataset = correct_dataset[:30]\n",
    "len(correct_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8bdd8",
   "metadata": {},
   "source": [
    "## Обучение по чанкам в цикле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc0804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bda176292d44de89f4911aba4f0be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.3264, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.3235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.3353, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.3136, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:26<01:59, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "вопрос+сжатые+сгенерированные=1193, всего_сгенерированно_токенов=1507 оригинальная_генерация=1959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.3798, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.3741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.2249, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.1980, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [00:31<01:23, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "вопрос+сжатые+сгенерированные=852, всего_сгенерированно_токенов=1735 оригинальная_генерация=1125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.4056, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.3287, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4619, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4382, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [00:30<01:21, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "вопрос+сжатые+сгенерированные=863, всего_сгенерированно_токенов=1759 оригинальная_генерация=1548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.4035, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.2993, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.2966, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:24<01:48, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRONG 6\n",
      "Okay, so I need to find the smallest positive integer \\( n \\) such that all the roots of the equation \\( z^4 + z^2 + 1 = 0 \\) are \\( n \\)-th roots of unity. Hmm, let me think about how to approach this.\n",
      "\n",
      "First, I remember that roots of unity are complex numbers that satisfy \\( z^n = 1 \\) for some integer \\( n \\). So, I need to find the smallest \\( n \\) where all the roots of this quartic equation are such roots.\n",
      "\n",
      "Let me start by analyzing the given equation: \\( z^4 + z^2 + 1 = 0 \\). It looks a bit like a quadratic in terms of \\( z^2 \\). Maybe I can factor it or find its roots by substitution.\n",
      "\n",
      "Let me set \\( w = z^2 \\). Then the equation becomes \\( w^2 + w + 1 = 0 \\). That's a quadratic equation, so I can solve for \\( w \\) using the quadratic formula:\n",
      "\n",
      "\\( w = \\frac{ -1 \\pm \\sqrt{1 - 4} }{2} = \\frac{ -1 \\pm \\sqrt{ -3 } }{2} \\).\n",
      "\n",
      "So, \\( w = \\frac{ -1 \\pm i\\sqrt{3} }{2} \\). These are complex numbers, specifically the primitive 3rd roots of unity. I remember that the roots of \\( w^2 + w + 1 = 0 \\) are the primitive 3rd roots of unity. That makes sense because \\( w^3 = 1 \\) for these roots.\n",
      "\n",
      "But wait, the original equation is \\( z^4 + z^2 + 1 = 0 \\), which is in terms of \\( z \\). So, if \\( w = z^2 \\), and \\( w \\) is a primitive 3rd root of unity, then \\( z^2 = w \\), which would imply that \\( z \\) is a square root of a primitive 3rd root of unity.\n",
      "\n",
      "So, let me find all the roots of \\( z^4 + z^2 + 1 = 0 \\). I can factor this equation or perhaps use substitution. Let me try substituting \\( w = z^2 \\), so the equation becomes \\( w^2 + w + 1 = 0 \\), which we already solved. The solutions are \\( w = \\frac{ -1 \\pm i\\sqrt{3} }{2} \\), which are \\( e^{2\\pi i /3} \\) and \\( e^{4\\pi i /3} \\).\n",
      "\n",
      "Therefore, \\( z^2 = e^{2\\pi i /3} \\) and \\( z^2 = e^{4\\pi i /3} \\). To find the roots \\( z \\), we take the square roots of each of these. The square roots of \\( e^{2\\pi i /3} \\) are \\( e^{\\pi i /3} \\) and \\( e^{5\\pi i /3} \\), and similarly, the square roots of \\( e^{4\\pi i /3} \\) are \\( e^{2\\pi i /3} \\) and \\( e^{8\\pi i /3} \\), which simplifies to \\( e^{2\\pi i /3} \\) and \\( e^{2\\pi i /3 + 2\\pi i} = e^{8\\pi i /3} = e^{2\\pi i /3} \\) (since \\( 8\\pi/3 = 2\\pi + 2\\pi/3 \\)).\n",
      "\n",
      "So, the roots \\( z \\) are \\( e^{\\pi i /3} \\), \\( e^{5\\pi i /3} \\), \\( e^{2\\pi i /3} \\), and \\( e^{8\\pi i /3} \\), which are all \\( 6^{th} \\) roots of unity. Therefore, the smallest positive integer \\( n \\) such that all the roots of \\( z^4 + z^2 + 1 = 0 \\) are \\( n^{th} \\) roots of unity is \\( 6 \\).\n",
      "</think>\n",
      "\n",
      "To determine the smallest positive integer \\( n \\) such that all the roots of the equation \\( z^4 + z^2 + 1 = 0 \\) are \\( n^{\\text{th}} \\) roots of unity, let's proceed step by step.\n",
      "\n",
      "### Step 1: Factor the Equation\n",
      "\n",
      "Notice that the equation \\( z^4 + z^2 + 1 = 0 \\) can be factored by recognizing it as a quadratic in terms of \\( z^2 \\):\n",
      "\n",
      "\\[\n",
      "z^4 + z^2 + 1 = (z^2)^2 + z^2 + 1\n",
      "\\]\n",
      "\n",
      "Let \\( w = z^2 \\), then the equation becomes:\n",
      "\n",
      "\\[\n",
      "w^2 + w + 1 = 0\n",
      "\\]\n",
      "\n",
      "This is a quadratic equation, which can be solved using the quadratic formula:\n",
      "\n",
      "\\[\n",
      "w = \\frac{-1 \\pm \\sqrt{1 - 4}}{2} = \\frac{-1 \\pm i\\sqrt{3}}{2}\n",
      "\\]\n",
      "\n",
      "Thus, the solutions for \\( w \\) are:\n",
      "\n",
      "\\[\n",
      "w = e^{\\pi i / 3} \\quad \\text{and} \\quad w = e^{5\\pi i / 3}\n",
      "\\]\n",
      "\n",
      "Since \\( w = z^2 \\), we substitute back to find \\( z \\):\n",
      "\n",
      "\\[\n",
      "z^2 = e^{\\pi i / 3} \\quad \\Rightarrow \\quad z = e^{\\pi i / 6} \\quad \\text{and} \\quad z = e^{7\\pi i / 6}\n",
      "\\]\n",
      "\n",
      "\\[\n",
      "z^2 = e^{5\\pi i / 3} \\quad \\Rightarrow \\quad z = e^{5\\pi i / 6} \\quad \\text{and} \\quad z = e^{11\\pi i / 6}\n",
      "\\]\n",
      "\n",
      "### Step 2: Expressing the Roots as Roots of Unity\n",
      "\n",
      "The solutions \\( z \\) are:\n",
      "\n",
      "\\[\n",
      "z = e^{\\pi i / 6}, \\quad e^{5\\pi i / 6}, \\quad e^{7\\pi i / 6}, \\quad e^{11\\pi i / 6}\n",
      "\\]\n",
      "\n",
      "These can be rewritten as:\n",
      "\n",
      "\\[\n",
      "z = e^{2\\pi i / 12}, \\quad e^{10\\pi i / 12}, \\quad e^{14\\pi i / 12}, \\quad e^{22\\pi i / 12}\n",
      "\\]\n",
      "\n",
      "Simplifying the exponents modulo \\( 2\\pi \\):\n",
      "\n",
      "\\[\n",
      "z = e^{2\\pi i / 12}, \\quad e^{10\\pi i / 12}, \\quad e^{14\\pi i / 12}, \\quad e^{22\\pi i / 12}\n",
      "\\]\n",
      "\n",
      "Notice that \\( 14\\pi / 12 = 7\\pi / 6 \\) and \\( 22\\pi / 12 = 11\\pi / 6 \\), which are equivalent to \\( e^{7\\pi i / 6} \\) and \\( e^{11\\pi i / 6} \\).\n",
      "\n",
      "### Step 3: Identifying the Least Common Multiple\n",
      "\n",
      "Each root \\( z \\) is a 12th root of unity because:\n",
      "\n",
      "\\[\n",
      "e^{2\\pi i k / 12} \\quad \\text{for} \\quad k = 1, 5, 7, 11\n",
      "\\]\n",
      "\n",
      "These correspond to the primitive 12th roots of unity. However, we need to find the smallest \\( n \\) such that all roots are \\( n^{\\text{th}} \\) roots of unity.\n",
      "\n",
      "Since all roots are 12th roots of unity, the smallest \\( n \\) that satisfies this condition is \\( n = 12 \\).\n",
      "\n",
      "### Final Answer\n",
      "\n",
      "\\[\n",
      "\\boxed{12}\n",
      "\\]<｜end▁of▁sentence｜>\n",
      "вопрос+сжатые+сгенерированные=1236, всего_сгенерированно_токенов=1738 оригинальная_генерация=3960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.3759, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 267\u001b[0m\n\u001b[1;32m    255\u001b[0m     compressed_inputs_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    256\u001b[0m         [\n\u001b[1;32m    257\u001b[0m             input_ids_embeds\u001b[38;5;241m.\u001b[39mdetach(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    262\u001b[0m     )\n\u001b[1;32m    263\u001b[0m compression_loss \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m    264\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mcompressed_inputs_embeds,\n\u001b[1;32m    265\u001b[0m     labels\u001b[38;5;241m=\u001b[39mcompressed_labels,\n\u001b[1;32m    266\u001b[0m )\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m--> 267\u001b[0m \u001b[43mcompression_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m acclumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    269\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as text_tqdm\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    tokenize_single_turn,\n",
    "    EOS_TOKEN_ID,\n",
    "    END_THINK_ID,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "max_new_tokens = 400\n",
    "compression_tokens_amount = 16\n",
    "max_total_tokens = 4096\n",
    "max_total_steps = max_total_tokens // max_new_tokens + 1\n",
    "\n",
    "evaluation_dataset = []\n",
    "correct_items = 0\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for dataset_pos in tqdm(range(len(correct_dataset))):\n",
    "    # for dataset_pos in tqdm(range(1, len(correct_dataset))):\n",
    "    tokenized_turn = tokenize_single_turn(\n",
    "        question=base_prompt.format(question=correct_dataset[dataset_pos][\"problem\"]),\n",
    "        answer=correct_dataset[dataset_pos][\"model_answer\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    for key in tokenized_turn.keys():\n",
    "        tokenized_turn[key] = torch.tensor(tokenized_turn[key])\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    content_compression_mask = tokenized_turn[\"content_compression_mask\"]\n",
    "\n",
    "    input_part_end = (content_compression_mask == 0).nonzero()[-3][0]\n",
    "    # get only question part\n",
    "    question_input_ids = (\n",
    "        tokenized_turn[\"input_ids\"][: int(input_part_end) + 1].unsqueeze(0).cuda()\n",
    "    )\n",
    "    # print(tokenizer.decode(question_input_ids[-1]))\n",
    "\n",
    "    ######## start loop generation\n",
    "    ########\n",
    "    compression_loop = True\n",
    "    input_ids_embeds = model.get_input_embeddings()(question_input_ids)\n",
    "    compression_part = torch.tensor([[0]])\n",
    "    generated_ids_new = None\n",
    "    generated_embeds = None\n",
    "    generated_embeds_prev = None\n",
    "    generated_ids_new_prev = None\n",
    "    end_of_think = False\n",
    "    total_generated_text = \"\"\n",
    "\n",
    "    for compression_step in text_tqdm(range(max_total_steps)):\n",
    "        ######## generate new tokens\n",
    "        ########\n",
    "        inputs_embeds = None\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if compression_part.shape[1] >= compression_tokens_amount:\n",
    "                generated_embeds_prev = generated_embeds[\n",
    "                    :, -(max_new_tokens // 2) :, :\n",
    "                ].clone()\n",
    "                inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds,\n",
    "                        compression_part,\n",
    "                        generated_embeds_prev,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            else:\n",
    "                # first time generation\n",
    "                inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            generated_ids_new = model.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=torch.ones(\n",
    "                    inputs_embeds.shape[:2],\n",
    "                    device=\"cuda\",\n",
    "                ).long(),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                # do_sample=False,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                top_k=30,\n",
    "                temperature=0.6,\n",
    "                # use_cache=compression_step > 0,\n",
    "                # use_cache=False,\n",
    "            )\n",
    "            # break\n",
    "        generated_result = tokenizer.decode(generated_ids_new[-1])\n",
    "        # print(generated_result)\n",
    "        total_generated_text += generated_result\n",
    "        print(\"=\" * 50)\n",
    "        generated_embeds = model.get_input_embeddings()(generated_ids_new)\n",
    "        if END_THINK_ID in generated_ids_new[-1].tolist():\n",
    "            end_of_think = True\n",
    "            break\n",
    "\n",
    "        ########\n",
    "        ######## get original language loss\n",
    "        ########\n",
    "        labels = None\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            labels = torch.cat(\n",
    "                [\n",
    "                    question_input_ids.cuda(),\n",
    "                    ((torch.ones(compression_part.shape[:2]) * -100).long()).cuda(),\n",
    "                    (\n",
    "                        (torch.ones(generated_embeds_prev.shape[:2]) * -100).long()\n",
    "                    ).cuda(),\n",
    "                    generated_ids_new.cuda(),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            # first time generation\n",
    "            labels = torch.cat(\n",
    "                [\n",
    "                    question_input_ids.cuda(),\n",
    "                    generated_ids_new.cuda(),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        question_content_mask = content_compression_mask[\n",
    "            : int(input_part_end) + 1\n",
    "        ].clone()\n",
    "        question_content_mask[question_content_mask == 0] = 4\n",
    "        question_content_mask[question_content_mask == 1] = 0\n",
    "        question_content_mask[question_content_mask == 4] = 1\n",
    "\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            train_content_mask_new = torch.cat(\n",
    "                [\n",
    "                    question_content_mask,\n",
    "                    torch.zeros(compression_part.shape[1]),\n",
    "                    torch.zeros(generated_embeds_prev.shape[1]),\n",
    "                    torch.zeros(generated_ids_new.shape[1] // 2),\n",
    "                    torch.ones(generated_ids_new.shape[1] // 2),\n",
    "                ]\n",
    "            ).long()\n",
    "        else:\n",
    "            train_content_mask_new = torch.cat(\n",
    "                [\n",
    "                    question_content_mask,\n",
    "                    torch.ones(generated_ids_new.shape[1] // 2) * 0,\n",
    "                    torch.ones(generated_ids_new.shape[1] // 2),\n",
    "                ]\n",
    "            ).long()\n",
    "\n",
    "        generated_ids_new_prev = generated_ids_new.clone()\n",
    "        # generated_embeds = model.get_input_embeddings()(generated_ids_new)\n",
    "\n",
    "        new_input_embeds = None\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            new_input_embeds = torch.cat(\n",
    "                [\n",
    "                    input_ids_embeds.cuda(),\n",
    "                    compression_part.cuda(),\n",
    "                    generated_embeds_prev.cuda(),\n",
    "                    generated_embeds,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            new_input_embeds = torch.cat(\n",
    "                [\n",
    "                    input_ids_embeds,\n",
    "                    generated_embeds,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        labels[:, train_content_mask_new == 0] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            original_loss = model(\n",
    "                inputs_embeds=new_input_embeds,\n",
    "                labels=labels,\n",
    "            ).loss\n",
    "        print(\"original_loss\", original_loss)\n",
    "        ########\n",
    "        ######## generate compress embeddings\n",
    "        ########\n",
    "        compression_tensor = torch.nn.Parameter(\n",
    "            torch.rand_like(\n",
    "                new_input_embeds[:, :compression_tokens_amount, :],\n",
    "            )\n",
    "            * model.get_input_embeddings().weight.data.std(),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "        question_labels = question_input_ids.clone().cuda()\n",
    "        question_labels[0][question_content_mask == 0] = -100\n",
    "        compression_tensor_labels = (\n",
    "            (torch.ones(compression_tensor.shape[:2]) * -100).long().cuda()\n",
    "        )\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            compression_part_labels = (\n",
    "                (torch.ones(compression_part.shape[:2]) * -100).long().cuda()\n",
    "            )\n",
    "            compressed_labels = torch.cat(\n",
    "                [\n",
    "                    question_labels,\n",
    "                    compression_part_labels,\n",
    "                    compression_tensor_labels,\n",
    "                    generated_ids_new[:, -(max_new_tokens // 2) :],\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "        else:\n",
    "            compressed_labels = torch.cat(\n",
    "                [\n",
    "                    question_labels,\n",
    "                    compression_tensor_labels,\n",
    "                    generated_ids_new[:, -(max_new_tokens // 2) :],\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "        ########\n",
    "        ######## train\n",
    "        ########\n",
    "        epoch_amount = 100\n",
    "\n",
    "        optimizer = torch.optim.Adam([compression_tensor], lr=0.1)\n",
    "        acclumulation_steps = 1\n",
    "        for epoch in range(epoch_amount):\n",
    "            if compression_part.shape[1] >= compression_tokens_amount:\n",
    "                compressed_inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds.detach(),\n",
    "                        compression_part.detach(),\n",
    "                        compression_tensor,\n",
    "                        generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            else:\n",
    "                compressed_inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds.detach(),\n",
    "                        compression_tensor,\n",
    "                        generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            compression_loss = model(\n",
    "                inputs_embeds=compressed_inputs_embeds,\n",
    "                labels=compressed_labels,\n",
    "            ).loss\n",
    "            compression_loss.backward()\n",
    "            if (epoch + 1) % acclumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if compression_loss.item() <= original_loss.item():\n",
    "                break\n",
    "            # if compression_loss.item() <= (original_loss.item() + 0.01):\n",
    "            #     break\n",
    "            # if (compression_loss.item() + 0.05) <= original_loss.item():\n",
    "            #     break\n",
    "        print(\"compression_loss\", compression_loss)\n",
    "        # compression_tensor = torch.rand_like(compression_tensor.detach())\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            compression_part = torch.cat(\n",
    "                [\n",
    "                    compression_part,\n",
    "                    compression_tensor.detach(),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            # compression_part = compression_tensor.detach()\n",
    "        else:\n",
    "            compression_part = compression_tensor.detach()\n",
    "    # if end_of_think:\n",
    "    inputs_embeds = torch.cat(\n",
    "        [\n",
    "            inputs_embeds,\n",
    "            generated_embeds,\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    final_response = model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=torch.ones(\n",
    "            inputs_embeds.shape[:2],\n",
    "            device=\"cuda\",\n",
    "        ).long(),\n",
    "        max_new_tokens=(max_total_steps - compression_step) * max_new_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    final_answer = tokenizer.decode(final_response[-1])\n",
    "    total_generated_text += final_answer\n",
    "    # print(\"FINAL ANSWER\", final_answer)\n",
    "\n",
    "    gold_answer = correct_dataset[dataset_pos][\"answer\"]\n",
    "    answer = dataset_answer_filter(gold_answer)\n",
    "    # print(\"GOLD ANSWER\", answer)\n",
    "    model_answer = model_answer_filter(total_generated_text)\n",
    "    if is_equiv(answer, model_answer):\n",
    "        correct_items += 1\n",
    "        print(\"CORRECT\")\n",
    "    else:\n",
    "        print(\"WRONG\", gold_answer)\n",
    "        print(total_generated_text)\n",
    "\n",
    "    compressed_total_len = inputs_embeds.shape[1] + final_response.shape[1]\n",
    "    total_generated_tokens = final_response.shape[1] + max_new_tokens * (\n",
    "        compression_step + 1\n",
    "    )\n",
    "    original_total_len = len(\n",
    "        tokenizer.encode(\n",
    "            correct_dataset[dataset_pos][\"model_answer\"],\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"вопрос+сжатые+сгенерированные={compressed_total_len}, всего_сгенерированно_токенов={total_generated_tokens} оригинальная_генерация={original_total_len}\"\n",
    "    )\n",
    "    evaluation_dataset.append(\n",
    "        {\n",
    "            \"original_total_len\": original_total_len,\n",
    "            \"compressed_total_len\": compressed_total_len,\n",
    "        }\n",
    "    )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7f082d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13392857142857142, 0.12946428571428573, 0.9666666666666667)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_dataset) / len(dataset), correct_items / len(dataset), correct_items / len(\n",
    "    correct_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c547f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56056, 29497, 0.5262059369202227)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_total_len = 0\n",
    "compressed_total_len = 0\n",
    "for item in evaluation_dataset:\n",
    "    original_total_len += item[\"original_total_len\"]\n",
    "    compressed_total_len += item[\"compressed_total_len\"]\n",
    "original_total_len, compressed_total_len, compressed_total_len / original_total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0401256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тестовая выборка из 30 элементов\n",
    "# (56056, 21924, 0.39110889110889113) - 0.8333333333333334, 200 токенов, 4 сжимающих\n",
    "# (56056, 29497, 0.5262059369202227) - 0.9666666666666667, 400 токенов, 16 сжимающих\n",
    "# (56056, 27994, 0.499393463679178) - 0.9, 400 токенов, 8 сжимающих\n",
    "# (56056, 23062, 0.4114100185528757) - 0.9, 200 токенов, 16 сжимающих\n",
    "# (56056, 30087, 0.5367311260168403) - 0.8666666666666667, 400, 32\n",
    "# (56056, 19406, 0.3461895247609533) - 0.7333333333333333, 100, 2\n",
    "# (56056, 28068, 0.5007135721421436) - 0.9333333333333333 - 400 токенов, 16 сжимающих, но они не конкатенировались, а просто пересоздавались\n",
    "# (56056, 28098, 0.5012487512487512) - 0.9333333333333333 - 400 токенов, 32 сжимающих, но они не конкатенировались, а просто пересоздавались\n",
    "\n",
    "# тестовая выборка на 202\n",
    "# (408262, 192057, 0.4704258539859208) - 0.8564356435643564, 400 токенов, 16 сжимающих"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
