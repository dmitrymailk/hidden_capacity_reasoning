{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffdb004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "224 202 0.9017857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "# model = model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    # \"dim/hendrycks_math_train_12k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096\"\n",
    "    # \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    # \"dim/hendrycks_math_train_1k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    ")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    # test_size=250,\n",
    "    test_size=350,\n",
    "    # test_size=999,\n",
    "    # test_size=1,\n",
    "    seed=42,\n",
    ")\n",
    "dataset = dataset[\"test\"].filter(lambda x: x[\"model_answer\"].count(\"</think>\") == 1)\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "\n",
    "correct_dataset = []\n",
    "\n",
    "for pos, item in enumerate(dataset):\n",
    "    try:\n",
    "        answer = dataset_answer_filter(item[\"answer\"])\n",
    "        model_answer = model_answer_filter(item[\"model_answer\"])\n",
    "        # print(answer, model_answer)\n",
    "        # break\n",
    "        if is_equiv(answer, model_answer):\n",
    "            correct_dataset.append(item)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(dataset), len(correct_dataset), len(correct_dataset) / len(dataset))\n",
    "\n",
    "correct_dataset = correct_dataset[:30]\n",
    "len(correct_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8bdd8",
   "metadata": {},
   "source": [
    "## Обучение по чанкам в цикле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc0804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b5ab39d7324182ba5fd170fb97730c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.4852, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.3640, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.3844, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4838, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.7059, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4/21 [00:42<03:02, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "вопрос+сжатые+сгенерированные=1457, всего_сгенерированно_токенов=1887 оригинальная_генерация=1959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.6115, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.6092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4828, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4040, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.3793, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.3714, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.3604, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.6290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.3638, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.3669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.5439, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.5295, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.5750, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4180, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4158, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11/21 [01:24<01:16,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "вопрос+сжатые+сгенерированные=445, всего_сгенерированно_токенов=2432 оригинальная_генерация=1125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.4880, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4484, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4405, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2/21 [00:14<02:21,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "вопрос+сжатые+сгенерированные=677, всего_сгенерированно_токенов=913 оригинальная_генерация=1548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.4977, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.5172, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4632, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.5629, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4721, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.5359, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6/21 [00:37<01:34,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRONG 6\n",
      "Okay, so I have this problem: I need to find the smallest positive integer \\( n \\) such that all the roots of the equation \\( z^4 + z^2 + 1 = 0 \\) are \\( n^{\\text{th}} \\) roots of unity. Hmm, okay. Let me think about how to approach this.\n",
      "\n",
      "First, I remember that the roots of unity are the solutions to the equation \\( z^n = 1 \\). So, if all the roots of \\( z^4 + z^2 + 1 = 0 \\) are \\( n^{\\text{th}} \\) roots of unity, that means each root \\( z \\) satisfies \\( z^n = 1 \\). Therefore, I need to find the smallest \\( n \\) such that every root of the given quartic equation is also a root of unity of order \\( n \\).\n",
      "\n",
      "Let me write down the equation again: \\( z^4 + z^2 + 1 = 0 \\). Hmm, this looks similar to some cyclotomic polynomials I've heard about. Cyclotomic polynomials are minimal polynomials over the integers for roots of unity. Maybe this quartic is related to a cyclotomic polynomial?\n",
      "\n",
      "I recall that cyclotomic polynomials are factors of \\( z^n - 1 \\) and are irreducible over the integers. The roots of cyclotomic polynomials are primitive \\( n^{\\text{th}} \\) roots of unity. So, if I can express \\( z^4 + z^2 + 1 \\) as a product of cyclotomic polynomials, the least common multiple of their orders would give me the smallest \\( n \\) such that all roots are \\( n^{\\text{th}} \\) roots of unity.\n",
      "\n",
      "Let me try to factor \\( z^4 + z^2 + 1 \\). Maybe I can factor it into quadratics or something. Let me see:\n",
      "\n",
      "\\( z^4 + z^2 + 1 \\). Hmm, I recall that \\( z^4 + z^2 + 1 \\) can be factored as \\( (z^2 + z + 1)(z^2 - z + 1) \\). Let me check that:\n",
      "\n",
      "\\( (z^2 + z + 1)(z^2 - z + 1) = z^4 - z^3 + z^2 + z^3 - z^2 + z + z^2 - z + 1 \\).\n",
      "\n",
      "Simplifying, the \\( -z^3 \\) and \\( +z^3 \\) cancel, \\( z^2 - z^2 \\) cancels, and \\( +z - z \\) cancels, leaving \\( z^4 + z^2 + 1 \\). Yes, that works!\n",
      "\n",
      "So, \\( z^4 + z^2 + 1 = (z^3 + 1)(z + 1) \\).\n",
      "\n",
      "Wait, let me check that multiplication again. Multiplying \\( (z^3 + 1)(z + 1) \\):\n",
      "\n",
      "\\( z^3 \\cdot z = z^4 \\),\n",
      "\\( z^3 \\cdot 1 = z^3 \\),\n",
      "\\( 1 \\cdot z = z \\),\n",
      "\\( 1 \\cdot 1 = 1 \\).\n",
      "\n",
      "Adding those up: \\( z^4 + z^3 + z + 1 \\). Hmm, that's not the same as \\( z^4 + z^2 + 1 \\). So, my initial factorization was incorrect.\n",
      "\n",
      "Maybe I need to try a different approach. Perhaps using the sum of cubes formula? Let me recall that \\( z^6 - 1 = (z^2 - 1)(z^4 + z^2 + 1) \\). So, \\( z^4 + z^2 + 1 \\) is a factor of \\( z^6 - 1 \\). That means all the roots of \\( z^4 + z^2 + 1 = 0 \\) are 6th roots of unity. But wait, is that the smallest \\( n \\) or could it be smaller?\n",
      "\n",
      "Let me check the roots of \\( z^4 + z^2 + 1 = 0 \\). I can rewrite this equation as \\( z^4 = -z^2 - 1 \\). Maybe I can express this in terms of \\( z^2 \\). Let me set \\( w = z^2 \\), so the equation becomes \\( w^2 + w + 1 = 0 \\). Solving for \\( w \\), we get \\( w = \\frac{-1 \\pm \\sqrt{-3}}{2} \\), which are the primitive 3rd roots of unity. So, \\( w = e^{2\\pi i /3} \\) or \\( w = e^{4\\pi i /3} \\).\n",
      "\n",
      "  Therefore, \\( z^2 = e^{2\\pi i /3} \\) or \\( z^2 = e^{4\\pi i /3} \\). Taking square roots, we have \\( z = e^{\\pi i /3} \\), \\( z = e^{5\\pi i /3} \\), \\( z = e^{2\\pi i /3} \\), or \\( z = e^{4\\pi i /3} \\).\n",
      "\n",
      "  Wait, but \\( e^{\\pi i /3} \\) and \\( e^{5\\pi i /3} \\) are primitive 6th roots of unity because their angles are multiples of \\( \\pi/3 \\), which is \\( 60^\\circ \\), and 6 is the smallest integer for which \\( 60^\\circ \\) divides \\( 360^\\circ \\).\n",
      "\n",
      "  Similarly, \\( e^{2\\pi i /3} \\) and \\( e^{4\\pi i /3} \\) are also primitive 6th roots of unity.\n",
      "\n",
      "  So, all roots of \\( z^4 + z^2 + 1 = 0 \\) are indeed 6th roots of unity, and 6 is the smallest such integer because the roots are primitive 6th roots.\n",
      "\n",
      "  Therefore, the smallest positive integer \\( n \\) is 6.\n",
      "</think>\n",
      "\n",
      "To determine the smallest positive integer \\( n \\) such that all roots of the equation \\( z^4 + z^2 + 1 = 0 \\) are \\( n^{\\text{th}} \\) roots of unity, let's analyze the roots of the given polynomial.\n",
      "\n",
      "**Step 1: Factor the Polynomial**\n",
      "\n",
      "Notice that the polynomial can be rewritten as:\n",
      "\\[\n",
      "z^4 + z^2 + 1 = (z^2)^2 + z^2 + 1\n",
      "\\]\n",
      "This resembles the quadratic form \\( x^2 + x + 1 \\), which factors as \\( (x - \\omega)(x - \\omega^2) \\) where \\( \\omega \\) is a primitive 3rd root of unity (\\( \\omega = e^{2\\pi i /3} \\)).\n",
      "\n",
      "Applying this to our polynomial:\n",
      "\\[\n",
      "z^4 + z^2 + 1 = (z^2)^2 + z^2 + 1 = (z^2 - \\omega)(z^2 - \\omega^2)\n",
      "\\]\n",
      "However, this factorization isn't directly helpful. Instead, let's consider the roots of the equation \\( z^4 + z^2 + 1 = 0 \\).\n",
      "\n",
      "**Step 2: Solve for \\( z \\)**\n",
      "\n",
      "Let \\( w = z^2 \\). Then the equation becomes:\n",
      "\\[\n",
      "w^2 + w + 1 = 0\n",
      "\\]\n",
      "Using the quadratic formula:\n",
      "\\[\n",
      "w = \\frac{-1 \\pm \\sqrt{1 - 4}}{2} = \\frac{-1 \\pm \\sqrt{-3}}{2} = \\frac{-1 \\pm i\\sqrt{3}}{2}\n",
      "\\]\n",
      "Thus, \\( w = e^{\\pm i\\pi/3} \\), which are primitive 6th roots of unity since their angles are multiples of \\( \\pi/3 \\) (60 degrees), and 6 is the smallest integer for which this holds true.\n",
      "\n",
      "**Step 3: Relate Back to \\( z \\)**\n",
      "\n",
      "Since \\( w = z^2 \\), we have:\n",
      "\\[\n",
      "z^2 = e^{\\pm i\\pi/3} \\implies z = \\pm e^{\\pm i\\pi/6}\n",
      "\\]\n",
      "These roots are \\( e^{i\\pi/6} \\), \\( e^{5i\\pi/6} \\), \\( e^{7i\\pi/6} \\), and \\( e^{11i\\pi/6} \\), which are all primitive 12th roots of unity. However, this contradicts our earlier conclusion. Let's re-examine this.\n",
      "\n",
      "Wait, actually, \\( e^{i\\pi/6} \\) and \\( e^{5i\\pi/6} \\) are primitive 12th roots of unity, but they are also 6th roots because \\( e^{i\\pi/6} \\) raised to the 6th power is \\( e^{i\\pi} = -1 \\), which is a root of unity. However, \\( e^{i\\pi/6} \\) is not a primitive 6th root because \\( e^{i\\pi/6}^6 = e^{i\\pi} = -1 \\), which is not a primitive 6th root. Therefore, the roots are primitive 12th roots of unity.\n",
      "\n",
      "But wait, let's double-check. The roots \\( e^{i\\pi/6} \\) and \\( e^{5i\\pi/6} \\) are indeed primitive 12th roots of unity because their angles are multiples of \\( \\pi/6 \\), and 12 is the smallest integer for which \\( \\pi/6 \\) divides \\( 2\\pi \\).\n",
      "\n",
      "However, earlier we thought they were primitive 6th roots, which was incorrect. Let's correct that.\n",
      "\n",
      "**Step 4: Correct Identification of Roots**\n",
      "\n",
      "The roots \\( e^{i\\pi/6} \\) and \\( e^{5i\\pi/6} \\) are primitive 12th roots of unity because:\n",
      "\\[\n",
      "(e^{i\\pi/6})^{12} = e^{i2\\pi} = 1\n",
      "\\]\n",
      "and there is no smaller positive integer \\( n \\) for which this holds true.\n",
      "\n",
      "Similarly, \\( e^{7i\\pi/6} = e^{i\\pi + i\\pi/6} = -e^{i\\pi/6} \\) and \\( e^{11i\\pi/6} = e^{i\\pi - i\\pi/6} = -e^{i5\\pi/6} \\) are also primitive 12th roots of unity.\n",
      "\n",
      "**Step 5: Conclusion**\n",
      "\n",
      "Since all roots of \\( z^4 + z^2 + 1 = 0 \\) are primitive 12th roots of unity, the smallest positive integer \\( n \\) such that all roots are \\( n^{\\text{th}} \\) roots of unity is \\( \\boxed{12} \\).<｜end▁of▁sentence｜>\n",
      "вопрос+сжатые+сгенерированные=1366, всего_сгенерированно_токенов=2376 оригинальная_генерация=3960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.6077, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.5295, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2/21 [00:16<02:37,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "вопрос+сжатые+сгенерированные=511, всего_сгенерированно_токенов=670 оригинальная_генерация=748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.5476, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4552, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.5352, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5253, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3/21 [00:19<01:56,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "вопрос+сжатые+сгенерированные=400, всего_сгенерированно_токенов=829 оригинальная_генерация=1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.5603, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4948, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4773, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4761, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3/21 [00:21<02:06,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "вопрос+сжатые+сгенерированные=505, всего_сгенерированно_токенов=920 оригинальная_генерация=1639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.5776, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4510, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.3970, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.3770, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.3752, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4/21 [00:28<02:00,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "вопрос+сжатые+сгенерированные=508, всего_сгенерированно_токенов=1136 оригинальная_генерация=1732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.5864, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4754, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4724, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2/21 [00:14<02:19,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "вопрос+сжатые+сгенерированные=717, всего_сгенерированно_токенов=924 оригинальная_генерация=1198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "original_loss tensor(0.6308, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.6272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4757, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.5037, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.5118, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.5116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "==================================================\n",
      "original_loss tensor(0.4416, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_loss tensor(0.4333, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 5/21 [00:32<01:43,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as text_tqdm\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    tokenize_single_turn,\n",
    "    EOS_TOKEN_ID,\n",
    "    END_THINK_ID,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "max_new_tokens = 200\n",
    "compression_tokens_amount = 4\n",
    "max_total_tokens = 4096\n",
    "max_total_steps = max_total_tokens // max_new_tokens + 1\n",
    "\n",
    "evaluation_dataset = []\n",
    "correct_items = 0\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for dataset_pos in tqdm(range(len(correct_dataset))):\n",
    "# for dataset_pos in tqdm(range(1, len(correct_dataset))):\n",
    "    tokenized_turn = tokenize_single_turn(\n",
    "        question=base_prompt.format(question=correct_dataset[dataset_pos][\"problem\"]),\n",
    "        answer=correct_dataset[dataset_pos][\"model_answer\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    for key in tokenized_turn.keys():\n",
    "        tokenized_turn[key] = torch.tensor(tokenized_turn[key])\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    content_compression_mask = tokenized_turn[\"content_compression_mask\"]\n",
    "\n",
    "    input_part_end = (content_compression_mask == 0).nonzero()[-3][0]\n",
    "    # get only question part\n",
    "    question_input_ids = (\n",
    "        tokenized_turn[\"input_ids\"][: int(input_part_end) + 1].unsqueeze(0).cuda()\n",
    "    )\n",
    "    # print(tokenizer.decode(question_input_ids[-1]))\n",
    "\n",
    "    ######## start loop generation\n",
    "    ########\n",
    "    compression_loop = True\n",
    "    input_ids_embeds = model.get_input_embeddings()(question_input_ids)\n",
    "    compression_part = torch.tensor([[0]])\n",
    "    generated_ids_new = None\n",
    "    generated_embeds = None\n",
    "    generated_embeds_prev = None\n",
    "    generated_ids_new_prev = None\n",
    "    end_of_think = False\n",
    "    total_generated_text = \"\"\n",
    "\n",
    "    for compression_step in text_tqdm(range(max_total_steps)):\n",
    "        ######## generate new tokens\n",
    "        ########\n",
    "        inputs_embeds = None\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if compression_part.shape[1] >= compression_tokens_amount:\n",
    "                generated_embeds_prev = generated_embeds[\n",
    "                    :, -(max_new_tokens // 2) :, :\n",
    "                ].clone()\n",
    "                inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds,\n",
    "                        compression_part,\n",
    "                        generated_embeds_prev,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            else:\n",
    "                # first time generation\n",
    "                inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            generated_ids_new = model.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=torch.ones(\n",
    "                    inputs_embeds.shape[:2],\n",
    "                    device=\"cuda\",\n",
    "                ).long(),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                use_cache=compression_step > 0,\n",
    "            )\n",
    "            # break\n",
    "        generated_result = tokenizer.decode(generated_ids_new[-1])\n",
    "        # print(generated_result)\n",
    "        total_generated_text += generated_result\n",
    "        print(\"=\" * 50)\n",
    "        generated_embeds = model.get_input_embeddings()(generated_ids_new)\n",
    "        if END_THINK_ID in generated_ids_new[-1].tolist():\n",
    "            end_of_think = True\n",
    "            break\n",
    "\n",
    "        ########\n",
    "        ######## get original language loss\n",
    "        ########\n",
    "        labels = None\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            labels = torch.cat(\n",
    "                [\n",
    "                    question_input_ids.cuda(),\n",
    "                    ((torch.ones(compression_part.shape[:2]) * -100).long()).cuda(),\n",
    "                    (\n",
    "                        (torch.ones(generated_embeds_prev.shape[:2]) * -100).long()\n",
    "                    ).cuda(),\n",
    "                    generated_ids_new.cuda(),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            # first time generation\n",
    "            labels = torch.cat(\n",
    "                [\n",
    "                    question_input_ids.cuda(),\n",
    "                    generated_ids_new.cuda(),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        question_content_mask = content_compression_mask[\n",
    "            : int(input_part_end) + 1\n",
    "        ].clone()\n",
    "        question_content_mask[question_content_mask == 0] = 4\n",
    "        question_content_mask[question_content_mask == 1] = 0\n",
    "        question_content_mask[question_content_mask == 4] = 1\n",
    "\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            train_content_mask_new = torch.cat(\n",
    "                [\n",
    "                    question_content_mask,\n",
    "                    torch.zeros(compression_part.shape[1]),\n",
    "                    torch.zeros(generated_embeds_prev.shape[1]),\n",
    "                    torch.zeros(generated_ids_new.shape[1] // 2),\n",
    "                    torch.ones(generated_ids_new.shape[1] // 2),\n",
    "                ]\n",
    "            ).long()\n",
    "        else:\n",
    "            train_content_mask_new = torch.cat(\n",
    "                [\n",
    "                    question_content_mask,\n",
    "                    torch.ones(generated_ids_new.shape[1] // 2) * 0,\n",
    "                    torch.ones(generated_ids_new.shape[1] // 2),\n",
    "                ]\n",
    "            ).long()\n",
    "\n",
    "        generated_ids_new_prev = generated_ids_new.clone()\n",
    "        # generated_embeds = model.get_input_embeddings()(generated_ids_new)\n",
    "\n",
    "        new_input_embeds = None\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            new_input_embeds = torch.cat(\n",
    "                [\n",
    "                    input_ids_embeds.cuda(),\n",
    "                    compression_part.cuda(),\n",
    "                    generated_embeds_prev.cuda(),\n",
    "                    generated_embeds,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            new_input_embeds = torch.cat(\n",
    "                [\n",
    "                    input_ids_embeds,\n",
    "                    generated_embeds,\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        labels[:, train_content_mask_new == 0] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            original_loss = model(\n",
    "                inputs_embeds=new_input_embeds,\n",
    "                labels=labels,\n",
    "            ).loss\n",
    "        print(\"original_loss\", original_loss)\n",
    "        ########\n",
    "        ######## generate compress embeddings\n",
    "        ########\n",
    "        compression_tensor = torch.nn.Parameter(\n",
    "            torch.rand_like(\n",
    "                new_input_embeds[:, :compression_tokens_amount, :],\n",
    "            )\n",
    "            * model.get_input_embeddings().weight.data.std(),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "        question_labels = question_input_ids.clone().cuda()\n",
    "        question_labels[0][question_content_mask == 0] = -100\n",
    "        compression_tensor_labels = (\n",
    "            (torch.ones(compression_tensor.shape[:2]) * -100).long().cuda()\n",
    "        )\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            compression_part_labels = (\n",
    "                (torch.ones(compression_part.shape[:2]) * -100).long().cuda()\n",
    "            )\n",
    "            compressed_labels = torch.cat(\n",
    "                [\n",
    "                    question_labels,\n",
    "                    compression_part_labels,\n",
    "                    compression_tensor_labels,\n",
    "                    generated_ids_new[:, -(max_new_tokens // 2) :],\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "        else:\n",
    "            compressed_labels = torch.cat(\n",
    "                [\n",
    "                    question_labels,\n",
    "                    compression_tensor_labels,\n",
    "                    generated_ids_new[:, -(max_new_tokens // 2) :],\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "        ########\n",
    "        ######## train\n",
    "        ########\n",
    "        epoch_amount = 100\n",
    "\n",
    "        optimizer = torch.optim.Adam([compression_tensor], lr=0.1)\n",
    "        acclumulation_steps = 1\n",
    "        for epoch in range(epoch_amount):\n",
    "            if compression_part.shape[1] >= compression_tokens_amount:\n",
    "                compressed_inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds.detach(),\n",
    "                        compression_part.detach(),\n",
    "                        compression_tensor,\n",
    "                        generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            else:\n",
    "                compressed_inputs_embeds = torch.cat(\n",
    "                    [\n",
    "                        input_ids_embeds.detach(),\n",
    "                        compression_tensor,\n",
    "                        generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            compression_loss = model(\n",
    "                inputs_embeds=compressed_inputs_embeds,\n",
    "                labels=compressed_labels,\n",
    "            ).loss\n",
    "            compression_loss.backward()\n",
    "            if (epoch + 1) % acclumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if compression_loss.item() <= original_loss.item():\n",
    "                break\n",
    "            # if compression_loss.item() <= (original_loss.item() + 0.01):\n",
    "            #     break\n",
    "        print(\"compression_loss\", compression_loss)\n",
    "        if compression_part.shape[1] >= compression_tokens_amount:\n",
    "            compression_part = torch.cat(\n",
    "                [\n",
    "                    compression_part,\n",
    "                    compression_tensor.detach(),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            compression_part = compression_tensor.detach()\n",
    "    inputs_embeds = torch.cat(\n",
    "        [\n",
    "            inputs_embeds,\n",
    "            generated_embeds,\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    final_response = model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=torch.ones(\n",
    "            inputs_embeds.shape[:2],\n",
    "            device=\"cuda\",\n",
    "        ).long(),\n",
    "        max_new_tokens=(max_total_steps - compression_step) * max_new_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    final_answer = tokenizer.decode(final_response[-1])\n",
    "    total_generated_text += final_answer\n",
    "    # print(\"FINAL ANSWER\", final_answer)\n",
    "\n",
    "    gold_answer = correct_dataset[dataset_pos][\"answer\"]\n",
    "    answer = dataset_answer_filter(gold_answer)\n",
    "    # print(\"GOLD ANSWER\", answer)\n",
    "    model_answer = model_answer_filter(total_generated_text)\n",
    "    if is_equiv(answer, model_answer):\n",
    "        correct_items += 1\n",
    "        print(\"CORRECT\")\n",
    "    else:\n",
    "        print(\"WRONG\", gold_answer)\n",
    "        print(total_generated_text)\n",
    "        \n",
    "    compressed_total_len = inputs_embeds.shape[1] + final_response.shape[1]\n",
    "    total_generated_tokens = final_response.shape[1] + max_new_tokens * (\n",
    "        compression_step + 1\n",
    "    )\n",
    "    original_total_len = len(\n",
    "        tokenizer.encode(\n",
    "            correct_dataset[dataset_pos][\"model_answer\"],\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"вопрос+сжатые+сгенерированные={compressed_total_len}, всего_сгенерированно_токенов={total_generated_tokens} оригинальная_генерация={original_total_len}\"\n",
    "    )\n",
    "    evaluation_dataset.append(\n",
    "        {\n",
    "            \"original_total_len\": original_total_len,\n",
    "            \"compressed_total_len\": compressed_total_len,\n",
    "        }\n",
    "    )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0401256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
