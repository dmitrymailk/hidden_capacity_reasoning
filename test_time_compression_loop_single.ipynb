{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffdb004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "224 202 0.9017857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"sdpa\",\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "# model = model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    # \"dim/hendrycks_math_train_12k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096\"\n",
    "    # \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    # \"dim/hendrycks_math_train_1k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    ")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    # test_size=250,\n",
    "    test_size=350,\n",
    "    # test_size=999,\n",
    "    # test_size=1,\n",
    "    seed=42,\n",
    ")\n",
    "dataset = dataset[\"test\"].filter(lambda x: x[\"model_answer\"].count(\"</think>\") == 1)\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "\n",
    "correct_dataset = []\n",
    "\n",
    "for pos, item in enumerate(dataset):\n",
    "    try:\n",
    "        answer = dataset_answer_filter(item[\"answer\"])\n",
    "        model_answer = model_answer_filter(item[\"model_answer\"])\n",
    "        # print(answer, model_answer)\n",
    "        # break\n",
    "        if is_equiv(answer, model_answer):\n",
    "            correct_dataset.append(item)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(dataset), len(correct_dataset), len(correct_dataset) / len(dataset))\n",
    "\n",
    "correct_dataset = correct_dataset[:30]\n",
    "len(correct_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f058fd3",
   "metadata": {},
   "source": [
    "## test time train generation (single train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303e6e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c13a591ad74ee68e7e7d804ac4ff47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_loss tensor(0.2998, device='cuda:0')\n",
      "compression_loss tensor(0.2918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1598, вопрос+сжатые+сгенерированные=1814 оригинальная генерация=1959\n",
      "original_loss tensor(0.3643, device='cuda:0')\n",
      "compression_loss tensor(0.3603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=765, вопрос+сжатые+сгенерированные=981 оригинальная генерация=1125\n",
      "original_loss tensor(0.3328, device='cuda:0')\n",
      "compression_loss tensor(0.3295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1304, вопрос+сжатые+сгенерированные=1520 оригинальная генерация=1548\n",
      "original_loss tensor(0.3823, device='cuda:0')\n",
      "compression_loss tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=2450, вопрос+сжатые+сгенерированные=2666 оригинальная генерация=3960\n",
      "original_loss tensor(0.3659, device='cuda:0')\n",
      "compression_loss tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=276, вопрос+сжатые+сгенерированные=492 оригинальная генерация=748\n",
      "original_loss tensor(0.3913, device='cuda:0')\n",
      "compression_loss tensor(0.3870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=788, вопрос+сжатые+сгенерированные=1004 оригинальная генерация=1080\n",
      "original_loss tensor(0.4168, device='cuda:0')\n",
      "compression_loss tensor(0.4065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=986, вопрос+сжатые+сгенерированные=1202 оригинальная генерация=1639\n",
      "original_loss tensor(0.2829, device='cuda:0')\n",
      "compression_loss tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1408, вопрос+сжатые+сгенерированные=1624 оригинальная генерация=1732\n",
      "original_loss tensor(0.3363, device='cuda:0')\n",
      "compression_loss tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=782, вопрос+сжатые+сгенерированные=998 оригинальная генерация=1198\n",
      "original_loss tensor(0.3540, device='cuda:0')\n",
      "compression_loss tensor(0.3532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=797, вопрос+сжатые+сгенерированные=1013 оригинальная генерация=2314\n",
      "original_loss tensor(0.3477, device='cuda:0')\n",
      "compression_loss tensor(0.3465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1063, вопрос+сжатые+сгенерированные=1279 оригинальная генерация=1262\n",
      "original_loss tensor(0.3004, device='cuda:0')\n",
      "compression_loss tensor(0.2991, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1363, вопрос+сжатые+сгенерированные=1579 оригинальная генерация=1176\n",
      "original_loss tensor(0.3661, device='cuda:0')\n",
      "compression_loss tensor(0.3641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=699, вопрос+сжатые+сгенерированные=915 оригинальная генерация=1482\n",
      "original_loss tensor(0.4485, device='cuda:0')\n",
      "compression_loss tensor(0.4450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=402, вопрос+сжатые+сгенерированные=618 оригинальная генерация=814\n",
      "original_loss tensor(0.2837, device='cuda:0')\n",
      "compression_loss tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1356, вопрос+сжатые+сгенерированные=1572 оригинальная генерация=2318\n",
      "original_loss tensor(0.3168, device='cuda:0')\n",
      "compression_loss tensor(0.3086, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1002, вопрос+сжатые+сгенерированные=1218 оригинальная генерация=2469\n",
      "original_loss tensor(0.2786, device='cuda:0')\n",
      "compression_loss tensor(0.2738, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1183, вопрос+сжатые+сгенерированные=1399 оригинальная генерация=1907\n",
      "original_loss tensor(0.4825, device='cuda:0')\n",
      "compression_loss tensor(0.4779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=2508, вопрос+сжатые+сгенерированные=2724 оригинальная генерация=2678\n",
      "original_loss tensor(0.3807, device='cuda:0')\n",
      "compression_loss tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=585, вопрос+сжатые+сгенерированные=801 оригинальная генерация=2664\n",
      "original_loss tensor(0.4948, device='cuda:0')\n",
      "compression_loss tensor(0.4846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=554, вопрос+сжатые+сгенерированные=770 оригинальная генерация=1143\n",
      "original_loss tensor(0.4140, device='cuda:0')\n",
      "compression_loss tensor(0.4097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1343, вопрос+сжатые+сгенерированные=1559 оригинальная генерация=1759\n",
      "original_loss tensor(0.3280, device='cuda:0')\n",
      "compression_loss tensor(0.3219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1862, вопрос+сжатые+сгенерированные=2078 оригинальная генерация=3001\n",
      "original_loss tensor(0.3092, device='cuda:0')\n",
      "compression_loss tensor(0.3089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=801, вопрос+сжатые+сгенерированные=1017 оригинальная генерация=1554\n",
      "original_loss tensor(0.2920, device='cuda:0')\n",
      "compression_loss tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1708, вопрос+сжатые+сгенерированные=1924 оригинальная генерация=2260\n",
      "original_loss tensor(0.3349, device='cuda:0')\n",
      "compression_loss tensor(0.3204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=489, вопрос+сжатые+сгенерированные=705 оригинальная генерация=1478\n",
      "original_loss tensor(0.3371, device='cuda:0')\n",
      "compression_loss tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1731, вопрос+сжатые+сгенерированные=1947 оригинальная генерация=3321\n",
      "original_loss tensor(0.4434, device='cuda:0')\n",
      "compression_loss tensor(0.4323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=575, вопрос+сжатые+сгенерированные=791 оригинальная генерация=1269\n",
      "original_loss tensor(0.2951, device='cuda:0')\n",
      "compression_loss tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=890, вопрос+сжатые+сгенерированные=1106 оригинальная генерация=2693\n",
      "original_loss tensor(0.3159, device='cuda:0')\n",
      "compression_loss tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=951, вопрос+сжатые+сгенерированные=1167 оригинальная генерация=1346\n",
      "original_loss tensor(0.3819, device='cuda:0')\n",
      "compression_loss tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=224, вопрос+сжатые+сгенерированные=440 оригинальная генерация=2159\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as text_tqdm\n",
    "from hidden_capacity_reasoning.utils import tokenize_single_turn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "max_new_tokens = 400\n",
    "compression_tokens = 16\n",
    "\n",
    "evaluation_dataset = []\n",
    "correct_items = 0\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for dataset_pos in tqdm(range(len(correct_dataset))):\n",
    "    tokenized_turn = tokenize_single_turn(\n",
    "        question=base_prompt.format(question=correct_dataset[dataset_pos][\"problem\"]),\n",
    "        answer=correct_dataset[dataset_pos][\"model_answer\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    for key in tokenized_turn.keys():\n",
    "        tokenized_turn[key] = torch.tensor(tokenized_turn[key])\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    content_compression_mask = tokenized_turn[\"content_compression_mask\"]\n",
    "\n",
    "    input_part_end = (content_compression_mask == 0).nonzero()[-3][0]\n",
    "    # get only question part\n",
    "    question_input_ids = (\n",
    "        tokenized_turn[\"input_ids\"][: int(input_part_end) + 1].unsqueeze(0).cuda()\n",
    "    )\n",
    "    # print(tokenizer.decode(question_input_ids[-1]))\n",
    "\n",
    "    ########\n",
    "    ######## generate first part of tokens\n",
    "    ########\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # input_ids = torch.tensor(question_input_ids).cuda()\n",
    "        input_ids_embeds = model.get_input_embeddings()(question_input_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat(\n",
    "            [\n",
    "                input_ids_embeds,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        generated_ids_new = model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=torch.ones(\n",
    "                inputs_embeds.shape[:2],\n",
    "                device=\"cuda\",\n",
    "            ).long(),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        # break\n",
    "    generated_result = tokenizer.decode(generated_ids_new[-1])\n",
    "    # print(generated_result)\n",
    "\n",
    "    ########\n",
    "    ######## get original language loss\n",
    "    ########\n",
    "    labels = torch.cat(\n",
    "        [\n",
    "            question_input_ids.cuda(),\n",
    "            generated_ids_new.cuda(),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    # print(tokenizer.decode(labels[-1]))\n",
    "\n",
    "    question_content_mask = content_compression_mask[: int(input_part_end) + 1].clone()\n",
    "    question_content_mask[question_content_mask == 0] = 4\n",
    "    question_content_mask[question_content_mask == 1] = 0\n",
    "    question_content_mask[question_content_mask == 4] = 1\n",
    "    train_content_mask_new = torch.cat(\n",
    "        [\n",
    "            question_content_mask,\n",
    "            torch.ones(\n",
    "                generated_ids_new.shape[1] // 2,\n",
    "            )\n",
    "            * 0,\n",
    "            torch.ones(\n",
    "                generated_ids_new.shape[1] // 2,\n",
    "            ),\n",
    "        ]\n",
    "    ).long()\n",
    "    # print(question_content_mask)\n",
    "\n",
    "    generated_embeds = model.get_input_embeddings()(generated_ids_new)\n",
    "    new_input_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds,\n",
    "            generated_embeds,\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    labels[:, train_content_mask_new == 0] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        original_loss = model(\n",
    "            inputs_embeds=new_input_embeds,\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "    print(\"original_loss\", original_loss)\n",
    "    ########\n",
    "    ######## generate compress embeddings\n",
    "    ########\n",
    "\n",
    "    compression_tensor = torch.nn.Parameter(\n",
    "        torch.rand_like(\n",
    "            new_input_embeds[:, :compression_tokens, :],\n",
    "        )\n",
    "        * model.get_input_embeddings().weight.data.std(),\n",
    "        requires_grad=True,\n",
    "    )\n",
    "    compressed_inputs_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds.detach(),\n",
    "            compression_tensor,\n",
    "            generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "            # generated_embeds[:, : (max_new_tokens // 2), :].detach(),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    question_labels = question_input_ids.clone()\n",
    "    question_labels[0][question_content_mask == 0] = -100\n",
    "    question_labels = question_labels.cuda()\n",
    "    compressed_part = (torch.ones(compression_tensor.shape[:2]) * -100).long().cuda()\n",
    "\n",
    "    compressed_labels = torch.cat(\n",
    "        [\n",
    "            question_labels,\n",
    "            compressed_part,\n",
    "            generated_ids_new[:, -(max_new_tokens // 2) :],\n",
    "            # generated_ids_new[:, : (max_new_tokens // 2)],\n",
    "        ],\n",
    "        dim=-1,\n",
    "    )\n",
    "\n",
    "    ########\n",
    "    ######## train\n",
    "    ########\n",
    "    epoch_amount = 100\n",
    "\n",
    "    optimizer = torch.optim.Adam([compression_tensor], lr=0.1)\n",
    "    acclumulation_steps = 1\n",
    "    for epoch in range(epoch_amount):\n",
    "        compressed_inputs_embeds = torch.cat(\n",
    "            [\n",
    "                input_ids_embeds.detach(),\n",
    "                compression_tensor,\n",
    "                generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "                # generated_embeds[:, : (max_new_tokens // 2), :].detach(),\n",
    "                # compression_tensor,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        compression_loss = model(\n",
    "            inputs_embeds=compressed_inputs_embeds,\n",
    "            labels=compressed_labels,\n",
    "        ).loss\n",
    "        compression_loss.backward()\n",
    "        if (epoch + 1) % acclumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # print(epoch, compression_loss)\n",
    "        # if compression_loss.item() <= (original_loss.item() - 0.02):\n",
    "        #     break\n",
    "        # if (compression_loss.item() - 0.02) <= original_loss.item():\n",
    "        #     break\n",
    "        if compression_loss.item() <= original_loss.item():\n",
    "            break\n",
    "    print(\"compression_loss\", compression_loss)\n",
    "    ########\n",
    "    ######## evaluate\n",
    "    ########\n",
    "    with torch.no_grad():\n",
    "\n",
    "        compressed_inputs_embeds = torch.cat(\n",
    "            [\n",
    "                input_ids_embeds.detach(),\n",
    "                compression_tensor,\n",
    "                generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "                # generated_embeds[:, : (max_new_tokens // 2), :].detach(),\n",
    "                # compression_tensor,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        generated_ids_compressed = model.generate(\n",
    "            inputs_embeds=compressed_inputs_embeds,\n",
    "            attention_mask=torch.ones(\n",
    "                compressed_inputs_embeds.shape[:2],\n",
    "                device=\"cuda\",\n",
    "            ).long(),\n",
    "            max_new_tokens=4096,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        # break\n",
    "    generated_result = tokenizer.decode(generated_ids_compressed[-1])\n",
    "    gold_answer = correct_dataset[dataset_pos][\"answer\"]\n",
    "    answer = dataset_answer_filter(gold_answer)\n",
    "    model_answer = model_answer_filter(generated_result)\n",
    "    if is_equiv(answer, model_answer):\n",
    "        correct_items += 1\n",
    "        print(\"CORRECT\")\n",
    "    else:\n",
    "        print(\"WRONG\", gold_answer)\n",
    "        print(generated_result)\n",
    "    compressed_total_len = (\n",
    "        compression_tensor.shape[1]\n",
    "        + generated_embeds[:, -(max_new_tokens // 2) :, :].shape[1]\n",
    "        + generated_ids_compressed.shape[1]\n",
    "    )\n",
    "    original_total_len = len(\n",
    "        tokenizer.encode(\n",
    "            correct_dataset[dataset_pos][\"model_answer\"],\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"сгенерированно={generated_ids_compressed.shape[1]}, вопрос+сжатые+сгенерированные={compressed_total_len} оригинальная генерация={original_total_len}\"\n",
    "    )\n",
    "    evaluation_dataset.append(\n",
    "        {\n",
    "            \"original_total_len\": original_total_len,\n",
    "            \"compressed_total_len\": compressed_total_len,\n",
    "        }\n",
    "    )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e08c4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151646, 13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode([0])\n",
    "tokenizer.encode(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8642b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fe7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c623f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13392857142857142, 0.13392857142857142, 1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_dataset) / len(dataset), correct_items / len(dataset), correct_items / len(\n",
    "    correct_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceadd571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56056, 38923, 0.6943592122163551)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_total_len = 0\n",
    "compressed_total_len = 0\n",
    "for item in evaluation_dataset:\n",
    "    original_total_len += item[\"original_total_len\"]\n",
    "    compressed_total_len += item[\"compressed_total_len\"]\n",
    "original_total_len, compressed_total_len, compressed_total_len / original_total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### количество тестовых примеров 30\n",
    "# в качестве токенов выступали токен с индексом 1 и их было 16\n",
    "# (56056, 38735, 0.6910054231482803) - обучение с 8 токенами, до того как лосс не станет оригинальным, все ответы правильные (0.9666666666666667) просто ошибка парсинга\n",
    "# (56056, 37788, 0.6741116026830313) - - обучение с 8 токенами, до того как лосс не станет оригинальным - 0.01, все ответы правильные (0.9666666666666667) просто ошибка парсинга\n",
    "# (56056, 41238, 0.7356571999429142) - никакого обучения, просто вставка рандомных 8 векторов, точность 0.9666666666666667\n",
    "\n",
    "# сжимающие токены игнорируются, 8 штук\n",
    "# (56056, 44913, 0.8012166405023547) - 0.9666666666666667\n",
    "# сжимающие токены игнорируются, 2 штук\n",
    "# (56056, 41532, 0.7409019551876694) - 0.9666666666666667\n",
    "\n",
    "\n",
    "# сжимающие токены 16, только теперь я маскирую часть которая будет сжиматься, чтобы вычислить оригинальный лосс\n",
    "# (56056, 38923, 0.6943592122163551) - все правильные, все сгенерилось корректно\n",
    "\n",
    "#### количество тестовых 202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306049d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 1536])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_embeds[:, : (max_new_tokens // 2), :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
