{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffdb004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "224 202 0.9017857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"sdpa\",\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "# model = model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    # \"dim/hendrycks_math_train_12k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096\"\n",
    "    # \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    # \"dim/hendrycks_math_train_1k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    ")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    # test_size=250,\n",
    "    test_size=350,\n",
    "    # test_size=999,\n",
    "    # test_size=1,\n",
    "    seed=42,\n",
    ")\n",
    "dataset = dataset[\"test\"].filter(lambda x: x[\"model_answer\"].count(\"</think>\") == 1)\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "\n",
    "correct_dataset = []\n",
    "\n",
    "for pos, item in enumerate(dataset):\n",
    "    try:\n",
    "        answer = dataset_answer_filter(item[\"answer\"])\n",
    "        model_answer = model_answer_filter(item[\"model_answer\"])\n",
    "        # print(answer, model_answer)\n",
    "        # break\n",
    "        if is_equiv(answer, model_answer):\n",
    "            correct_dataset.append(item)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(dataset), len(correct_dataset), len(correct_dataset) / len(dataset))\n",
    "\n",
    "correct_dataset = correct_dataset[:30]\n",
    "len(correct_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f058fd3",
   "metadata": {},
   "source": [
    "## test time train generation (single train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "303e6e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eb54caed924a3a86fbb2f552846cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_loss tensor(0.2270, device='cuda:0')\n",
      "compression_loss tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1662, вопрос+сжатые+сгенерированные=1878 оригинальная генерация=1959\n",
      "original_loss tensor(0.2759, device='cuda:0')\n",
      "compression_loss tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=691, вопрос+сжатые+сгенерированные=907 оригинальная генерация=1125\n",
      "original_loss tensor(0.2294, device='cuda:0')\n",
      "compression_loss tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=896, вопрос+сжатые+сгенерированные=1112 оригинальная генерация=1548\n",
      "original_loss tensor(0.2595, device='cuda:0')\n",
      "compression_loss tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1896, вопрос+сжатые+сгенерированные=2112 оригинальная генерация=3960\n",
      "original_loss tensor(0.2939, device='cuda:0')\n",
      "compression_loss tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=458, вопрос+сжатые+сгенерированные=674 оригинальная генерация=748\n",
      "original_loss tensor(0.2882, device='cuda:0')\n",
      "compression_loss tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=787, вопрос+сжатые+сгенерированные=1003 оригинальная генерация=1080\n",
      "original_loss tensor(0.2947, device='cuda:0')\n",
      "compression_loss tensor(0.2677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1010, вопрос+сжатые+сгенерированные=1226 оригинальная генерация=1639\n",
      "original_loss tensor(0.2449, device='cuda:0')\n",
      "compression_loss tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1292, вопрос+сжатые+сгенерированные=1508 оригинальная генерация=1732\n",
      "original_loss tensor(0.2826, device='cuda:0')\n",
      "compression_loss tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=821, вопрос+сжатые+сгенерированные=1037 оригинальная генерация=1198\n",
      "original_loss tensor(0.2778, device='cuda:0')\n",
      "compression_loss tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=453, вопрос+сжатые+сгенерированные=669 оригинальная генерация=2314\n",
      "original_loss tensor(0.2608, device='cuda:0')\n",
      "compression_loss tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=637, вопрос+сжатые+сгенерированные=853 оригинальная генерация=1262\n",
      "original_loss tensor(0.2694, device='cuda:0')\n",
      "compression_loss tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=985, вопрос+сжатые+сгенерированные=1201 оригинальная генерация=1176\n",
      "original_loss tensor(0.2898, device='cuda:0')\n",
      "compression_loss tensor(0.2675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=510, вопрос+сжатые+сгенерированные=726 оригинальная генерация=1482\n",
      "original_loss tensor(0.3246, device='cuda:0')\n",
      "compression_loss tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=627, вопрос+сжатые+сгенерированные=843 оригинальная генерация=814\n",
      "original_loss tensor(0.2591, device='cuda:0')\n",
      "compression_loss tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1385, вопрос+сжатые+сгенерированные=1601 оригинальная генерация=2318\n",
      "original_loss tensor(0.2492, device='cuda:0')\n",
      "compression_loss tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=781, вопрос+сжатые+сгенерированные=997 оригинальная генерация=2469\n",
      "original_loss tensor(0.2055, device='cuda:0')\n",
      "compression_loss tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=885, вопрос+сжатые+сгенерированные=1101 оригинальная генерация=1907\n",
      "original_loss tensor(0.3752, device='cuda:0')\n",
      "compression_loss tensor(0.3535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=3034, вопрос+сжатые+сгенерированные=3250 оригинальная генерация=2678\n",
      "original_loss tensor(0.2832, device='cuda:0')\n",
      "compression_loss tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "'NoneType' object has no attribute 'group'\n",
      "WRONG 11\n",
      "1\n",
      "\n",
      "Okay, that's clearer now. So, maybe I can simplify this expression before plugging in x = 4.\n",
      "\n",
      "Let me expand the first term: (3x - 2)(4x + 1)\n",
      "\n",
      "Using the distributive property:\n",
      "\n",
      "3x * 4x = 12x²\n",
      "\n",
      "3x * 1 = 3x\n",
      "\n",
      "-2 * 4x = -8x\n",
      "\n",
      "-2 * 1 = -2\n",
      "\n",
      "So, adding those up: 12x² + 3x - 8x - 2 = 12x² - 5x - 2\n",
      "\n",
      "Now, the second term is (3x - 2)(4x). Let's expand that:\n",
      "\n",
      "3x * 4x = 12x²\n",
      "\n",
      "-2 * 4x = -8x\n",
      "\n",
      "So, that's 12x² - 8x\n",
      "\n",
      "Now, the entire expression is:\n",
      "\n",
      "[12x² - 5x - 2] - [12x² - 8x] + 1\n",
      "\n",
      "Let's distribute the negative sign to the second term:\n",
      "\n",
      "12x² - 5x - 2 - 12x² + 8x + 1\n",
      "\n",
      "Now, combine like terms:\n",
      "\n",
      "12x² - 12x² = 0\n",
      "\n",
      "-5x + 8x = 3x\n",
      "\n",
      "-2 + 1 = -1\n",
      "\n",
      "So, the simplified expression is 3x - 1\n",
      "\n",
      "Now, plug in x = 4:\n",
      "\n",
      "3(4) - 1 = 12 - 1 = 11\n",
      "\n",
      "So, the value is 11.\n",
      "\n",
      "Wait, let me double-check my steps to make sure I didn't make a mistake.\n",
      "\n",
      "First, expanding (3x - 2)(4x + 1):\n",
      "\n",
      "12x² + 3x - 8x - 2 = 12x² - 5x - 2. That seems correct.\n",
      "\n",
      "Then, expanding (3x - 2)(4x):\n",
      "\n",
      "12x² - 8x. That's correct.\n",
      "\n",
      "Subtracting the second expansion from the first:\n",
      "\n",
      "12x² - 5x - 2 - 12x² + 8x + 1\n",
      "\n",
      "Simplify:\n",
      "\n",
      "12x² - 12x² = 0\n",
      "\n",
      "-5x + 8x = 3x\n",
      "\n",
      "-2 + 1 = -1\n",
      "\n",
      "So, 3x - 1. Plugging in x = 4: 12 - 1 = 11. That seems right.\n",
      "\n",
      "Alternatively, maybe I can approach it differently without expanding. Let's see.\n",
      "\n",
      "Original expression: (3x - 2)(4x + 1) - (3x - 2)4x + 1\n",
      "\n",
      "Factor out (3x - 2) from the first two terms:\n",
      "\n",
      "(3x - 2)[(4x + 1) - 4x] + 1\n",
      "\n",
      "Simplify inside the brackets:\n",
      "\n",
      "(4x + 1 - 4x) = 1\n",
      "\n",
      "So, now it's (3x - 2)(1) + 1 = 3x - 2 + 1 = 3x - 1\n",
      "\n",
      "Same result. So, definitely, the expression simplifies to 3x - 1, which when x = 4 is 11.\n",
      "\n",
      "I think that's solid. I don't see any mistakes in my calculations. So, the answer should be 11.\n",
      "</think>\n",
      "\n",
      "The value of the expression when \\( x = 4 \\) is calculated as follows:\n",
      "\n",
      "First, simplify the expression:\n",
      "\\[\n",
      "(3x - 2)(4x + 1) - (3x - 2)4x + 1 = (3x - 2)[(4x + 1) - 4x] + 1 = (3x - 2)(1) + 1 = 3x - 2 + 1 = 3x - 1\n",
      "\\]\n",
      "\n",
      "Substitute \\( x = 4 \\):\n",
      "\\[\n",
      "3(4) - 1 = 12 - 1 = 11\n",
      "\\]\n",
      "\n",
      "**Answer:** 11<｜end▁of▁sentence｜>\n",
      "сгенерированно=894, вопрос+сжатые+сгенерированные=1110 оригинальная генерация=2664\n",
      "original_loss tensor(0.3187, device='cuda:0')\n",
      "compression_loss tensor(0.2887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=696, вопрос+сжатые+сгенерированные=912 оригинальная генерация=1143\n",
      "original_loss tensor(0.3054, device='cuda:0')\n",
      "compression_loss tensor(0.2795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=4024, вопрос+сжатые+сгенерированные=4240 оригинальная генерация=1759\n",
      "original_loss tensor(0.2425, device='cuda:0')\n",
      "compression_loss tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=2624, вопрос+сжатые+сгенерированные=2840 оригинальная генерация=3001\n",
      "original_loss tensor(0.2341, device='cuda:0')\n",
      "compression_loss tensor(0.2098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=966, вопрос+сжатые+сгенерированные=1182 оригинальная генерация=1554\n",
      "original_loss tensor(0.2270, device='cuda:0')\n",
      "compression_loss tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1392, вопрос+сжатые+сгенерированные=1608 оригинальная генерация=2260\n",
      "original_loss tensor(0.2278, device='cuda:0')\n",
      "compression_loss tensor(0.2059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1869, вопрос+сжатые+сгенерированные=2085 оригинальная генерация=1478\n",
      "original_loss tensor(0.2346, device='cuda:0')\n",
      "compression_loss tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=1625, вопрос+сжатые+сгенерированные=1841 оригинальная генерация=3321\n",
      "original_loss tensor(0.3020, device='cuda:0')\n",
      "compression_loss tensor(0.2730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=485, вопрос+сжатые+сгенерированные=701 оригинальная генерация=1269\n",
      "original_loss tensor(0.2668, device='cuda:0')\n",
      "compression_loss tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=895, вопрос+сжатые+сгенерированные=1111 оригинальная генерация=2693\n",
      "original_loss tensor(0.2632, device='cuda:0')\n",
      "compression_loss tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=876, вопрос+сжатые+сгенерированные=1092 оригинальная генерация=1346\n",
      "original_loss tensor(0.3102, device='cuda:0')\n",
      "compression_loss tensor(0.2674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "CORRECT\n",
      "сгенерированно=363, вопрос+сжатые+сгенерированные=579 оригинальная генерация=2159\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as text_tqdm\n",
    "from hidden_capacity_reasoning.utils import tokenize_single_turn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "max_new_tokens = 400\n",
    "compression_tokens = 16\n",
    "\n",
    "evaluation_dataset = []\n",
    "correct_items = 0\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for dataset_pos in tqdm(range(len(correct_dataset))):\n",
    "    tokenized_turn = tokenize_single_turn(\n",
    "        question=base_prompt.format(question=correct_dataset[dataset_pos][\"problem\"]),\n",
    "        answer=correct_dataset[dataset_pos][\"model_answer\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    for key in tokenized_turn.keys():\n",
    "        tokenized_turn[key] = torch.tensor(tokenized_turn[key])\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    content_compression_mask = tokenized_turn[\"content_compression_mask\"]\n",
    "\n",
    "    input_part_end = (content_compression_mask == 0).nonzero()[-3][0]\n",
    "    # get only question part\n",
    "    question_input_ids = (\n",
    "        tokenized_turn[\"input_ids\"][: int(input_part_end) + 1].unsqueeze(0).cuda()\n",
    "    )\n",
    "    # print(tokenizer.decode(question_input_ids[-1]))\n",
    "\n",
    "    ########\n",
    "    ######## generate first part of tokens\n",
    "    ########\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # input_ids = torch.tensor(question_input_ids).cuda()\n",
    "        input_ids_embeds = model.get_input_embeddings()(question_input_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat(\n",
    "            [\n",
    "                input_ids_embeds,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        generated_ids_new = model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=torch.ones(\n",
    "                inputs_embeds.shape[:2],\n",
    "                device=\"cuda\",\n",
    "            ).long(),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        # break\n",
    "    generated_result = tokenizer.decode(generated_ids_new[-1])\n",
    "    # print(generated_result)\n",
    "\n",
    "    ########\n",
    "    ######## get original language loss\n",
    "    ########\n",
    "    labels = torch.cat(\n",
    "        [\n",
    "            question_input_ids.cuda(),\n",
    "            generated_ids_new.cuda(),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    # print(tokenizer.decode(labels[-1]))\n",
    "\n",
    "    question_content_mask = content_compression_mask[: int(input_part_end) + 1].clone()\n",
    "    question_content_mask[question_content_mask == 0] = 4\n",
    "    question_content_mask[question_content_mask == 1] = 0\n",
    "    question_content_mask[question_content_mask == 4] = 1\n",
    "    train_content_mask_new = torch.cat(\n",
    "        [\n",
    "            question_content_mask,\n",
    "            torch.ones(\n",
    "                generated_ids_new.shape[1],\n",
    "            ),\n",
    "        ]\n",
    "    ).long()\n",
    "    # print(question_content_mask)\n",
    "\n",
    "    generated_embeds = model.get_input_embeddings()(generated_ids_new)\n",
    "    new_input_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds,\n",
    "            generated_embeds,\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    labels[:, train_content_mask_new == 0] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        original_loss = model(\n",
    "            inputs_embeds=new_input_embeds,\n",
    "            labels=labels,\n",
    "        ).loss\n",
    "    print(\"original_loss\", original_loss)\n",
    "    ########\n",
    "    ######## generate compress embeddings\n",
    "    ########\n",
    "\n",
    "    compression_tensor = torch.nn.Parameter(\n",
    "        torch.rand_like(\n",
    "            new_input_embeds[:, :compression_tokens, :],\n",
    "        )\n",
    "        * model.get_input_embeddings().weight.data.std(),\n",
    "        requires_grad=True,\n",
    "    )\n",
    "    compressed_inputs_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds.detach(),\n",
    "            compression_tensor,\n",
    "            generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "            # generated_embeds[:, : (max_new_tokens // 2), :].detach(),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    question_labels = question_input_ids.clone()\n",
    "    question_labels[0][question_content_mask == 0] = -100\n",
    "    question_labels = question_labels.cuda()\n",
    "    compressed_part = (torch.ones(compression_tensor.shape[:2]) * -100).long().cuda()\n",
    "\n",
    "    compressed_labels = torch.cat(\n",
    "        [\n",
    "            question_labels,\n",
    "            compressed_part,\n",
    "            generated_ids_new[:, -(max_new_tokens // 2) :],\n",
    "            # generated_ids_new[:, : (max_new_tokens // 2)],\n",
    "        ],\n",
    "        dim=-1,\n",
    "    )\n",
    "\n",
    "    ########\n",
    "    ######## train\n",
    "    ########\n",
    "    epoch_amount = 100\n",
    "\n",
    "    optimizer = torch.optim.Adam([compression_tensor], lr=0.1)\n",
    "    acclumulation_steps = 1\n",
    "    for epoch in range(epoch_amount):\n",
    "        compressed_inputs_embeds = torch.cat(\n",
    "            [\n",
    "                input_ids_embeds.detach(),\n",
    "                compression_tensor,\n",
    "                generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "                # generated_embeds[:, : (max_new_tokens // 2), :].detach(),\n",
    "                # compression_tensor,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        compression_loss = model(\n",
    "            inputs_embeds=compressed_inputs_embeds,\n",
    "            labels=compressed_labels,\n",
    "        ).loss\n",
    "        compression_loss.backward()\n",
    "        if (epoch + 1) % acclumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # print(epoch, compression_loss)\n",
    "        if compression_loss.item() <= (original_loss.item() - 0.02):\n",
    "            break\n",
    "        # if (compression_loss.item() - 0.02) <= original_loss.item():\n",
    "        #     break\n",
    "        # if compression_loss.item() <= original_loss.item():\n",
    "        #     break\n",
    "    print(\"compression_loss\", compression_loss)\n",
    "    ########\n",
    "    ######## evaluate\n",
    "    ########\n",
    "    with torch.no_grad():\n",
    "\n",
    "        compressed_inputs_embeds = torch.cat(\n",
    "            [\n",
    "                input_ids_embeds.detach(),\n",
    "                compression_tensor,\n",
    "                generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "                # generated_embeds[:, : (max_new_tokens // 2), :].detach(),\n",
    "                # compression_tensor,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        generated_ids_compressed = model.generate(\n",
    "            inputs_embeds=compressed_inputs_embeds,\n",
    "            attention_mask=torch.ones(\n",
    "                compressed_inputs_embeds.shape[:2],\n",
    "                device=\"cuda\",\n",
    "            ).long(),\n",
    "            max_new_tokens=4096,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        # break\n",
    "    generated_result = tokenizer.decode(generated_ids_compressed[-1])\n",
    "    gold_answer = correct_dataset[dataset_pos][\"answer\"]\n",
    "    answer = dataset_answer_filter(gold_answer)\n",
    "    model_answer = model_answer_filter(generated_result)\n",
    "    if is_equiv(answer, model_answer):\n",
    "        correct_items += 1\n",
    "        print(\"CORRECT\")\n",
    "    else:\n",
    "        print(\"WRONG\", gold_answer)\n",
    "        print(generated_result)\n",
    "    compressed_total_len = (\n",
    "        compression_tensor.shape[1]\n",
    "        + generated_embeds[:, -(max_new_tokens // 2) :, :].shape[1]\n",
    "        + generated_ids_compressed.shape[1]\n",
    "    )\n",
    "    original_total_len = len(\n",
    "        tokenizer.encode(\n",
    "            correct_dataset[dataset_pos][\"model_answer\"],\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"сгенерированно={generated_ids_compressed.shape[1]}, вопрос+сжатые+сгенерированные={compressed_total_len} оригинальная генерация={original_total_len}\"\n",
    "    )\n",
    "    evaluation_dataset.append(\n",
    "        {\n",
    "            \"original_total_len\": original_total_len,\n",
    "            \"compressed_total_len\": compressed_total_len,\n",
    "        }\n",
    "    )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e08c4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151646, 13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode([0])\n",
    "tokenizer.encode(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8642b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fe7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c623f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13392857142857142, 0.12946428571428573, 0.9666666666666667)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_dataset) / len(dataset), correct_items / len(dataset), correct_items / len(\n",
    "    correct_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ceadd571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56056, 41999, 0.7492329099471957)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_total_len = 0\n",
    "compressed_total_len = 0\n",
    "for item in evaluation_dataset:\n",
    "    original_total_len += item[\"original_total_len\"]\n",
    "    compressed_total_len += item[\"compressed_total_len\"]\n",
    "original_total_len, compressed_total_len, compressed_total_len / original_total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в качестве токенов выступали токен с индексом 1 и их было 16\n",
    "# (56056, 38735, 0.6910054231482803) - обучение с 8 токенами, до того как лосс не станет оригинальным, все ответы правильные (0.9666666666666667) просто ошибка парсинга\n",
    "# (56056, 37788, 0.6741116026830313) - - обучение с 8 токенами, до того как лосс не станет оригинальным - 0.01, все ответы правильные (0.9666666666666667) просто ошибка парсинга\n",
    "# (56056, 41238, 0.7356571999429142) - никакого обучения, просто вставка рандомных 8 векторов, точность 0.9666666666666667\n",
    "\n",
    "# сжимающие токены игнорируются, 8 штук\n",
    "# (56056, 44913, 0.8012166405023547) - 0.9666666666666667\n",
    "# сжимающие токены игнорируются, 2 штук\n",
    "# (56056, 41532, 0.7409019551876694) - 0.9666666666666667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306049d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 1536])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_embeds[:, : (max_new_tokens // 2), :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
