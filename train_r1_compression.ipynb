{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b09a812477c4e3b8a711211a6f6894b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 956.22it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 38.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 580\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 895\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "trainable params: 254,197,760 || all params: 3,577,359,360 || trainable%: 7.1057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_585463/158584843.py:174: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='169' max='169' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [169/169 01:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=169, training_loss=0.0, metrics={'train_runtime': 61.8415, 'train_samples_per_second': 10.931, 'train_steps_per_second': 2.733, 'total_flos': 0.0, 'train_loss': 0.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"hidden_capacity_reasoning\"\n",
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from trl import (\n",
    "    ModelConfig,\n",
    "    ScriptArguments,\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    "    TrlParser,\n",
    "    get_kbit_device_map,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    generate_train_examples,\n",
    "    pad_train_examples,\n",
    "    tokenize_single_turn,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "import types\n",
    "\n",
    "# need for auto SFTTrainer patch(possible increase speed)\n",
    "from unsloth import is_bfloat16_supported\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    EOS_TOKEN_ID,\n",
    "    TEXT_TOKEN_ID,\n",
    "    WINDOW_SIZE,\n",
    "    VISION_START,\n",
    "    VISION_END,\n",
    "    find_all_linear_names_v3,\n",
    ")\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from hidden_capacity_reasoning.models import (\n",
    "    Qwen2ForCausalLMCompressionV1,\n",
    "    Qwen2ModelEmbedPoolerV1,\n",
    "    Qwen2ForCausalLMCompressionV2,\n",
    "    Qwen2ModelEmbedPoolerV2,\n",
    "    Qwen2ForCausalLMCompressionV3,\n",
    "    Qwen2ModelEmbedPoolerV3,\n",
    ")\n",
    "\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_name = \"my_r1_model_v3\"\n",
    "model = Qwen2ForCausalLMCompressionV3.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.model.requires_grad_(False)\n",
    "# model = model.to(torch.bfloat16)\n",
    "\n",
    "# temp_model = Qwen2ModelEmbedPoolerV3.from_pretrained(\n",
    "#     model_name,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map={\"\": 0},\n",
    "#     # quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n",
    "# )\n",
    "# print(\n",
    "#     model.embed_pooler.load_state_dict(\n",
    "#         temp_model.state_dict(),\n",
    "#         strict=False,\n",
    "#     ),\n",
    "# )\n",
    "# temp_model = temp_model.cpu()\n",
    "# del temp_model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "dataset = load_dataset(\"dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=10, seed=42)\n",
    "\n",
    "# test pass\n",
    "tokenize_single_turn(\n",
    "    question=dataset[\"train\"][0][\"question\"],\n",
    "    answer=dataset[\"train\"][0][\"answer\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "train_examples = [\n",
    "    tokenize_single_turn(tokenizer=tokenizer, **item)\n",
    "    for item in tqdm(dataset[\"train\"].to_list()[:3])\n",
    "]\n",
    "\n",
    "prepared_train_examples = []\n",
    "for item in tqdm(train_examples):\n",
    "    for example in generate_train_examples(\n",
    "        dataset_batch=[item],\n",
    "        window_size=WINDOW_SIZE,\n",
    "    ):\n",
    "        prepared_train_examples.append(example)\n",
    "\n",
    "print(\n",
    "    \"max_len\",\n",
    "    max([len(item[\"original_tokens\"]) for item in prepared_train_examples]),\n",
    ")\n",
    "\n",
    "new_dataset = Dataset.from_list(prepared_train_examples)\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    padded_batch = pad_train_examples(\n",
    "        train_examples=batch,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    padded_batch = {\n",
    "        \"replaced_original_tokens\": padded_batch[\"replaced_original_tokens\"][\n",
    "            \"input_ids\"\n",
    "        ],\n",
    "        \"compressed_input_ids\": padded_batch[\"compressed_input_ids\"][\"input_ids\"],\n",
    "        \"original_tokens\": padded_batch[\"original_tokens\"][\"input_ids\"],\n",
    "        \"attention_mask\": padded_batch[\"compressed_input_ids\"][\"attention_mask\"],\n",
    "        \"labels\": padded_batch[\"compressed_input_ids\"][\"input_ids\"],\n",
    "        \"content_compression_mask\": padded_batch[\"content_compression_mask\"][\n",
    "            \"input_ids\"\n",
    "        ],\n",
    "    }\n",
    "    for key in padded_batch.keys():\n",
    "        padded_batch[key] = torch.tensor(padded_batch[key])\n",
    "    skip_ids = [\n",
    "        TEXT_TOKEN_ID,\n",
    "        EOS_TOKEN_ID,\n",
    "        VISION_START,\n",
    "        VISION_END,\n",
    "    ]\n",
    "    for skip_id in skip_ids:\n",
    "        padded_batch[\"labels\"][padded_batch[\"labels\"] == skip_id] = -100\n",
    "    # —á–∞—Å—Ç—å –∏–Ω–ø—É—Ç–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    last_index = (padded_batch[\"content_compression_mask\"] == 1).long().nonzero()[-1][1]\n",
    "    padded_batch[\"labels\"][:, :last_index][\n",
    "        padded_batch[\"content_compression_mask\"][:, :last_index] == 1\n",
    "    ] = -100\n",
    "    # print(padded_batch)\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    target_modules=find_all_linear_names_v3(model=model),\n",
    "    modules_to_save=[\n",
    "        \"embed_pooler.model.embed_tokens\",\n",
    "        \"embed_pooler.weight_pooler\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "formatted_date = datetime.fromtimestamp(time.time()).strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "model.embed_pooler = prepare_model_for_kbit_training(model.embed_pooler)\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=new_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=1,  # 90,  # Set this for 1 full training run.\n",
    "        # num_train_epochs=90,  # Set this for 1 full training run.\n",
    "        # max_steps=10000,\n",
    "        learning_rate=1e-4,\n",
    "        bf16=True,\n",
    "        # fp16=model.dtype == torch.float16,\n",
    "        logging_steps=8,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=f\"outputs/{formatted_date}\",\n",
    "        # report_to=\"wandb\",\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "        # gradient_checkpointing=True,\n",
    "        save_steps=10000,\n",
    "        run_name=formatted_date,\n",
    "    ),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"my_r1_model_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my_r1_model_v3/tokenizer_config.json',\n",
       " 'my_r1_model_v3/special_tokens_map.json',\n",
       " 'my_r1_model_v3/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"my_r1_model_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.base_model.embed_pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1536])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch.bfloat16)\n",
    "model.embed_pooler(\n",
    "    torch.randn(\n",
    "        2,\n",
    "        2,\n",
    "        1536,\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.bfloat16,\n",
    "    )\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embed_pooler.model.embed_tokens.modules_to_save.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.weight_pooler.modules_to_save.default.weight, Requires Gradient: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, Requires Gradient: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.model.embed_pooler.model.embed_tokens.modules_to_save.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in trainer.model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, Requires Gradient: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-23 00:08:33 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c66b12ebed4458a14c2bef3043432d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from hidden_capacity_reasoning.models import (\n",
    "    Qwen2ForCausalLMCompressionV1,\n",
    "    Qwen2ModelEmbedPoolerV1,\n",
    "    Qwen2ForCausalLMCompressionV2,\n",
    "    Qwen2ModelEmbedPoolerV2,\n",
    ")\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_name = \"r1_compressor_v2\"\n",
    "model = Qwen2ForCausalLMCompressionV2.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    # \"outputs/2025_04_19_17_17_34_493839/checkpoint-210000\",\n",
    "    # \"outputs/2025_04_19_17_17_34_493839/checkpoint-50000\",\n",
    "    # \"outputs/2025_04_21_19_23_11_642509/checkpoint-10000\",\n",
    "    \"outputs/2025_04_22_01_43_43_347583/checkpoint-90000\",\n",
    "    # \"outputs/2025_04_22_01_43_43_347583/checkpoint-10000\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(155, 136, 0.8774193548387097)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"dim/hendrycks_math_train_12k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096\"\n",
    "    # \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096\"\n",
    ")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    test_size=250,\n",
    "    seed=42,\n",
    ")\n",
    "dataset = dataset[\"test\"].filter(lambda x: x[\"model_answer\"].count(\"</think>\") == 1)\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "\n",
    "correct_dataset = []\n",
    "\n",
    "for pos, item in enumerate(dataset):\n",
    "    try:\n",
    "        answer = dataset_answer_filter(item[\"answer\"])\n",
    "        model_answer = model_answer_filter(item[\"model_answer\"])\n",
    "        # print(answer, model_answer)\n",
    "        # break\n",
    "        if is_equiv(answer, model_answer):\n",
    "            correct_dataset.append(item)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "len(dataset), len(correct_dataset), len(correct_dataset) / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Below is a magic square, meaning that the sum of the numbers in each row, in each column, and in each of the $2$ main diagonals are equal. What is the value of $n$?\\n\\n[asy]size(125);\\nfor(int i = 0; i<4; ++i)\\n{\\n\\ndraw((0,i)--(3,i),linewidth(1));\\n}\\n\\nfor(int j = 0; j<4; ++j)\\n{\\n\\ndraw((j,0)--(j,3),linewidth(1));\\n}\\n\\nlabel(\"$n-3$\",(.5,.5));\\nlabel(\"3\",(.5,1.5));\\nlabel(\"$n+1$\",(.5,2.5));\\n\\nlabel(\"$n+2$\",(1.5,.5));\\nlabel(\"$2n-9$\",(1.5,1.5));\\nlabel(\"$1$\",(1.5,2.5));\\n\\nlabel(\"$2$\",(2.5,.5));\\nlabel(\"$n$\",(2.5,1.5));\\nlabel(\"$n-1$\",(2.5,2.5));\\n[/asy]',\n",
       " 'solution': 'First, we can evaluate the sum across the first row, which gives $(n+1)+1+(n-1)=2n+1$.  Evaluate the sum of the entries across the second row, $3+(2n-9)+n=3n-6$. Now, since we have a magic square, these two sums are equal.  So $2n+1=3n-6$. Isolating $n$, we obtain $n = \\\\boxed{7}$.\\n\\nThe square will look like: [asy] size(2cm);\\ndraw((0,0)--(3,0)--(3,3)--(0,3)--cycle,linewidth(1));\\ndraw((1,0)--(1,3),linewidth(1));\\ndraw((2,0)--(2,3),linewidth(1));\\ndraw((0,1)--(3,1),linewidth(1));\\ndraw((0,2)--(3,2),linewidth(1));\\nlabel(\"8\",(.5,2.5));\\nlabel(\"1\",(1.5,2.5));\\nlabel(\"6\",(2.5,2.5));\\nlabel(\"3\",(.5,1.5));\\nlabel(\"5\",(1.5,1.5));\\nlabel(\"7\",(2.5,1.5));\\nlabel(\"4\",(.5,.5));\\nlabel(\"9\",(1.5,.5));\\nlabel(\"2\",(2.5,.5));\\n[/asy]',\n",
       " 'answer': '7',\n",
       " 'subject': 'Prealgebra',\n",
       " 'level': 5,\n",
       " 'unique_id': 'test/prealgebra/1930.json',\n",
       " 'model_answer': \"Okay, so I have this magic square problem here, and I need to find the value of \\\\( n \\\\). A magic square is one where the sums of the numbers in each row, each column, and both main diagonals are all equal. The Asymptote code is provided, which I can visualize. Let me try to reconstruct the magic square based on the labels given.\\n\\nFirst, I'll try to draw it out mentally. The Asymptote code is creating a 3x3 grid. Each cell is labeled with some expression involving \\\\( n \\\\). Let me write down the positions and the expressions.\\n\\nLooking at the Asymptote code:\\n\\n- The first row (top row) is labeled at positions (0.5, 0.5), (0.5, 1.5), and (0.5, 2.5), so that's the top row. The labels are:\\n  - \\\\( n - 3 \\\\) at the left,\\n  - \\\\( 3 \\\\) in the center,\\n  - \\\\( n + 1 \\\\) at the right.\\n  \\nSo, the top row is: \\\\( n - 3 \\\\), \\\\( 3 \\\\), \\\\( n + 1 \\\\).\\n\\n- The second row (middle row) is labeled at positions (1.5, 0.5), (1.5, 1.5), and (1.5, 2.5). The labels are:\\n  - \\\\( n + 2 \\\\) on the left,\\n  - \\\\( 2n - 9 \\\\) in the center,\\n  - \\\\( 1 \\\\) on the right.\\n  \\nSo, the middle row is: \\\\( n + 2 \\\\), \\\\( 2n - 9 \\\\), \\\\( 1 \\\\).\\n\\n- The third row (bottom row) is labeled at positions (2.5, 0.5), (2.5, 1.5), and (2.5, 2.5). The labels are:\\n  - \\\\( 2 \\\\) on the left,\\n  - \\\\( n \\\\) in the center,\\n  - \\\\( n - 1 \\\\) on the right.\\n  \\nSo, the bottom row is: \\\\( 2 \\\\), \\\\( n \\\\), \\\\( n - 1 \\\\).\\n\\nLet me write this down as a table for clarity:\\n\\n\\\\[\\n\\\\begin{array}{|c|c|c|}\\n\\\\hline\\nn - 3 & 3 & n + 1 \\\\\\\\\\n\\\\hline\\nn + 2 & 2n - 9 & 1 \\\\\\\\\\n\\\\hline\\n2 & n & n - 1 \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n\\\\]\\n\\nAlright, so now I need to find \\\\( n \\\\) such that the sums of each row, column, and both main diagonals are equal. Let's denote the magic constant (the common sum) as \\\\( S \\\\). So, I need to express \\\\( S \\\\) in terms of \\\\( n \\\\) and set up equations accordingly.\\n\\nFirst, let me compute the sum of the first row:\\n\\n\\\\( (n - 3) + 3 + (n + 1) \\\\)\\n\\nSimplify that:\\n\\n\\\\( n - 3 + 3 + n + 1 = 2n + 1 \\\\)\\n\\nSo, the sum of the first row is \\\\( 2n + 1 \\\\). That gives me one equation.\\n\\nNext, let me check the sum of the second row:\\n\\n\\\\( (n + 2) + (2n - 9) + 1 \\\\)\\n\\nSimplify:\\n\\n\\\\( n + 2 + 2n - 9 + 1 = 3n - 6 \\\\)\\n\\nSo, the sum of the second row is \\\\( 3n - 6 \\\\). Since it's a magic square, this should also equal \\\\( S \\\\). Therefore, I have:\\n\\n\\\\( 2n + 1 = 3n - 6 \\\\)\\n\\nLet me solve this equation for \\\\( n \\\\):\\n\\nSubtract \\\\( 2n \\\\) from both sides:\\n\\n\\\\( 1 = n - 6 \\\\)\\n\\nAdd 6 to both sides:\\n\\n\\\\( 7 = n \\\\)\\n\\nSo, \\\\( n = 7 \\\\). Hmm, let me check if this is consistent with the other rows, columns, and diagonals.\\n\\nLet's compute the third row:\\n\\n\\\\( 2 + 7 + (7 - 1) = 2 + 7 + 6 = 15 \\\\)\\n\\nSo, the sum is 15. Let's verify if this holds with the columns and diagonals.\\n\\nFirst, the first column:\\n\\n\\\\( (n - 3) + (n + 2) + 2 \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (7 - 3) + (7 + 2) + 2 = 4 + 9 + 2 = 15 \\\\)\\n\\nGood, that's 15.\\n\\nSecond column:\\n\\n\\\\( 3 + (2n - 9) + n \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( 3 + (14 - 9) + 7 = 3 + 5 + 7 = 15 \\\\)\\n\\nAlso 15.\\n\\nThird column:\\n\\n\\\\( (n + 1) + 1 + (n - 1) \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (8) + 1 + 6 = 15 \\\\)\\n\\nGood.\\n\\nNow, the main diagonals.\\n\\nFirst diagonal (top-left to bottom-right):\\n\\n\\\\( (n - 3) + (2n - 9) + (n - 1) \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (4) + (14 - 9) + 6 = 4 + 5 + 6 = 15 \\\\)\\n\\nSecond diagonal (top-right to bottom-left):\\n\\n\\\\( (n + 1) + (2n - 9) + 2 \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (8) + (14 - 9) + 2 = 8 + 5 + 2 = 15 \\\\)\\n\\nPerfect, both diagonals sum to 15.\\n\\nSo, all rows, columns, and diagonals sum to 15 when \\\\( n = 7 \\\\).\\n\\nWait, but let me double-check the initial row sum. When \\\\( n = 7 \\\\), the first row sum was \\\\( 2n + 1 = 14 + 1 = 15 \\\\). Yep, that's consistent.\\n\\nSo, it seems that \\\\( n = 7 \\\\) is the correct value.\\n\\nBut just to be thorough, let me check the second row sum when \\\\( n = 7 \\\\):\\n\\nSecond row: \\\\( (7 + 2) + (14 - 9) + 1 = 9 + 5 + 1 = 15 \\\\). Yep, that's correct.\\n\\nSimilarly, the third row was 2 + 7 + 6 = 15.\\n\\nAll columns, diagonals, and rows check out.\\n\\nSo, I think \\\\( n = 7 \\\\) is the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( n \\\\) is \\\\boxed{7}.\\n</think>\\n\\nGiven a magic square where the sum of the numbers in each row, column, and the two main diagonals are equal, we need to find the value of \\\\( n \\\\).\\n\\nThe magic square is reconstructed as follows:\\n\\n\\\\[\\n\\\\begin{array}{|c|c|c|}\\n\\\\hline\\nn - 3 & 3 & n + 1 \\\\\\\\\\n\\\\hline\\nn + 2 & 2n - 9 & 1 \\\\\\\\\\n\\\\hline\\n2 & n & n - 1 \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n\\\\]\\n\\nWe denote the magic constant (the common sum) as \\\\( S \\\\).\\n\\n1. Calculate the sum of the first row:\\n   \\\\[\\n   (n - 3) + 3 + (n + 1) = 2n + 1\\n   \\\\]\\n   This gives us the equation:\\n   \\\\[\\n   2n + 1 = S\\n   \\\\]\\n\\n2. Calculate the sum of the second row:\\n   \\\\[\\n   (n + 2) + (2n - 9) + 1 = 3n - 6\\n   \\\\]\\n   This gives us the equation:\\n   \\\\[\\n   3n - 6 = S\\n   \\\\]\\n\\n3. Equate the two expressions for \\\\( S \\\\):\\n   \\\\[\\n   2n + 1 = 3n - 6\\n   \\\\]\\n   Solving for \\\\( n \\\\):\\n   \\\\[\\n   1 = n - 6 \\\\implies n = 7\\n   \\\\]\\n\\n4. Verify the value of \\\\( n \\\\) by checking the sums of the third row, columns, and diagonals:\\n   - Third row: \\\\( 2 + 7 + 6 = 15 \\\\)\\n   - First column: \\\\( 4 + 9 + 2 = 15 \\\\)\\n   - Second column: \\\\( 3 + 5 + 7 = 15 \\\\)\\n   - Third column: \\\\( 8 + 1 + 6 = 15 \\\\)\\n   - First diagonal: \\\\( 4 + 5 + 6 = 15 \\\\)\\n   - Second diagonal: \\\\( 8 + 5 + 2 = 15 \\\\)\\n\\nAll sums are consistent and equal to 15 when \\\\( n = 7 \\\\).\\n\\nThus, the value of \\\\( n \\\\) is \\\\(\\\\boxed{7}\\\\).\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>Problem: An elephant and a lion are currently 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour.  How many minutes will it take for the lion to catch the elephant?\n",
      "\n",
      "Please reason step by step, and put your final answer within \\boxed{}.<ÔΩúAssistantÔΩú><think>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards it at 24 mph. So, their relative speed is the difference between their speeds because they're moving towards each other in terms of closing the distance. Wait, no, actually, the elephant is moving away, so the lion is closing the distance at the sum of their speeds? Or is it the difference?\\n\\nLet me clarify. If two objects are moving towards each other, their relative speed is the sum of their individual speeds. But in this case, the elephant is moving away, so the lion is moving towards the moving elephant. So, the distance between them is decreasing at a rate equal to the lion's speed minus the elephant's speed. That makes sense because the lion is closing the gap while the elephant is moving away.\\n\\nSo, the relative speed is 24 mph (lion's speed) minus 19 mph (elephant's speed). Let me calculate that: 24 - 19 = 5 mph. So, the distance between them is decreasing at 5 miles per hour.\\n\\nNow, the initial distance between them is 1 mile. So, if the distance is decreasing at 5 mph, how long will it take to cover that 1 mile at 5 mph?\\n\\nI remember that time is equal to distance divided by speed. So, time = 1 mile / 5 mph. Let me compute that: 1 divided by 5 is 0.2 hours. But the question asks for the time in minutes, so I need to convert hours to minutes. There are 60 minutes in an hour, so 0.2 hours multiplied by 60 minutes per hour is 12 minutes.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph towards the elephant, and the elephant is moving away at 19 mph, then the relative speed is indeed 24 - 19 = 5 mph. So, the lion is closing the gap at 5 mph. Since they start 1 mile apart, the time to catch up is 1 / 5 hours, which is 12 minutes. That seems right.\\n\\nBut just to make sure I didn't make a mistake, let me think about it another way. Suppose the lion starts 1 mile behind the elephant. The elephant is moving away at 19 mph, so in one hour, it would have moved 19 miles away. Meanwhile, the lion is moving towards the elephant at 24 mph, so in one hour, it would have moved 24 miles towards the elephant. So, the distance between them after one hour would be 24 - 19 = 5 miles. Wait, that doesn't seem right because the initial distance was 1 mile. So, in one hour, the lion would have covered 24 miles, and the elephant would have moved 19 miles away, so the distance between them would be 24 - 19 = 5 miles. But that's after one hour, not one mile. So, to cover the initial 1 mile, it would take less than an hour.\\n\\nWait, maybe I should model this with equations. Let me denote the time it takes for the lion to catch the elephant as t hours. In that time, the lion would have traveled 24t miles towards the elephant, and the elephant would have traveled 19t miles away from the lion. Since they start 1 mile apart, the total distance the lion needs to cover to catch the elephant is 1 mile plus the distance the elephant has moved away. So, the equation would be:\\n\\n24t = 1 + 19t\\n\\nSubtracting 19t from both sides:\\n\\n5t = 1\\n\\nSo, t = 1/5 hours, which is 12 minutes. Okay, that matches my earlier calculation. So, that seems correct.\\n\\nAlternatively, I can think about it in terms of relative speed. The relative speed is 24 - 19 = 5 mph, as established before. So, the time to cover the 1 mile is 1 / 5 hours, which is 12 minutes. Yep, that's consistent.\\n\\nI think I'm confident now that 12 minutes is the correct answer. So, the lion will catch the elephant in 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs towards the elephant at 24 miles per hour. \\n\\nTo determine how long it will take for the lion to catch the elephant, we need to consider their relative speed. Since the lion is moving towards the elephant and the elephant is moving away, the relative speed at which the distance between them is decreasing is the difference between their speeds:\\n\\n\\\\[\\n24 \\\\text{ mph} - 19 \\\\text{ mph} = 5 \\\\text{ mph}\\n\\\\]\\n\\nThe initial distance between them is 1 mile. The time it takes to close this distance at a relative speed of 5 mph is calculated by dividing the distance by the relative speed:\\n\\n\\\\[\\n\\\\text{Time} = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = 0.2 \\\\text{ hours}\\n\\\\]\\n\\nConverting this time from hours to minutes:\\n\\n\\\\[\\n0.2 \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes}\\n\\\\]\\n\\nThus, the lion will catch the elephant in \\\\boxed{12} minutes.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\"\n",
    "# prompt = \"how many wings has a bird?\"\n",
    "prompt = correct_dataset[:5][0][\"problem\"]\n",
    "\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": base_prompt.format(question=prompt)},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # generated_ids = model.generate(\n",
    "    #     model_inputs.input_ids,\n",
    "    #     max_new_tokens=1,\n",
    "    #     do_sample=False,\n",
    "    # )\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=4096,\n",
    "        do_sample=False,\n",
    "        # do_sample=not False,\n",
    "        # temperature=0.6,\n",
    "        # top_p=0.95,\n",
    "    )\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\"\n",
    "prompt = correct_dataset[:5][0][\"problem\"]\n",
    "\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": base_prompt.format(question=prompt)},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "# initial_len = generated_tokens.shape\n",
    "with torch.no_grad():\n",
    "    generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "    generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "    max_steps = 1200\n",
    "    past_key_values = None\n",
    "    for step in range(max_steps):\n",
    "        if step == 0:\n",
    "            logits = model(\n",
    "                inputs_embeds=generated_embeds,\n",
    "                attention_mask=torch.ones(1, generated_embeds.shape[1]).long().cuda(),\n",
    "                position_ids=torch.arange(generated_embeds.shape[1])\n",
    "                .cuda()\n",
    "                .unsqueeze(0),\n",
    "                use_cache=True,\n",
    "                past_key_values=None,\n",
    "            )\n",
    "            past_key_values = logits.past_key_values\n",
    "            logits = logits.logits[:, -1, :].clone().float()\n",
    "        else:\n",
    "            logits = model(\n",
    "                # input_ids=generated_tokens[-1][-1:].unsqueeze(0),\n",
    "                inputs_embeds=generated_embeds[:, -1:, :],\n",
    "                attention_mask=torch.ones(1, generated_embeds.shape[1]).long().cuda(),\n",
    "                position_ids=torch.tensor(generated_embeds.shape[1] - 1)\n",
    "                .reshape(1, 1)\n",
    "                .cuda(),\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "            )\n",
    "            past_key_values = logits.past_key_values\n",
    "\n",
    "            logits = logits.logits[:, -1, :].clone().float()\n",
    "\n",
    "        top_token = logits.argmax(-1)[-1]\n",
    "        top_token_embed = model.get_input_embeddings()(top_token)\n",
    "        # print(top)\n",
    "        generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "        generated_embeds = torch.cat(\n",
    "            [generated_embeds, top_token_embed.reshape(1, 1, -1)],\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(step, tokenizer.decode(generated_tokens[-1]))\n",
    "    # break\n",
    "print(tokenizer.decode(generated_tokens[-1]))\n",
    "# break\n",
    "embeds_generation_tokens = generated_tokens[-1]\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>Problem: An elephant and a lion are currently 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour.  How many minutes will it take for the lion to catch the elephant?\\n\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<ÔΩúAssistantÔΩú><think>\\nOkay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards it at 24 mph. So, their relative speed is the difference between their speeds because they're moving towards each other. Wait, no, actually, since the elephant is moving away, the lion has to cover the distance that the elephant is moving away plus the distance the elephant covers while the lion is moving towards it.\\n\\nLet me clarify. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the lion is closing the gap at a rate of 24 mph minus 19 mph, which is 5 mph. That makes sense because if two objects are moving towards each other, their relative speed is the sum of their speeds, but in this case, one is moving away and the other is moving towards, so it's the difference.\\n\\nSo, the initial distance between them is 1 mile. The lion is closing the gap at 5 mph. To find the time it takes to catch up, I can use the formula:\\n\\nTime = Distance / Speed\\n\\nSo, plugging in the numbers, the time should be 1 mile divided by 5 mph. That gives me 0.2 hours. But the question asks for the time in minutes, so I need to convert 0.2 hours to minutes. Since 1 hour is 60 minutes, 0.2 hours is 0.2 * 60 = 12 minutes.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, the relative speed is 24 - 19 = 5 mph. So, yes, the lion is gaining on the elephant at 5 mph. So, 1 mile divided by 5 mph is indeed 0.2 hours, which is 12 minutes. That seems right.\\n\\nBut just to make sure I didn't make a mistake, let me think about it another way. Maybe set up an equation for their positions as functions of time and see if I get the same result.\\n\\nLet's denote t as the time in hours it takes for the lion to catch the elephant. In that time, the elephant will have moved a distance of 19t miles away from the starting point, and the lion will have moved 24t miles towards the elephant. Since they start 1 mile apart, the distance between them when the lion catches the elephant will be zero.\\n\\nSo, the distance the lion covers plus the distance the elephant covers should equal the initial distance between them. Wait, no, actually, the lion is moving towards the elephant, so the distance the lion covers is 24t, and the distance the elephant covers is 19t. But since the elephant is moving away, the total distance between them when the lion catches up is 24t - 19t = 5t. This should equal the initial distance, which is 1 mile.\\n\\nSo, 5t = 1 mile. Solving for t, we get t = 1/5 hours, which is 0.2 hours. Converting that to minutes, 0.2 * 60 = 12 minutes. Yep, same result. So, that seems consistent.\\n\\nAlternatively, maybe I can think about it in terms of how much distance the lion needs to cover relative to the elephant. Since the lion is moving faster, it's gaining on the elephant at 5 mph. So, the lion needs to cover the 1 mile gap at a relative speed of 5 mph. So, time = distance / speed = 1 / 5 hours, which is 12 minutes. Yep, same answer.\\n\\nI think that's solid. So, the lion will catch the elephant in 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. \\n\\nTo find the time it takes for the lion to catch the elephant, we first determine their relative speed. Since the lion is moving towards the elephant and the elephant is moving away, their relative speed is the difference between their speeds:\\n\\n\\\\[\\n24 \\\\text{ mph} - 19 \\\\text{ mph} = 5 \\\\text{ mph}\\n\\\\]\\n\\nThe initial distance between them is 1 mile. Using the formula for time, which is distance divided by speed, we get:\\n\\n\\\\[\\n\\\\text{Time} = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = 0.2 \\\\text{ hours}\\n\\\\]\\n\\nConverting 0.2 hours to minutes:\\n\\n\\\\[\\n0.2 \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes}\\n\\\\]\\n\\nThus, the lion will catch the elephant in \\\\boxed{12} minutes.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>\\n\\nTo determine how long it will take for the lion to catch the elephant, we need to consider their relative speed. The elephant is running away at \""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "from transformers.generation import utils\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\"\n",
    "dataset_pos = 10\n",
    "prompt = correct_dataset[dataset_pos][\"problem\"]\n",
    "\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": base_prompt.format(question=prompt),\n",
    "        },\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    inputs_embeds=generated_embeds,\n",
    "    max_new_tokens=1800,\n",
    "    # max_new_tokens=5,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': \"Alicia's average on her five tests is 88 points. The score range for each test is 0 points to 100 points, inclusive. What is the lowest possible score that Alicia could have earned on one of the five tests?\",\n",
       " 'solution': \"If Alicia's average score on her five tests is 88 points, then the sum of her scores must be $88 \\\\times 5 = 440$ points. If she earned   100 points on four of the tests, then she could have earned a score as low as $\\\\boxed{40\\\\text{ points}}$ on the other test.\",\n",
       " 'answer': '40\\\\text{ points}',\n",
       " 'subject': 'Prealgebra',\n",
       " 'level': 4,\n",
       " 'unique_id': 'test/prealgebra/1697.json',\n",
       " 'model_answer': \"Okay, so I need to figure out the lowest possible score Alicia could have earned on one of her five tests. She has an average of 88 points, and each test is scored between 0 and 100, inclusive. Hmm, let me think about this step by step.\\n\\nFirst, I know that an average is calculated by taking the sum of all the test scores and dividing it by the number of tests. In this case, she has five tests, and her average is 88. So, to find the total points she has across all five tests, I can multiply the average by the number of tests.\\n\\nLet me write that down:\\n\\nTotal points = Average √ó Number of tests\\nTotal points = 88 √ó 5\\n\\nLet me calculate that. 88 times 5 is... 440. So, the total points she has across all five tests is 440.\\n\\nNow, the question is asking for the lowest possible score she could have earned on one of these tests. To minimize one of the test scores, I need to maximize the other scores because the total is fixed at 440.\\n\\nBut there are constraints: each test can only have a score between 0 and 100, inclusive. So, the maximum score on any test is 100. If I try to maximize the other four test scores, each being 100, that would give me the maximum possible sum for four tests.\\n\\nLet me calculate that:\\n\\nMaximizing four tests:\\nMax sum = 100 √ó 4 = 400\\n\\nSo, if four tests are 100 each, that's 400 points. Then, the fifth test must make up the remaining points to reach the total of 440.\\n\\nLet me find the score for the fifth test:\\n\\nScore on fifth test = Total points - Max sum of four tests\\nScore = 440 - 400\\nScore = 40\\n\\nWait, so that means if four tests are 100 each, the fifth test must be 40? That seems low, but it's mathematically correct because 100 + 100 + 100 + 100 + 40 = 440.\\n\\nBut hold on, the question is asking for the lowest possible score on one of the tests. So, if I set the other four tests as high as possible (100 each), the fifth test would be 40. But is there a way to get a lower score? Let me think.\\n\\nIf I make one of the other tests lower than 100, does that allow the fifth test to be lower than 40? Wait, no, because if I lower one test, I have to raise the others to compensate, but they can't go above 100. So, actually, to minimize one test, you need to maximize the others as much as possible, which is 100 each. So, 40 is indeed the minimal possible score.\\n\\nBut just to double-check, let's think about it differently. Suppose I try to set four tests to 100, which gives me 400. The fifth test is 40. If I tried to make the fifth test lower, say 30, then the sum of the other four tests would have to be 440 - 30 = 410. But if I have four tests, each can be at most 100, so 100 √ó 4 is 400. 410 is more than 400, which isn't possible. So, it's not possible to have a lower score on the fifth test than 40 because the other four tests can't exceed 400.\\n\\nTherefore, 40 is the lowest possible score Alicia could have earned on one of the five tests.\\n\\nWait, but another thought: what if one test is 0? Would that allow the others to be higher? Let me see. If one test is 0, then the total of the other four tests would have to be 440 - 0 = 440. But each of those four tests can only go up to 100, so 100 √ó 4 is 400. But 440 is more than 400, which is impossible. Therefore, having a 0 on one test would require the others to sum to 440, which isn't possible since the maximum they can sum to is 400. So, 0 is too low.\\n\\nSo, 40 is indeed the lowest possible score because it's the only score that allows the other four tests to be 100 each without exceeding the maximum.\\n\\nAnother angle: Let's think about the formula for the average. The average is the total divided by the number of tests. So, if I want to minimize one test, I have to maximize the others, but they can't exceed 100. So, the maximum sum of four tests is 400, as we had before. Therefore, the minimal fifth test is 40.\\n\\nWait, let me make sure I'm not missing any other constraints. The problem says each test is between 0 and 100, inclusive. So, 0 is allowed, but in this case, if we tried to set four tests to 100, the fifth must be 40. If we set one test to 100, the others could be higher, but that doesn't help in minimizing a single test. Alternatively, if I set two tests to 100, the others can be higher, but again, that doesn't help.\\n\\nTherefore, the only way to minimize a single test score is to maximize the other four, which is 100 each. So, the minimal possible score is 40.\\n\\nI think that's solid reasoning. I don't see any other way to lower one test score without violating the test score range or the total points.\\n\\n**Final Answer**\\nThe lowest possible score Alicia could have earned on one of the five tests is \\\\boxed{40}.\\n</think>\\n\\nAlicia's average score on her five tests is 88 points. To find the total points she has across all five tests, we multiply the average by the number of tests:\\n\\n\\\\[\\n\\\\text{Total points} = 88 \\\\times 5 = 440\\n\\\\]\\n\\nTo find the lowest possible score on one of the tests, we need to maximize the scores of the other four tests. Since each test can score a maximum of 100, the maximum sum of four tests is:\\n\\n\\\\[\\n\\\\text{Max sum of four tests} = 100 \\\\times 4 = 400\\n\\\\]\\n\\nThe score on the fifth test must then be:\\n\\n\\\\[\\n\\\\text{Score on fifth test} = 440 - 400 = 40\\n\\\\]\\n\\nThus, the lowest possible score Alicia could have earned on one of the five tests is \\\\boxed{40}.\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_dataset[dataset_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "example_id = correct_dataset[dataset_pos][\"unique_id\"].replace(\"/\", \"_\")\n",
    "with open(f\"./temp/{example_id}\", \"w\") as f:\n",
    "    temp = {\n",
    "        \"generated_ids\": generated_ids.tolist(),\n",
    "        \"input_ids\": generated_tokens.tolist(),\n",
    "    }\n",
    "    json.dump(temp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\n",
    "    correct_dataset[dataset_pos][\"model_answer\"],\n",
    "    add_special_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_ids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>Problem: An elephant and a lion are currently 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour.  How many minutes will it take for the lion to catch the elephant?\\n\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<ÔΩúAssistantÔΩú><think>\\nOkay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards it at 24 mph. So, their relative speed is the difference between their speeds because they're moving towards each other. Wait, no, actually, since the elephant is moving away, the lion has to cover the distance that the elephant is moving away plus the distance the elephant covers while the lion is moving towards it.\\n\\nLet me clarify. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the lion is closing the gap at a rate of 24 mph minus 19 mph, which is 5 mph. That makes sense because if two objects are moving towards each other, their relative speed is the sum of their speeds, but in this case, one is moving away and the other is moving towards, so it's the difference.\\n\\nSo, the initial distance between them is 1 mile. The lion is closing the gap at 5 mph. To find the time it takes to catch up, I can use the formula:\\n\\nTime = Distance / Speed\\n\\nSo, plugging in the numbers, the time should be 1 mile divided by 5 mph. That gives me 0.2 hours. But the question asks for the time in minutes, so I need to convert 0.2 hours to minutes. Since 1 hour is 60 minutes, 0.2 hours is 0.2 * 60 = 12 minutes.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, the relative speed is 24 - 19 = 5 mph. So, yes, the lion is gaining on the elephant at 5 mph. So, 1 mile divided by 5 mph is indeed 0.2 hours, which is 12 minutes. That seems right.\\n\\nBut just to make sure I didn't make a mistake, let me think about it another way. Maybe set up an equation for their positions as functions of time and see if I get the same result.\\n\\nLet's denote t as the time in hours it takes for the lion to catch the elephant. In that time, the elephant will have moved a distance of 19t miles away from the starting point, and the lion will have moved 24t miles towards the elephant. Since they start 1 mile apart, the distance between them when the lion catches the elephant will be zero.\\n\\nSo, the distance the lion covers plus the distance the elephant covers should equal the initial distance between them. Wait, no, actually, the lion is moving towards the elephant, so the distance the lion covers is 24t, and the distance the elephant covers is 19t. But since the elephant is moving away, the total distance between them when the lion catches up is 24t - 19t = 5t. This should equal the initial distance, which is 1 mile.\\n\\nSo, 5t = 1 mile. Solving for t, we get t = 1/5 hours, which is 0.2 hours. Converting that to minutes, 0.2 * 60 = 12 minutes. Yep, same result. So, that seems consistent.\\n\\nAlternatively, maybe I can think about it in terms of how much distance the lion needs to cover relative to the elephant. Since the lion is moving faster, it's gaining on the elephant at 5 mph. So, the lion needs to cover the 1 mile gap at a relative speed of 5 mph. So, time = distance / speed = 1 / 5 hours, which is 12 minutes. Yep, same answer.\\n\\nI think that's solid. So, the lion will catch the elephant in 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. \\n\\nTo find the time it takes for the lion to catch the elephant, we first determine their relative speed. Since the lion is moving towards the elephant and the elephant is moving away, their relative speed is the difference between their speeds:\\n\\n\\\\[\\n24 \\\\text{ mph} - 19 \\\\text{ mph} = 5 \\\\text{ mph}\\n\\\\]\\n\\nThe initial distance between them is 1 mile. Using the formula for time, which is distance divided by speed, we get:\\n\\n\\\\[\\n\\\\text{Time} = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = 0.2 \\\\text{ hours}\\n\\\\]\\n\\nConverting 0.2 hours to minutes:\\n\\n\\\\[\\n0.2 \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes}\\n\\\\]\\n\\nThus, the lion will catch the elephant in \\\\boxed{12} minutes.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>\\n\\nTo determine how long it will take for the lion to catch the elephant, we need to consider their relative speed. The elephant is running away at\n",
    "# Okay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards it at 24 mph. So, their relative speed is the difference between their speeds because they're moving towards each other. Wait, no, actually, since the elephant is moving away, the lion has to cover the distance that the elephant is moving away plus the distance the elephant covers while the lion is moving towards it.\\n\\nLet me clarify. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the lion is closing the gap at a rate of 24 mph minus 19 mph, which is 5 mph. That makes sense because if two objects are moving towards each other, their relative speed is the sum of their speeds, but in this case, one is moving away and the other is moving towards, so it's the difference.\\n\\nSo, the initial distance between them is 1 mile. The lion is closing the gap at 5 mph. To find the time it takes to catch up, I can use the formula:\\n\\nTime = Distance / Speed\\n\\nSo, plugging in the numbers, the time should be 1 mile divided by 5 mph. That gives me 0.2 hours. But the question asks for the time in minutes, so I need to convert 0.2 hours to minutes. Since 1 hour is 60 minutes, 0.2 hours is 0.2 * 60 = 12 minutes.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, the relative speed is 24 - 19 = 5 mph. So, yes, the lion is gaining on the elephant at 5 mph. So, 1 mile divided by 5 mph is indeed 0.2 hours, which is 12 minutes. That seems right.\\n\\nBut just to make sure I didn't make a mistake, let me think about it another way. Maybe set up an equation for their positions as functions of time and see if I get the same result.\\n\\nLet's denote t as the time in hours it takes for the lion to catch the elephant. In that time, the elephant will have moved a distance of 19t miles away from the starting point, and the lion will have moved 24t miles towards the elephant. Since they start 1 mile apart, the distance between them when the lion catches the elephant will be zero.\\n\\nSo, the distance the lion covers plus the distance the elephant covers should equal the initial distance between them. Wait, no, actually, the lion is moving towards the elephant, so the distance the lion covers is 24t, and the distance the elephant covers is 19t. But since the elephant is moving away, the total distance between them when the lion catches up is 24t - 19t = 5t. This should equal the initial distance, which is 1 mile.\\n\\nSo, 5t = 1 mile. Solving for t, we get t = 1/5 hours, which is 0.2 hours. Converting that to minutes, 0.2 * 60 = 12 minutes. Yep, same result. So, that seems consistent.\\n\\nAlternatively, maybe I can think about it in terms of how much distance the lion needs to cover relative to the elephant. Since the lion is moving faster, it's gaining on the elephant at 5 mph. So, the lion needs to cover the 1 mile gap at a relative speed of 5 mph. So, time = distance / speed = 1 / 5 hours, which is 12 minutes. Yep, same answer.\\n\\nI think that's solid. So, the lion will catch the elephant in 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. \\n\\nTo find the time it takes for the lion to catch the elephant, we first determine their relative speed. Since the lion is moving towards the elephant and the elephant is moving away, their relative speed is the difference between their speeds:\\n\\n\\\\[\\n24 \\\\text{ mph} - 19 \\\\text{ mph} = 5 \\\\text{ mph}\\n\\\\]\\n\\nThe initial distance between them is 1 mile. Using the formula for time, which is distance divided by speed, we get:\\n\\n\\\\[\\n\\\\text{Time} = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = 0.2 \\\\text{ hours}\\n\\\\]\\n\\nConverting 0.2 hours to minutes:\\n\\n\\\\[\\n0.2 \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes}\\n\\\\]\\n\\nThus, the lion will catch the elephant in \\\\boxed{12} minutes.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(generated_ids[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(generated_tokens[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>An elephant and a lion are currently 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour.  How many minutes will it take for the lion to catch the elephant?<ÔΩúAssistantÔΩú><think>\\nOkay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards the elephant at 24 mph. So, their speeds are different, which means the distance between them will be changing over time. I need to find the time it takes for the lion to close that 1 mile gap.\\n\\nI remember that when two objects are moving towards each other, their relative speed is the sum of their individual speeds. But in this case, the elephant is moving away, so it's like the lion is chasing the elephant, but the elephant is moving away. So, maybe I should think of the relative speed as the lion's speed minus the elephant's speed? Wait, no, that might not be right.\\n\\nLet me clarify. If the lion is moving towards the elephant at 24 mph, and the elephant is moving away from the lion at 19 mph, then the lion is effectively closing the gap at a rate of 24 mph minus 19 mph. That makes sense because the lion is moving towards the elephant, but the elephant is moving away. So, the net speed at which the distance between them is decreasing is 24 - 19 = 5 mph.\\n\\nSo, the distance between them is decreasing at 5 miles per hour. They start 1 mile apart, so I can use the formula:\\n\\nTime = Distance / Speed\\n\\nHere, the distance is 1 mile, and the speed is 5 mph. So, time = 1 / 5 hours. But the question asks for the time in minutes, so I need to convert hours to minutes. There are 60 minutes in an hour, so 1/5 hours * 60 minutes/hour = 12 minutes.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph towards the elephant, and the elephant is moving away at 19 mph, then the relative speed is indeed 24 - 19 = 5 mph. So, the lion is gaining on the elephant at 5 miles per hour. Since they start 1 mile apart, the time it takes to close that distance is 1 / 5 hours, which is 12 minutes. That seems right.\\n\\nBut just to make sure, let me think about it another way. Maybe set up an equation for their positions as functions of time and see when they meet.\\n\\nLet me denote t as the time in hours. The position of the elephant as a function of time would be starting from some point and moving away at 19 mph. Let's say the initial position of the elephant is at position 0, and the lion is at position 1 mile. Then, the position of the elephant at time t is 0 + 19t, and the position of the lion is 1 - 24t. They meet when their positions are equal, so:\\n\\n19t = 1 - 24t\\n\\nAdding 24t to both sides:\\n\\n19t + 24t = 1\\n\\n43t = 1\\n\\nt = 1/43 hours\\n\\nWait, that's different from what I got before. Hmm, so which one is correct? 12 minutes or 1/43 hours?\\n\\nWait, 1/43 hours is approximately 1.395 minutes, which is about 1 minute and 23 seconds. But earlier, I got 12 minutes. That's a big difference. So, I must have made a mistake somewhere.\\n\\nLet me go back. When I thought about the relative speed, I considered the lion moving towards the elephant and the elephant moving away, so the relative speed is 24 - 19 = 5 mph. So, the time should be 1/5 hours, which is 12 minutes. But when I set up the equations, I got t = 1/43 hours, which is about 1.395 minutes. That's conflicting.\\n\\nWait, maybe I messed up the initial positions. Let me define the positions more carefully. Let's say the lion starts at position 0, and the elephant starts at position 1 mile. Then, the lion is moving towards the elephant, so its position as a function of time is 0 + 24t. The elephant is moving away from the lion, so its position is 1 + 19t. They meet when their positions are equal:\\n\\n24t = 1 + 19t\\n\\nSubtracting 19t from both sides:\\n\\n5t = 1\\n\\nt = 1/5 hours, which is 12 minutes. Okay, that makes sense. So, why did I get 1/43 hours earlier?\\n\\nWait, in my second approach, I set the lion's position as 1 - 24t, but that might be incorrect. If the lion starts at position 0, moving towards the elephant, which is at position 1, then the lion's position is 24t. The elephant is moving away from the lion, so its position is 1 + 19t. So, setting them equal:\\n\\n24t = 1 + 19t\\n\\nWhich gives t = 1/5 hours, which is 12 minutes. So, that seems correct.\\n\\nWait, so why did I get 1/43 hours earlier? Maybe I confused the starting positions. Let me re-examine that.\\n\\nIn the second approach, I set the lion's position as 1 - 24t, which would be incorrect because if the lion is moving towards the elephant, starting from position 0, its position should be increasing, not decreasing. So, that was my mistake. I should have set the lion's position as 24t, not 1 - 24t.\\n\\nSo, correcting that, the correct equation is 24t = 1 + 19t, leading to t = 1/5 hours, which is 12 minutes. So, that's consistent with the first method.\\n\\nTherefore, the correct answer is 12 minutes.\\n\\nWait, but just to make sure, let me think about it in another way. Suppose the lion is moving at 24 mph towards the elephant, and the elephant is\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards the elephant at 24 mph. So, their speeds are different, which means the distance between them is changing over time. I need to find the time it takes for the lion to close that 1 mile gap.\\n\\nI remember that when two objects are moving towards each other, their relative speed is the sum of their individual speeds. But in this case, the elephant is moving away, so it's like the lion is chasing the elephant, but the elephant is moving away. So, maybe I should consider the relative speed between the lion and the elephant.\\n\\nWait, actually, since the elephant is moving away, the lion has to cover the initial distance plus any additional distance the elephant might cover while the lion is chasing. But since the lion is faster, it should eventually catch up. So, maybe I can model this as a relative speed problem.\\n\\nLet me denote the speed of the lion as \\\\( v_l = 24 \\\\) mph and the speed of the elephant as \\\\( v_e = 19 \\\\) mph. The initial distance between them is \\\\( d = 1 \\\\) mile.\\n\\nSince the lion is moving towards the elephant and the elephant is moving away, the relative speed at which the distance between them is decreasing is the difference between the lion's speed and the elephant's speed. So, the relative speed \\\\( v_{relative} = v_l - v_e = 24 - 19 = 5 \\\\) mph.\\n\\nWait, is that right? If the lion is moving towards the elephant and the elephant is moving away, then the lion is effectively closing the gap at a rate of 5 mph. So, the time it takes to close the 1 mile gap would be the initial distance divided by the relative speed.\\n\\nSo, time \\\\( t = \\\\frac{d}{v_{relative}} = \\\\frac{1}{5} \\\\) hours. But the question asks for the time in minutes, so I need to convert that.\\n\\nSince 1 hour is 60 minutes, \\\\( \\\\frac{1}{5} \\\\) hours is \\\\( \\\\frac{1}{5} \\\\times 60 = 12 \\\\) minutes. So, it should take 12 minutes for the lion to catch the elephant.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, then the lion is gaining on the elephant at a rate of 5 mph. So, to cover the 1 mile gap, it would take \\\\( \\\\frac{1}{5} \\\\) hours, which is indeed 12 minutes. That seems correct.\\n\\nAlternatively, I can model this with equations. Let me set up a coordinate system where at time \\\\( t \\\\) hours, the position of the lion is \\\\( 24t \\\\) miles from its starting point, and the position of the elephant is \\\\( 1 + 19t \\\\) miles from the starting point of the lion. Wait, actually, hold on. If the elephant is moving away from the lion, then the distance between them is increasing. So, the position of the elephant is \\\\( 1 + 19t \\\\) miles from the starting point of the lion, and the position of the lion is \\\\( 24t \\\\) miles from the same starting point.\\n\\nWait, but initially, they are 1 mile apart. So, if the lion starts at position 0 and the elephant starts at position 1 mile, then the distance between them at time \\\\( t \\\\) is \\\\( |24t - (1 + 19t)| \\\\). We want this distance to be 0 when the lion catches the elephant.\\n\\nSo, setting up the equation:\\n\\n\\\\( 24t - (1 + 19t) = 0 \\\\)\\n\\nSimplify:\\n\\n\\\\( 24t - 19t - 1 = 0 \\\\)\\n\\n\\\\( 5t - 1 = 0 \\\\)\\n\\n\\\\( 5t = 1 \\\\)\\n\\n\\\\( t = \\\\frac{1}{5} \\\\) hours, which is 12 minutes. So, that confirms my earlier result.\\n\\nAlternatively, I can think about it in terms of distance covered. The lion needs to cover the initial 1 mile plus whatever distance the elephant covers in that time. But since the lion is faster, the time is determined by the relative speed.\\n\\nWait, another way to think about it is using the concept of relative velocity. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the relative speed is 24 - 19 = 5 mph. So, the lion is closing the gap at 5 mph. Therefore, the time to close 1 mile is 1/5 hours, which is 12 minutes.\\n\\nI think that's solid. I can't see any mistakes in this reasoning. So, I think the answer is 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. To determine how long it will take for the lion to catch the elephant, we need to consider their relative speed.\\n\\nThe relative speed at which the lion is closing the gap is the difference between their speeds:\\n\\\\[ v_{\\\\text{relative}} = v_l - v_e = 24 \\\\text{ mph} - 19 \\\\text{ mph} = 5 \\\\text{ mph} \\\\]\\n\\nThe time it takes to close the 1 mile gap is calculated by dividing the initial distance by the relative speed:\\n\\\\[ t = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = \\\\frac{1}{5} \\\\text{ hours} \\\\]\\n\\nConverting this time into minutes:\\n\\\\[ \\\\frac{1}{5} \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes} \\\\]\\n\\nThus, the lion will catch the elephant in \\\\boxed{12} minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suppose that $f$ is a function and $f^{-1}$ is the inverse of $f$.  If $f(1)=2$, $f(2) = 6$, and $f(3)=5$, then what is $f^{-1}(f^{-1}(6))$?\n",
      "===\n",
      "===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPRESSED PART Okay, so I have this problem here: There's a function f, and its inverse function is f inverse. I know that f(1) = 2, f(2) = 6, and f(3) = 5. I need to find f inverse of f inverse of 6, which is written as f^{-1}(f^{-1}(6)). Hmm, okay. Let me think about how to approach this.\n",
      "\n",
      "First, let me recall what an inverse function does. If f is a function that maps an input to an output, then the inverse function f^{-1} maps the output back to the input. So, basically, if f(a) = b, then f^{-1}(b) = a. That makes sense. So, if I have f(1)=2, that means f^{-1}(2)=1. Similarly, f(2)=6 implies f^{-1}(6)=2, and f(3)=5 means\n",
      "===\n",
      "„ÄÇ)\n",
      "\n",
      "È¶ñÂÖàÔºåÊàëÈúÄË¶ÅÊòéÁ°ÆÈ¢òÁõÆ‰∏≠ÁöÑ‰ø°ÊÅØ„ÄÇÈ¢òÁõÆÁªôÂá∫‰∫ÜÂáΩÊï∞fÁöÑ‰∏â‰∏™ÁÇπÔºöf(1)=2Ôºåf(2)=6Ôºåf(3)=5„ÄÇÁÑ∂ÂêéË¶ÅÊ±ÇËÆ°ÁÆóf^{-1}(f^{-1}(6))„ÄÇ\n",
      "\n",
      "È¶ñÂÖàÔºåÊàëÈúÄË¶ÅÁêÜËß£f^{-1}ÊòØ‰ªÄ‰πà„ÄÇf^{-1}ÊòØfÁöÑÂèçÂáΩÊï∞Ôºå‰πüÂ∞±ÊòØËØ¥ÔºåÂ¶ÇÊûúf(a)=bÔºåÈÇ£‰πàf^{-1}(b)=a„ÄÇÂõ†Ê≠§ÔºåÊàëÈúÄË¶ÅÂÖàÊâæÂà∞f^{-1}ÁöÑÂÄºÔºåÁÑ∂ÂêéÂ∫îÁî®‰∏§Ê¨°f^{-1}„ÄÇ\n",
      "\n",
      "Êé•‰∏ãÊù•ÔºåÊàëÊù•ÈÄêÊ≠•ËÆ°ÁÆó„ÄÇÈ¶ñÂÖàÔºåËÆ°ÁÆóf^{-1}(6)„ÄÇÊ†πÊçÆfÁöÑÂÆö‰πâÔºåf(2)=6ÔºåÊâÄ‰ª•f^{-1}(6)=2„ÄÇÊé•‰∏ãÊù•ÔºåÊàëÈúÄË¶ÅËÆ°ÁÆóf^{-1}(2)„ÄÇËøôÈáåÔºåÊàëÈúÄË¶ÅÊâæÂà∞‰∏Ä‰∏™xÔºå‰ΩøÂæóf(x)=2„ÄÇÊ†πÊçÆfÁöÑÂÆö‰πâÔºåf(1)=2ÔºåÊâÄ‰ª•f^{-1}(2)=1„ÄÇÂõ†Ê≠§Ôºåf^{-1}(f^{-1}(6))=f^{-1}(2)=1„ÄÇ\n",
      "\n",
      "‰∏çËøáÔºå‰∏∫‰∫ÜÁ°Æ‰øùÊàëÁöÑËÆ°ÁÆóÊòØÊ≠£Á°ÆÁöÑÔºåÊàëÂÜç‰ªîÁªÜÊ£ÄÊü•‰∏ÄÈÅç„ÄÇÈ¶ñÂÖàÔºåf(1)=2ÔºåÊâÄ‰ª•f^{-1}(2)=1„ÄÇÁÑ∂ÂêéÔºåf(2)=6ÔºåÊâÄ‰ª•f^{-1}(6)=2„ÄÇÊé•ÁùÄÔºåf^{-1}(2)=1ÔºåÊâÄ‰ª•f^{-1}(f^{-1}(6))=1„ÄÇÁúãËµ∑Êù•Ê≤°ÊúâÈóÆÈ¢ò„ÄÇ\n",
      "\n",
      "ÊàñËÄÖÔºåÊàëÂèØ‰ª•Êç¢‰∏ÄÁßçÊñπÂºèÊÄùËÄÉÔºåÁõ¥Êé•Â∫îÁî®f^{-1}‰∏§Ê¨°„ÄÇÈ¶ñÂÖàÔºåf^{-1}(6)=2ÔºåÁÑ∂Âêéf^{-1}(2)=1ÔºåÊâÄ‰ª•ÁªìÊûúÊòØ1„ÄÇËøôÂíå‰πãÂâçÁöÑÁªìÊûú‰∏ÄËá¥„ÄÇ\n",
      "\n",
      "ÂÜçËøõ‰∏ÄÊ≠•ÔºåÊàëÂèØ‰ª•ÂàóÂá∫fÁöÑÂÄºÂíåf^{-1}ÁöÑÂÄºÔºå‰ª•‰æøÊõ¥Ê∏ÖÊô∞Âú∞ÁêÜËß£„ÄÇf(1)=2ÔºåÊâÄ‰ª•f^{-1}(2)=1Ôºõf(2)=6ÔºåÊâÄ‰ª•f^{-1}(6)=2Ôºõf(3)=5ÔºåÊâÄ‰ª•f^{-1}(5)=3„ÄÇËøôÊ†∑Ôºåf^{-1}ÁöÑÂÄºÂüüÊòØ{1,2,3}ÔºåËÄåfÁöÑÂÆö‰πâÂüüÊòØ{1,2,3}„ÄÇ\n",
      "\n",
      "Âõ†Ê≠§ÔºåËÆ°ÁÆóf^{-1}(6)ÁöÑÊó∂ÂÄôÔºåÁõ¥Êé•ÊâæÂà∞f(x)=6ÁöÑxÂÄºÔºåÂç≥x=2ÔºåÊâÄ‰ª•f^{-1}(6)=2„ÄÇÁÑ∂ÂêéÔºåËÆ°ÁÆóf^{-1}(2)ÔºåÊâæÂà∞f(x)=2ÁöÑxÂÄºÔºåÂç≥x=1ÔºåÊâÄ‰ª•f^{-1}(2)=1„ÄÇÂõ†Ê≠§Ôºåf^{-1}(f^{-1}(6))=1„ÄÇ\n",
      "\n",
      "Áªº‰∏äÊâÄËø∞ÔºåÁ≠îÊ°àÂ∫îËØ•ÊòØ1„ÄÇ\n",
      "\n",
      "**Á≠îÊ°à**\n",
      "\\boxed{1}\n",
      "</think>\n",
      "\n",
      "È¶ñÂÖàÔºåÈ¢òÁõÆÁªôÂá∫‰∫ÜÂáΩÊï∞ \\( f \\) ÁöÑ‰∏â‰∏™ÁÇπÔºö\\( f(1) = 2 \\)Ôºå\\( f(2) = 6 \\)ÔºåÂíå \\( f(3) = 5 \\)„ÄÇÊàë‰ª¨ÈúÄË¶ÅËÆ°ÁÆó \\( f^{-1}(f^{-1}(6)) \\)„ÄÇ\n",
      "\n",
      "1. ËÆ°ÁÆó \\( f^{-1}(6) \\)Ôºö\n",
      "   - Ê†πÊçÆ \\( f(2) = 6 \\)ÔºåÊâÄ‰ª• \\( f^{-1}(6) = 2 \\)„ÄÇ\n",
      "\n",
      "2. ËÆ°ÁÆó \\( f^{-1}(2) \\)Ôºö\n",
      "   - Ê†πÊçÆ \\( f(1) = 2 \\)ÔºåÊâÄ‰ª• \\( f^{-1}(2) = 1 \\)„ÄÇ\n",
      "\n",
      "Âõ†Ê≠§Ôºå\\( f^{-1}(f^{-1}(6)) = f^{-1}(2) = 1 \\)„ÄÇ\n",
      "\n",
      "ÊúÄÁªàÁ≠îÊ°àÊòØÔºö\n",
      "\\[\n",
      "\\boxed{1}\n",
      "\\]<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    WINDOW_SIZE,\n",
    "    VISION_START,\n",
    "    VISION_END,\n",
    "    EOS_TOKEN_ID,\n",
    ")\n",
    "import torch\n",
    "import json\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "dataset_pos = 11\n",
    "input_ids = correct_dataset[dataset_pos][\"problem\"]\n",
    "print(input_ids)\n",
    "print(\"===\")\n",
    "print(\"===\")\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "input_ids = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": base_prompt.format(question=input_ids),\n",
    "            },\n",
    "        ],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "# input_ids = dataset_item[\"input_ids\"]\n",
    "# generated_ids = dataset_item[\"generated_ids\"]\n",
    "generated_ids = [\n",
    "    tokenizer.encode(\n",
    "        correct_dataset[dataset_pos][\"model_answer\"],\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\"\n",
    "\n",
    "# generated_tokens = tokenizer.apply_chat_template(\n",
    "#     [\n",
    "#         # {\"role\": \"user\", \"content\": \"how many wings has a bird?\"},\n",
    "#         {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "#     ],\n",
    "#     tokenize=True,\n",
    "#     add_generation_prompt=True,\n",
    "# )\n",
    "generated_tokens = input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "        # start_embed = model.base_model.embed_pooler.model.embed_tokens.modules_to_save.default(\n",
    "        torch.tensor([[VISION_START]], device=\"cuda\")\n",
    "    )\n",
    "    end_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "        # end_embed = model.base_model.embed_pooler.model.embed_tokens.modules_to_save.default(\n",
    "        torch.tensor([[VISION_END]], device=\"cuda\")\n",
    "    )\n",
    "    input_ids = torch.tensor(input_ids).cuda()\n",
    "    input_ids_embeds = model.get_input_embeddings()(input_ids)\n",
    "    windows_amount = 100\n",
    "    # windows_amount = 200\n",
    "    # windows_amount = 300\n",
    "    # windows_amount = 400\n",
    "    # windows_amount = 500\n",
    "    # windows_amount = 2\n",
    "    next_true_tokens = torch.tensor(generated_ids, device=\"cuda\")[\n",
    "        :, : WINDOW_SIZE * windows_amount\n",
    "    ]\n",
    "\n",
    "    # next_true_tokens = torch.tensor(next_true_tokens, device=\"cuda\")\n",
    "\n",
    "    original_embeds = (\n",
    "        # model.base_model.embed_pooler.model.get_input_embeddings()(next_true_tokens)\n",
    "        # model.base_model.model.get_input_embeddings()(next_true_tokens)\n",
    "        model.get_input_embeddings()(next_true_tokens)\n",
    "    ).to(torch.bfloat16)\n",
    "\n",
    "    # compressed_part = model.base_model.embed_pooler(new_embeds_for_compression)\n",
    "    new_embeds_for_compression = original_embeds.reshape(\n",
    "        windows_amount, WINDOW_SIZE, -1\n",
    "    )\n",
    "    compressed_part = model.base_model.embed_pooler(new_embeds_for_compression)\n",
    "    compressed_part = compressed_part.reshape(1, windows_amount, -1)\n",
    "    start_embed = torch.rand_like(start_embed)\n",
    "    compressed_part = torch.rand_like(compressed_part)\n",
    "    end_embed = torch.rand_like(end_embed)\n",
    "    generated_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds,\n",
    "            start_embed,\n",
    "            compressed_part,\n",
    "            end_embed,\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    print(\"COMPRESSED PART\", tokenizer.decode(next_true_tokens[-1]))\n",
    "    print(\"===\")\n",
    "    generated_ids_compressed = model.generate(\n",
    "        inputs_embeds=generated_embeds,\n",
    "        max_new_tokens=2800,\n",
    "        # max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "        # do_sample=True,\n",
    "        # temperature=0.6,\n",
    "        # top_p=0.95,\n",
    "    )\n",
    "    # break\n",
    "print(tokenizer.decode(generated_ids_compressed[-1]))\n",
    "# break\n",
    "# embeds_generation_tokens = generated_tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Suppose that $f$ is a function and $f^{-1}$ is the inverse of $f$.  If $f(1)=2$, $f(2) = 6$, and $f(3)=5$, then what is $f^{-1}(f^{-1}(6))$?',\n",
       " 'solution': 'Since $f(2) = 6$, we have $f^{-1}(6)=2$. (Note that the hypothesis that $f$ has an inverse implies that there are no other values of $x$ with $f(x) = 6$.)  Similarly, $f(1) =2$ implies $f^{-1}(2)=1$.   So $f^{-1}(f^{-1}(6))=f^{-1}(2)=\\\\boxed{1}$.',\n",
       " 'answer': '1',\n",
       " 'subject': 'Algebra',\n",
       " 'level': 4,\n",
       " 'unique_id': 'test/algebra/1199.json',\n",
       " 'model_answer': \"Okay, so I have this problem here: There's a function f, and its inverse function is f inverse. I know that f(1) = 2, f(2) = 6, and f(3) = 5. I need to find f inverse of f inverse of 6, which is written as f^{-1}(f^{-1}(6)). Hmm, okay. Let me think about how to approach this.\\n\\nFirst, let me recall what an inverse function does. If f is a function that maps an input to an output, then the inverse function f^{-1} maps the output back to the input. So, basically, if f(a) = b, then f^{-1}(b) = a. That makes sense. So, if I have f(1)=2, that means f^{-1}(2)=1. Similarly, f(2)=6 implies f^{-1}(6)=2, and f(3)=5 means f^{-1}(5)=3. Got that.\\n\\nSo, I need to compute f^{-1}(f^{-1}(6)). Let me break this down step by step. First, let's compute the innermost part: f^{-1}(6). From the given information, since f(2)=6, f^{-1}(6) should be 2. So, f^{-1}(6) = 2.\\n\\nNow, the next step is to compute f inverse of that result. So, f^{-1}(2). Again, from the given data, since f(1)=2, that means f^{-1}(2)=1. So, f^{-1}(2)=1.\\n\\nTherefore, putting it together, f^{-1}(f^{-1}(6)) = f^{-1}(2) = 1.\\n\\nWait, is that all? It seems straightforward, but let me double-check to make sure I haven't missed anything.\\n\\nFirst, f^{-1}(6) is 2 because f(2)=6. Then, f^{-1}(2) is 1 because f(1)=2. So, yes, it looks like the answer is 1. But just to be thorough, let me think about whether the function f is invertible. For a function to have an inverse, it must be bijective, meaning it's both injective (one-to-one) and surjective (onto). Given the information, we have f(1)=2, f(2)=6, f(3)=5. So, each input maps to a unique output, and since the outputs are 2, 6, and 5, which are all distinct, f is injective. Is it surjective? The codomain of f is at least the set {2,5,6}, but unless the function is defined for all real numbers or some other set, it might not be surjective. But since the problem doesn't specify the codomain, I think it's safe to assume that the function is defined on the domain {1,2,3} and maps to {2,5,6}.\\n\\nSo, as long as f is bijective on its domain, which I think it is based on the given points, f is invertible. Therefore, the inverse function f^{-1} exists.\\n\\nAlternatively, if I think of f as a set of ordered pairs: f = {(1,2), (2,6), (3,5)}. Then, the inverse function f^{-1} would be the set of ordered pairs where the first and second elements are swapped: f^{-1} = {(2,1), (6,2), (5,3)}. So, f^{-1}(6) would be 2 because in f^{-1}, 6 is paired with 2. Similarly, f^{-1}(2) is 1. So, f^{-1}(f^{-1}(6)) is f^{-1}(2) = 1. Yep, that confirms it.\\n\\nI don't see any mistakes in this reasoning. The key was understanding how inverse functions work and applying them step by step. Each application of f^{-1} essentially undoes the original function f. So, starting from 6, we first find the input that maps to 6, which is 2, then find the input that maps to 2, which is 1. Therefore, the result is 1.\\n\\nJust to make sure I didn't mix up the order or misapply the function, let me visualize the functions. If I draw the function f, it's a mapping from 1‚Üí2, 2‚Üí6, 3‚Üí5. The inverse function f^{-1} would map 2‚Üí1, 6‚Üí2, 5‚Üí3. So, applying f^{-1} twice would first take 6 to 2, then take 2 to 1. So, f^{-1}(f^{-1}(6)) is indeed 1.\\n\\nAlternatively, if I think in terms of functions and their inverses, applying f^{-1} is like undoing f. So, first undo f to get from 6 back to 2, then undo f again to get from 2 back to 1. So, that's consistent with the previous reasoning.\\n\\nIs there any alternative way someone might approach this problem incorrectly? Maybe someone could think that since f(1)=2, then f^{-1}(2)=1, and then maybe f^{-1}(1) is 3? But no, because f(3)=5, so f^{-1}(5)=3, not f^{-1}(1). So, if someone thought that f^{-1}(1) is 3, they might make a mistake, but in this case, the question is f^{-1}(f^{-1}(6)), so the second inverse is f^{-1}(2), which is 1, not f^{-1}(1). So, it's important to apply the inverse functions in the correct order.\\n\\nAnother potential confusion could be if the function f is not invertible. For example, if f wasn't bijective, then it wouldn't have an inverse. But in this problem, since f is given as a function with three distinct inputs mapping to three distinct outputs, it's injective and surjective on its domain, so it is invertible.\\n\\nWait, actually, hold on a second. The problem didn't specify whether the domain is the set {1,2,3} or some other set. If the function f is defined on a larger domain, but only these three points are given, then f isn't necessarily bijective unless it's defined for all real numbers or some other domain. But since we are only dealing with the mappings given, I think it's safe to assume that f is only defined on {1,2,3}, so it's bijective on that domain, hence invertible.\\n\\nTherefore, all my reasoning seems solid. I think the answer is definitely 1.\\n\\n**Final Answer**\\n\\\\boxed{1}\\n</think>\\n\\nGiven the function \\\\( f \\\\) and its inverse \\\\( f^{-1} \\\\), we know the following values:\\n- \\\\( f(1) = 2 \\\\)\\n- \\\\( f(2) = 6 \\\\)\\n- \\\\( f(3) = 5 \\\\)\\n\\nWe need to find \\\\( f^{-1}(f^{-1}(6)) \\\\).\\n\\nFirst, we determine \\\\( f^{-1}(6) \\\\). Since \\\\( f(2) = 6 \\\\), it follows that \\\\( f^{-1}(6) = 2 \\\\).\\n\\nNext, we use this result to find \\\\( f^{-1}(2) \\\\). Since \\\\( f(1) = 2 \\\\), it follows that \\\\( f^{-1}(2) = 1 \\\\).\\n\\nTherefore, \\\\( f^{-1}(f^{-1}(6)) = f^{-1}(2) = 1 \\\\).\\n\\nThe final answer is \\\\(\\\\boxed{1}\\\\).\"}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_dataset[dataset_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_true_tokens.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(725, 200, 825, torch.Size([1, 1643]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids_compressed.shape[1], next_true_tokens.shape[\n",
    "    1\n",
    "], generated_ids_compressed.shape[1] + compressed_part.shape[1], torch.tensor(\n",
    "    generated_ids\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(generated_ids[-1][:925]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_think = tokenizer.encode(\"</think>\", add_special_tokens=False)[0]\n",
    "generated_ids[-1].index(end_think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(generated_ids_compressed[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards it at 24 mph. So, their relative speed is the difference between their speeds because they're moving towards each other. Wait, no, actually, since the elephant is moving away, the lion has to cover the distance that the elephant is moving away plus the distance the elephant covers while the lion is moving towards it.\\n\\nLet me clarify. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the lion is closing the gap at a rate of 24 mph minus 19 mph, which is 5 mph. That makes sense because if two objects are moving towards each other, their relative speed is the sum of their speeds, but in this case, one is moving away and the other is moving towards, so it's the difference.\\n\\nSo, the initial distance between them is 1 mile. The lion is closing the gap at 5 mph. To find the time it takes to catch up, I can use the formula:\\n\\nTime = Distance / Speed\\n\\nSo, plugging in the numbers, the time should be 1 mile divided by 5 mph. That gives me 0.2 hours. But the question asks for the time in minutes, so I need to convert 0.2 hours to minutes. Since 1 hour is 60 minutes, 0.2 hours is 0.2 * 60 = 12 minutes.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, the relative speed is 24 - 19 = 5 mph. So, yes, the lion is gaining on the elephant at 5 mph. So, 1 mile divided by 5 mph is indeed 0.2 hours, which is 12 minutes. That seems right.\\n\\nBut just to make sure I didn't make a mistake, let me think about it another way. Maybe set up an equation for their positions as functions of time and see if I get the same result.\\n\\nLet's denote t as the time in hours it takes for the lion to catch the elephant. In that time, the elephant will have moved a distance of 19t miles away from the starting point, and the lion will have moved 24t miles towards the elephant. Since they start 1 mile apart, the distance between them when the lion catches the elephant will be zero.\\n\\nSo, the distance the lion covers plus the distance the elephant covers should equal the initial distance between them. Wait, no, actually, the lion is moving towards the elephant, so the distance the lion covers is 24t, and the distance the elephant covers is 19t. But since the elephant is moving away, the total distance between them when the lion catches up is 24t - 19t = 5t. This should equal the initial distance, which is 1 mile.\\n\\nSo, 5t = 1 mile. Solving for t, we get t = 1/5 hours, which is 0.2 hours. Converting that to minutes, 0.2 * 60 = 12 minutes. Yep, same result. So, that seems consistent.\\n\\nAlternatively, maybe I can\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next_true_tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" think about it in terms of relative velocity. The lion is moving at 24 mph towards the elephant, which is moving away at 19 mph. So, the lion's speed relative to the elephant is 24 - 19 = 5 mph. So, the lion needs to cover 1 mile at a relative speed of 5 mph, which is 1/5 hours, or 12 minutes. Yep, same answer.\\n\\nWait, another way to think about it is to set up equations for their positions over time. Let's assume the lion starts at position 0, and the elephant starts at position 1 mile. The lion is moving towards the elephant at 24 mph, so its position at time t is 24t. The elephant is moving away from the lion at 19 mph, so its position at time t is 1 + 19t. The lion catches up when their positions are equal, so:\\n\\n24t = 1 + 19t\\n\\nSubtract 19t from both sides:\\n\\n5t = 1\\n\\nSo, t = 1/5 hours, which is 12 minutes. Yep, same result.\\n\\nI think I've approached this problem from multiple angles now, and each time I get the same answer: 12 minutes. So, I feel confident that 12 minutes is the correct answer.\\n\\nJust to make sure, let me think about the units. The speeds are in miles per hour, and the distance is in miles. So, when I divide 1 mile by 5 mph, the units are hours, which is correct. Then, converting hours to minutes by multiplying by 60 gives me 12 minutes. Perfect.\\n\\nI don't think I made any unit conversion errors here. The speeds are in mph, the distance is in miles, so the time comes out in hours, which I can convert to minutes. So, that seems right.\\n\\nAnother way to check is to think about how much distance the lion covers in 12 minutes. 12 minutes is 0.2 hours. So, 24 mph * 0.2 hours = 4.8 miles. The elephant is moving away at 19 mph, so in 12 minutes, it moves 19 * 0.2 = 3.8 miles. So, the elephant is 3.8 miles away from the starting point, and the lion is 4.8 miles from the starting point. The distance between them is 4.8 - 3.8 = 1 mile, which is the initial distance. So, that checks out.\\n\\nSimilarly, if I think about the elephant's speed relative to the lion, it's 19 mph, and the lion is closing the gap at 5 mph. So, in 12 minutes, the lion closes 1 mile, which is exactly the initial distance. So, that also makes sense.\\n\\nI think I've covered all the angles here. I can't think of another way to approach this problem, and all the methods lead to the same conclusion. So, I'm pretty sure that the answer is 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs towards the elephant at 24 miles per hour. To determine how long it will take for the lion to catch the elephant, we need to consider their relative speed.\\n\\nThe relative speed at which the lion is closing the gap is the difference between their speeds:\\n\\\\[ 24 \\\\text{ mph} - 19 \\\\text{ mph} = 5 \\\\text{ mph} \\\\]\\n\\nThe time it takes for the lion to catch the elephant can be calculated by dividing the initial distance by the relative speed:\\n\\\\[ \\\\text{Time} = \\\\frac{\\\\text{Distance}}{\\\\text{Relative Speed}} = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = 0.2 \\\\text{ hours} \\\\]\\n\\nConverting 0.2 hours to minutes:\\n\\\\[ 0.2 \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes} \\\\]\\n\\nThus, the lion will catch the elephant in \\\\boxed{12} minutes.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_ids_compressed[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 1536])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\n",
      "\n",
      "First, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards it at 24 mph. So, their relative speed is the difference between their speeds because they're moving towards each other. Wait, no, actually, since the elephant is moving away, the lion has to cover the distance that the elephant is moving away plus the distance the elephant covers while the lion is moving towards it.\n",
      "\n",
      "Let me clarify. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the lion is closing the gap at a rate of 24 mph minus 19 mph, which is 5 mph. That makes sense because if two objects are moving towards each other, their relative speed is the sum of their speeds, but in this case, one is moving away and the other is moving towards, so it's the difference.\n",
      "\n",
      "So, the initial distance between them is 1 mile. The lion is closing the gap at 5 mph. To find the time it takes to catch up, I can use the formula:\n",
      "\n",
      "Time = Distance / Speed\n",
      "\n",
      "So, plugging in the numbers, the time should be 1 mile divided by 5 mph. That gives me 0.2 hours. But the question asks for the time in minutes, so I need to convert 0.2 hours to minutes. Since 1 hour is 60 minutes, 0.2 hours is 0.2 * 60 = 12 minutes.\n",
      "\n",
      "Wait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, the relative speed is 24 - 19 = 5 mph. So, yes, the lion is gaining on the elephant at 5 mph. So, 1 mile divided by 5 mph is indeed 0.2 hours, which is 12 minutes. That seems right.\n",
      "\n",
      "But just to make sure I didn't make a mistake, let me think about it another way. Maybe set up an equation for their positions as functions of time and see if I get the same result.\n",
      "\n",
      "Let's denote t as the time in hours it takes for the lion to catch the elephant. In that time, the elephant will have moved a distance of 19t miles away from the starting point, and the lion will have moved 24t miles towards the elephant. Since they start 1 mile apart, the distance between them when the lion catches the elephant will be zero.\n",
      "\n",
      "So, the distance the lion covers plus the distance the elephant covers should equal the initial distance between them. Wait, no, actually, the lion is moving towards the elephant, so the distance the lion covers is 24t, and the distance the elephant covers is 19t. But since the elephant is moving away, the total distance between them when the lion catches up is 24t - 19t = 5t. This should equal the initial distance, which is 1 mile.\n",
      "\n",
      "So, 5t = 1 mile. Solving for t, we get t = 1/5 hours, which is 0.2 hours. Converting that to minutes, 0.2 * 60 = 12 minutes. Yep, same result. So, that seems consistent.\n",
      "\n",
      "Alternatively, maybe I can think about it in terms of how much distance the lion needs to cover relative to the elephant. Since the lion is moving faster, it's gaining on the elephant at 5 mph. So, the lion needs to cover the 1 mile gap at a relative speed of 5 mph. So, time = distance / speed = 1 / 5 hours, which is 12 minutes. Yep, same answer.\n",
      "\n",
      "I think that's solid. So, the lion will catch the elephant in 12 minutes.\n",
      "\n",
      "**Final Answer**\n",
      "The lion will catch the elephant in \\boxed{12} minutes.\n",
      "</think>\n",
      "\n",
      "The elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. \n",
      "\n",
      "To find the time it takes for the lion to catch the elephant, we first determine their relative speed. Since the lion is moving towards the elephant and the elephant is moving away, their relative speed is the difference between their speeds:\n",
      "\n",
      "\\[\n",
      "24 \\text{ mph} - 19 \\text{ mph} = 5 \\text{ mph}\n",
      "\\]\n",
      "\n",
      "The initial distance between them is 1 mile. Using the formula for time, which is distance divided by speed, we get:\n",
      "\n",
      "\\[\n",
      "\\text{Time} = \\frac{1 \\text{ mile}}{5 \\text{ mph}} = 0.2 \\text{ hours}\n",
      "\\]\n",
      "\n",
      "Converting 0.2 hours to minutes:\n",
      "\n",
      "\\[\n",
      "0.2 \\text{ hours} \\times 60 \\text{ minutes per hour} = 12 \\text{ minutes}\n",
      "\\]\n",
      "\n",
      "Thus, the lion will catch the elephant in \\boxed{12} minutes.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(torch.tensor(generated_ids[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards it at 24 mph. So, their relative speed is the difference between their speeds because they're moving towards each other. Wait, no, actually, since the elephant is moving away, the lion has to cover the distance that the elephant is moving away plus the distance the elephant covers while the lion is moving towards it.\\n\\nLet me clarify. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the lion is closing the gap at a rate of 24 mph minus 19 mph, which is 5 mph. That makes sense because if two objects are moving towards each other, their relative speed is the sum of their speeds, but in this case, one is moving away and the other is moving towards, so it's the difference.\\n\\nSo, the initial distance between them is 1 mile. The lion is closing the gap at 5 mph. To find the time it takes to catch up, I can use the formula:\\n\\nTime = Distance / Speed\\n\\nSo, plugging in the numbers, the time should be 1 mile divided by 5 mph. That gives me 0.2 hours. But the question asks for the time in minutes, so I need to convert 0.2 hours to minutes. Since 1 hour is 60 minutes, 0.2 hours is 0.2 * 60 = 12 minutes.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, the relative speed is 24 - 19 = 5 mph. So, yes, the lion is gaining on the elephant at 5 mph. So, 1 mile divided by 5 mph is indeed 0.2 hours, which is 12 minutes. That seems right.\\n\\nBut just to make sure I didn't make a mistake, let me think about it another way. Maybe set up an equation for their positions as functions of time and see if I get the same result.\\n\\nLet's denote t as the time in hours it takes for the lion to catch the elephant. In that time, the elephant will have moved a distance of 19t miles away from the starting point, and the lion will have moved 24t miles towards the elephant. Since they start 1 mile apart, the distance between them when the lion catches the elephant will be zero.\\n\\nSo, the distance the lion covers plus the distance the elephant covers should equal the initial distance between them. Wait, no, actually, the lion is moving towards the elephant, so the distance the lion covers is 24t, and the distance the elephant covers is 19t. But since the elephant is moving away, the total distance between them when the lion catches up is 24t - 19t = 5t. This should equal the initial distance, which is 1 mile.\\n\\nSo, 5t = 1 mile. Solving for t, we get t = 1/5 hours, which is 0.2 hours. Converting that to minutes, 0.2 * 60 = 12 minutes. Yep, same result. So, that seems consistent.\\n\\nAlternatively, maybe I can think about it in terms of how much distance the lion needs to cover relative to the elephant. Since the lion is moving faster, it's gaining on the elephant at 5 mph. So, the lion needs to cover the 1 mile gap at a relative speed of 5 mph. So, time = distance / speed = 1 / 5 hours, which is 12 minutes. Yep, same answer.\\n\\nI think that's solid. So, the lion will catch the elephant in 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. \\n\\nTo find the time it takes for the lion to catch the elephant, we first determine their relative speed. Since the lion is moving towards the elephant and the elephant is moving away, their relative speed is the difference between their speeds:\\n\\n\\\\[\\n24 \\\\text{ mph} - 19 \\\\text{ mph} = 5 \\\\text{ mph}\\n\\\\]\\n\\nThe initial distance between them is 1 mile. Using the formula for time, which is distance divided by speed, we get:\\n\\n\\\\[\\n\\\\text{Time} = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = 0.2 \\\\text{ hours}\\n\\\\]\\n\\nConverting 0.2 hours to minutes:\\n\\n\\\\[\\n0.2 \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes}\\n\\\\]\\n\\nThus, the lion will catch the elephant in \\\\boxed{12} minutes.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.tensor(generated_ids[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" think about it in terms of relative velocity. The lion is moving at 24 mph towards the elephant, which is moving away at 19 mph. So, the lion's speed relative to the elephant is 24 - 19 = 5 mph. So, the lion needs to cover 1 mile at a relative speed of 5 mph, which is 1/5 hours, or 12 minutes. Yep, same answer.\\n\\nWait, another way to think about it is to set up equations for their positions over time. Let's assume the lion starts at position 0, and the elephant starts at position 1 mile. The lion is moving towards the elephant at 24 mph, so its position at time t is 24t. The elephant is moving away from the lion at 19 mph, so its position at time t is 1 + 19t. The lion catches up when their positions are equal, so:\\n\\n24t = 1 + 19t\\n\\nSubtract 19t from both sides:\\n\\n5t = 1\\n\\nSo, t = 1/5 hours, which is 12 minutes. Yep, same result.\\n\\nI think I've approached this problem from multiple angles now, and each time I get the same answer: 12 minutes. So, I feel confident that 12 minutes is the correct answer.\\n\\nJust to make sure, let me think about the units. The speeds are in miles per hour, and the distance is in miles. So, when I divide 1 mile by 5 mph, the units are hours, which is correct. Then, converting hours to minutes by multiplying by 60 gives me 12 minutes. Perfect.\\n\\nI don't think I made any unit conversion errors here. The speeds are in mph, the distance is in miles, so the time comes out in hours, which I can convert to minutes. So, that seems right.\\n\\nAnother way to check is to think about how much distance the lion covers in 12 minutes. 12 minutes is 0.2 hours. So, 24 mph * 0.2 hours = 4.8 miles. The elephant is moving away at 19 mph, so in 12 minutes, it moves 19 * 0.2 = 3.8 miles. So, the elephant is 3.8 miles away from the starting point, and the lion is 4.8 miles from the starting point. The distance between them is 4.8 - 3.8 = 1 mile, which is the initial distance. So, that checks out.\\n\\nSimilarly, if I think about the elephant's speed relative to the lion, it's 19 mph, and the lion is closing the gap at 5 mph. So, in 12 minutes, the lion closes 1 mile, which is exactly the initial distance. So, that also makes sense.\\n\\nI think I've covered all the angles here. I can't think of another way to approach this problem, and all the methods lead to the same conclusion. So, I'm pretty sure that the answer is 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs towards the elephant at 24 miles per hour. To determine how long it will take for the lion to catch the elephant, we need to consider their relative speed.\\n\\nThe relative speed at which the lion is closing the gap is the difference between their speeds:\\n\\\\[ 24 \\\\text{ mph} - 19 \\\\text{ mph} = 5 \\\\text{ mph} \\\\]\\n\\nThe time it takes for the lion to catch the elephant can be calculated by dividing the initial distance by the relative speed:\\n\\\\[ \\\\text{Time} = \\\\frac{\\\\text{Distance}}{\\\\text{Relative Speed}} = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = 0.2 \\\\text{ hours} \\\\]\\n\\nConverting 0.2 hours to minutes:\\n\\\\[ 0.2 \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes} \\\\]\\n\\nThus, the lion will catch the elephant in \\\\boxed{12} minutes.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\"\n",
    "\n",
    "# \"Okay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards it at 24 mph. So, their relative speed is the difference between their speeds because they're moving towards each other. Wait, no, actually, since the elephant is moving away, the lion has to cover the distance that the elephant is moving away plus the distance the elephant covers while the lion is moving towards it.\\n\\nLet me clarify. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the lion is closing the gap at a rate of 24 mph minus 19 mph, which is 5 mph. That makes sense because if two objects are moving towards each other, their relative speed is the sum of their speeds, but in this case, one is moving away and the other is moving towards, so it's the difference.\\n\\nSo, the initial distance between them is 1 mile. The lion is closing the gap at 5 mph. To find the time it takes to catch up, I can use the formula:\\n\\nTime = Distance / Speed\\n\\nSo, plugging in the numbers, the time should be 1 mile divided by 5 mph. That gives me 0.2 hours. But the question asks for the time in minutes, so I need to convert 0.2 hours to minutes. Since 1 hour is 60 minutes, 0.2 hours is 0.2 * 60 = 12 minutes.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, the relative speed is 24 - 19 = 5 mph. So, yes, the lion is gaining on the elephant at 5 mph. So, 1 mile divided by 5 mph is indeed 0.2 hours, which is 12 minutes. That seems right.\\n\\nBut just to make sure I didn't make a mistake, let me think about it another way. Maybe set up an equation for their positions as functions of time and see if I get the same result.\\n\\nLet's denote t as the time in hours it takes for the lion to catch the elephant. In that time, the elephant will have moved a distance of 19t miles away from the starting point, and the lion will have moved 24t miles towards the elephant. Since they start 1 mile apart, the distance between them when the lion catches the elephant will be zero.\\n\\nSo, the distance the lion covers plus the distance the elephant covers should equal the initial distance between them. Wait, no, actually, the lion is moving towards the elephant, so the distance the lion covers is 24t, and the distance the elephant covers is 19t. But since the elephant is moving away, the total distance between them when the lion catches up is 24t - 19t = 5t. This should equal the initial distance, which is 1 mile.\\n\\nSo, 5t = 1 mile. Solving for t, we get t = 1/5 hours, which is 0.2 hours. Converting that to minutes, 0.2 * 60 = 12 minutes. Yep, same result. So, that seems consistent.\\n\\nAlternatively, maybe I can think about it in terms of how much distance the lion needs to cover relative to the elephant. Since the lion is moving faster, it's gaining on the elephant at 5 mph. So, the lion needs to cover the 1 mile gap at a relative speed of 5 mph. So, time = distance / speed = 1 / 5 hours, which is 12 minutes. Yep, same answer.\\n\\nI think that's solid. So, the lion will catch the elephant in 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. \\n\\nTo find the time it takes for the lion to catch the elephant, we first determine their relative speed. Since the lion is moving towards the elephant and the elephant is moving away, their relative speed is the difference between their speeds:\\n\\n\\\\[\\n24 \\\\text{ mph} - 19 \\\\text{ mph} = 5 \\\\text{ mph}\\n\\\\]\\n\\nThe initial distance between them is 1 mile. Using the formula for time, which is distance divided by speed, we get:\\n\\n\\\\[\\n\\\\text{Time} = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = 0.2 \\\\text{ hours}\\n\\\\]\\n\\nConverting 0.2 hours to minutes:\\n\\n\\\\[\\n0.2 \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes}\\n\\\\]\\n\\nThus, the lion will catch the elephant in \\\\boxed{12} minutes.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_true_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1536])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 224256])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeds_for_compression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_embeds torch.Size([1, 62, 1536])\n",
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>Here's a question: What do many people believe happens after you die?  Here are possible answers to this question: - stop moving - nothing - go to heaven - stop living - stop breathing  I believe the correct choice is \"go to heaven\", here's why:\n",
      "Answer:<ÔΩúAssistantÔΩú><think>\n",
      "Okay, so I'm trying to figure out the answer is correct. Let me think about it again. I think the user is trying to figure out the correct answer to the question they're asking. They provided a list of answers, and I need to heaven, but I'm not sure if I'm sure if I'm on the right track. Let me break it down step by step. The question is about what people believe happens after you die. I know that when someone dies, they believe that people often believe in something called the afterlife, which is the belief that after you die, you don't need to move or anything else. So, the correct answer is \"go to heaven\", which is the correct answer. The other options, like stopping moving, nothing, stop breathing, etc., aren't correct\n"
     ]
    }
   ],
   "source": [
    "from hidden_capacity_reasoning.utils import WINDOW_SIZE, VISION_START, VISION_END\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "\n",
    "def _crop_past_key_values(model, past_key_values, max_length):\n",
    "    \"\"\"Crops the past key values up to a certain maximum length.\"\"\"\n",
    "    new_past = []\n",
    "    if model.config.is_encoder_decoder:\n",
    "        for idx in range(len(past_key_values)):\n",
    "            new_past.append(\n",
    "                (\n",
    "                    past_key_values[idx][0][:, :, :max_length, :],\n",
    "                    past_key_values[idx][1][:, :, :max_length, :],\n",
    "                    past_key_values[idx][2],\n",
    "                    past_key_values[idx][3],\n",
    "                )\n",
    "            )\n",
    "        past_key_values = tuple(new_past)\n",
    "    # gptbigcode is special and stores kv in shape (batch_size, seq_len, dim), if it's a multi_query model\n",
    "    elif \"gptbigcode\" in model.__class__.__name__.lower() or (\n",
    "        model.config.architectures is not None\n",
    "        and \"gptbigcode\" in model.config.architectures[0].lower()\n",
    "    ):\n",
    "        if model.config.multi_query:\n",
    "            for idx in range(len(past_key_values)):\n",
    "                past_key_values[idx] = past_key_values[idx][:, :max_length, :]\n",
    "        else:\n",
    "            for idx in range(len(past_key_values)):\n",
    "                past_key_values[idx] = past_key_values[idx][:, :, :max_length, :]\n",
    "    elif isinstance(past_key_values, DynamicCache):\n",
    "        past_key_values.crop(max_length)\n",
    "    elif past_key_values is not None:\n",
    "        for idx in range(len(past_key_values)):\n",
    "            if past_key_values[idx] != ([], []):\n",
    "                new_past.append(\n",
    "                    (\n",
    "                        past_key_values[idx][0][:, :, :max_length, :],\n",
    "                        past_key_values[idx][1][:, :, :max_length, :],\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                new_past.append((past_key_values[idx][0], past_key_values[idx][1]))\n",
    "        past_key_values = tuple(new_past)\n",
    "    return past_key_values\n",
    "\n",
    "\n",
    "# model = trainer.model\n",
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        # {\"role\": \"user\", \"content\": \"how many wings has a bird?\"},\n",
    "        {\"role\": \"user\", \"content\": dataset[\"test\"].to_list()[:5][0][\"question\"]},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "with torch.no_grad(), torch.autocast(device_type=\"cuda\"):\n",
    "    start_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "        torch.tensor([[VISION_START]], device=\"cuda\")\n",
    "    )\n",
    "    end_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "        torch.tensor([[VISION_END]], device=\"cuda\")\n",
    "    )\n",
    "    generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "    generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "    temp_gen_size = 0\n",
    "    window_size = WINDOW_SIZE  # + 1\n",
    "    # new_tokens = 4\n",
    "    new_tokens = 1\n",
    "    generation_started = False\n",
    "    max_steps = (new_tokens + window_size) * 15\n",
    "    past_key_values_big = None\n",
    "    print(\"generated_embeds\", generated_embeds.shape)\n",
    "    for step in range(max_steps):\n",
    "        if temp_gen_size == window_size + new_tokens:\n",
    "            # print(\n",
    "            #     \"TOKENS FOR EMDED\",\n",
    "            #     tokenizer.decode(\n",
    "            #         generated_tokens[:, -(window_size + new_tokens) :][:, :WINDOW_SIZE]\n",
    "            #         .cpu()\n",
    "            #         .tolist()[0]\n",
    "            #     ),\n",
    "            # )\n",
    "            # tokenizer.decode(generated_tokens[:, : -window_size ].cpu().tolist()[0])\n",
    "            if hasattr(model.base_model, \"embed_pooler\"):\n",
    "                new_embeds_for_compression = (\n",
    "                    model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "                        generated_tokens[:, -(window_size + new_tokens) :][\n",
    "                            :, :WINDOW_SIZE\n",
    "                        ]\n",
    "                    )\n",
    "                ).to(torch.bfloat16)\n",
    "                compressed_part = model.base_model.embed_pooler(\n",
    "                    new_embeds_for_compression\n",
    "                )\n",
    "            else:\n",
    "                compressed_part = model.embed_pooler(new_embeds_for_compression)\n",
    "            # gen_embeds_prev = generated_tokens.shape[1]\n",
    "            if generation_started:\n",
    "                # past_key_values_big = _crop_past_key_values(\n",
    "                #     model=model,\n",
    "                #     past_key_values=past_key_values_big,\n",
    "                #     max_length=generated_embeds.shape[1] - new_tokens - 2,\n",
    "                # )\n",
    "                generated_embeds = torch.cat(\n",
    "                    [\n",
    "                        generated_embeds[:, : -(window_size + new_tokens + 1)],\n",
    "                        # generated_embeds[:, : -(window_size + new_tokens)],\n",
    "                        compressed_part,\n",
    "                        # torch.randn(1, 1, 1536, device=\"cuda\"),\n",
    "                        end_embed,\n",
    "                        generated_embeds[:, -new_tokens:],\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            else:\n",
    "                # past_key_values_big = _crop_past_key_values(\n",
    "                #     model=model,\n",
    "                #     past_key_values=past_key_values_big,\n",
    "                #     max_length=generated_embeds.shape[1] - new_tokens - 3,\n",
    "                # )\n",
    "                generated_embeds = torch.cat(\n",
    "                    [\n",
    "                        generated_embeds[:, : -(window_size + new_tokens)],\n",
    "                        start_embed,\n",
    "                        # torch.randn(1, 1, 1536, device=\"cuda\"),\n",
    "                        compressed_part,\n",
    "                        end_embed,\n",
    "                        generated_embeds[:, -new_tokens:],\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                generation_started = True\n",
    "            past_key_values_big = _crop_past_key_values(\n",
    "                model=model,\n",
    "                past_key_values=past_key_values_big,\n",
    "                max_length=generated_embeds.shape[1] - new_tokens - 2,\n",
    "            )\n",
    "            temp_gen_size = 1\n",
    "\n",
    "        outputs = model(\n",
    "            inputs_embeds=generated_embeds,\n",
    "            past_key_values=past_key_values_big,\n",
    "            # use_cache=False,\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        past_key_values_big = outputs.past_key_values\n",
    "        top_token = logits.argmax(-1)[-1][-1]\n",
    "        top_token_embed = model.get_input_embeddings()(top_token)\n",
    "        # print(top)\n",
    "        generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "\n",
    "        generated_embeds = torch.cat(\n",
    "            [generated_embeds, top_token_embed.reshape(1, 1, -1)], dim=1\n",
    "        )\n",
    "        # print(temp_gen_size, tokenizer.decode(generated_tokens[-1]))\n",
    "\n",
    "        temp_gen_size += 1\n",
    "\n",
    "print(tokenizer.decode(generated_tokens[-1]))\n",
    "\n",
    "# break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# —è –Ω–µ –∑–Ω–∞—é, –º–æ–∂–µ—Ç –±—ã—Ç—å —è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ —É–ø—Ä–∞–≤–ª—è—é—Å—å —Å KV-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 227])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1536])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeds_for_compression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5, 6, 7, 8][:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"test\"][0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 1536])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 227])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ MATH-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff860ec38f340e0a1dc58e696e99c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRONG 12\n",
      " 1 mile.\n",
      "\n",
      "Let me define the position of the lion as a function of time: \\( P_{lion}(t) = 0 + 24t \\).\n",
      "\n",
      "The position of the elephant, moving away from the lion, is \\( P_{elephant}(t) = 1 - 19t \\).\n",
      "\n",
      "They will meet when \\( P_{lion}(t) = P_{elephant}(t) \\).\n",
      "\n",
      "So, setting them equal:\n",
      "\n",
      "\\( 24t = 1 - 19t \\)\n",
      "\n",
      "Combine like terms:\n",
      "\n",
      "\\( 24t + 19t = 1 \\)\n",
      "\n",
      "\\( 43t = 1 \\)\n",
      "\n",
      "\\( t = 1 / 43 \\) hours.\n",
      "\n",
      "Wait, hold on, this is different from before. Earlier, I thought it was 1/5 hours, which is 12 minutes, but now I'm getting 1/43 hours. Which one is correct?\n",
      "\n",
      "Wait, let me check my equations.\n",
      "\n",
      "Position of lion: starting at 0, moving towards positive direction at 24 mph. So, \\( P_{lion}(t) = 24t \\).\n",
      "\n",
      "Position of elephant: starting at 1, moving away from lion towards negative direction at 19 mph. So, \\( P_{elephant}(t) = 1 - 19t \\).\n",
      "\n",
      "Setting them equal: \\( 24t = 1 - 19t \\).\n",
      "\n",
      "So, \\( 24t + 19t = 1 \\)\n",
      "\n",
      "\\( 43t = 1 \\)\n",
      "\n",
      "\\( t = 1 / 43 \\) hours.\n",
      "\n",
      "Wait, so this is 1 divided by 43 hours. Let me convert that to minutes.\n",
      "\n",
      "Since 1 hour is 60 minutes, 1 / 43 hours is (1 / 43) * 60 minutes, which is approximately 1.395 minutes, which is about 1 minute and 23.7 seconds. But that doesn't seem right because if the lion is moving at 24 mph and the elephant is moving at 19 mph, the lion should catch up faster than that.\n",
      "\n",
      "Wait, but according to the first method, the relative speed is 5 mph, so time is 1 / 5 hours, which is 12 minutes. But here, I'm getting 1 / 43 hours, which is much less. So, there must be a mistake in my equations.\n",
      "\n",
      "Wait, let me think again. If the lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph, then the relative speed at which the distance between them is decreasing is 24 - 19 = 5 mph. So, the lion is gaining on the elephant at 5 mph.\n",
      "\n",
      "Since they are 1 mile apart, the time to close that distance is 1 / 5 hours, which is 12 minutes. So, that seems correct.\n",
      "\n",
      "But in the second method, I set up the positions as functions of time and solved for t, getting 1 / 43 hours. That's conflicting.\n",
      "\n",
      "Wait, maybe I mixed up the directions. Let me redefine the positions.\n",
      "\n",
      "Let me assume that the lion is at position 0, and the elephant is at position 1 mile. The lion is moving towards the positive direction at 24 mph, so its position is \\( 24t \\).\n",
      "\n",
      "The elephant is moving away from the lion, so its position is \\( 1 - 19t \\). Wait, why is it negative? Because if the elephant is moving away from the lion, which is at 0, then its position increases, but since it's moving away, it's going in the negative direction? Wait, no, if the lion is at 0, and the elephant is at 1, and the elephant is moving away from the lion, which direction is that? If the lion is at 0, moving towards the positive direction, the elephant is at 1, moving away would be towards the negative direction.\n",
      "\n",
      "Wait, so if I define the positive direction as the direction the lion is moving, then the elephant is moving in the negative direction at 19 mph.\n",
      "\n",
      "So, the position of the lion is \\( 24t \\).\n",
      "\n",
      "The position of the elephant is \\( 1 - 19t \\).\n",
      "\n",
      "They will meet when \\( 24t = 1 - 19t \\).\n",
      "\n",
      "So, \\( 24t + 19t = 1 \\)\n",
      "\n",
      "\\( 43t = 1 \\)\n",
      "\n",
      "\\( t = 1 / 43 \\) hours.\n",
      "\n",
      "But this is conflicting with the earlier method. So, which one is correct?\n",
      "\n",
      "Wait, maybe the issue is in the sign of the elephant's position. If the lion is moving towards the positive direction, and the elephant is moving away from the lion, which is at 0, then the elephant's position is decreasing if we consider the positive direction.\n",
      "\n",
      "Wait, no, if the lion is at 0, moving towards positive, and the elephant is at 1, moving away from the lion, which is moving towards positive, so the elephant is moving in the negative direction.\n",
      "\n",
      "So, the position of the elephant is \\( 1 - 19t \\).\n",
      "\n",
      "The position of the lion is \\( 24t \\).\n",
      "\n",
      "Setting them equal: \\( 24t = 1 - 19t \\).\n",
      "\n",
      "So, \\( 43t = 1 \\), \\( t = 1 / 43 \\) hours.\n",
      "\n",
      "But this is giving a much shorter time than the previous method. So, which is correct?\n",
      "\n",
      "Wait, let me think about it differently. Let me compute the distance covered by each elephant and the lion in t hours.\n",
      "\n",
      "In t hours, the lion moves 24t miles towards the elephant.\n",
      "\n",
      "In the same t hours, the elephant moves 19t miles away from the lion.\n",
      "\n",
      "But since they are moving towards each other, the distance between them decreases by (24 - 19) = 5 mph.\n",
      "\n",
      "So, the initial distance is 1 mile, moving towards each other at 5 mph, so time is 1 / 5 hours, which is 12 minutes.\n",
      "\n",
      "But in the position function, I'm getting 1 / 43 hours, which is much less. So, there must be a mistake in the position function setup.\n",
      "\n",
      "Wait, perhaps I should define the positions differently. Let me assume that the lion is at position 0, and the elephant is at position 1. The lion is moving towards the positive direction at 24 mph, so its position is 24t.\n",
      "\n",
      "The elephant is moving away from the lion, which is at 0, so if the lion is moving towards positive, the elephant is moving in the negative direction at 19 mph. So, its position is 1 - 19t.\n",
      "\n",
      "So, setting them equal: 24t = 1 - 19t.\n",
      "\n",
      "So, 24t + 19t = 1\n",
      "\n",
      "43t = 1\n",
      "\n",
      "t = 1 / 43 hours.\n",
      "\n",
      "But this is conflicting with the relative speed method.\n",
      "\n",
      "Wait, maybe I mixed up the directions. Let me think again.\n",
      "\n",
      "If the lion is moving towards the elephant, and the elephant is moving away, then the relative speed is lion speed minus elephant speed, because they are moving in opposite directions. Wait, no, if both are moving towards each other, their relative speed is the sum. If they are moving in the same direction, it's the difference.\n",
      "\n",
      "But in this case, the lion is moving towards the elephant, and the elephant is moving away, so their speeds are in opposite directions. So, relative to the lion, the elephant is moving towards it at 19 mph, but relative to the elephant, the lion is moving towards it at 24 mph.\n",
      "\n",
      "Wait, no, that's not correct. The relative speed when moving towards each other is the sum, but when moving in the same direction, it's the difference.\n",
      "\n",
      "But in this case, the lion is moving towards the elephant, and the elephant is moving away, so their speeds are in opposite directions. So, the relative speed is 24 + 19 = 43 mph.\n",
      "\n",
      "Wait, no, that can't be. If two objects are moving towards each other, their relative speed is the sum. If they are moving in the same direction, it's the difference.\n",
      "\n",
      "But in this case, the lion is moving towards the elephant, and the elephant is moving away, so their speeds are in opposite directions. So, the relative speed is 24 + 19 = 43 mph.\n",
      "\n",
      "Wait, but that would mean the lion is gaining on the elephant at 43 mph, so the time to catch up would be initial distance divided by relative speed, which is 1 / 43 hours, which is about 1.395 minutes, which is about 1 minute 23.7 seconds.\n",
      "\n",
      "But this contradicts the earlier method where I thought it was 1 / 5 hours.\n",
      "\n",
      "Wait, I think the confusion is in the definition of relative speed. Let me clarify.\n",
      "\n",
      "When two objects are moving towards each other, their relative speed is the sum of their individual speeds. When moving in the same direction, it's the difference.\n",
      "\n",
      "But in this case, the lion is moving towards the elephant, and the elephant is moving away, so their speeds are in opposite directions. So, the relative speed is indeed 24 + 19 = 43 mph.\n",
      "\n",
      "But wait, that would mean the lion is gaining on the elephant at 43 mph, so the time to cover 1 mile is 1 / 43 hours, which is about 1.395 minutes.\n",
      "\n",
      "But earlier, I thought it was 1 / 5 hours. So, which is correct?\n",
      "\n",
      "Wait, let me think about it again. If the lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph, then the distance between them is decreasing at a rate of 24 + 19 = 43 mph.\n",
      "\n",
      "Therefore, the time to cover the initial 1 mile at 43 mph is 1 / 43 hours, which is 60 / 43 minutes, which is approximately 1.395 minutes, which is about 1 minute 23.7 seconds.\n",
      "\n",
      "But this contradicts the earlier method where I thought it was 1 / 5 hours. So, which is correct?\n",
      "\n",
      "Wait, maybe I was wrong earlier. Let me think about it step by step.\n",
      "\n",
      "Let me define the positions again. Let me assume the lion is at position 0, and the elephant is at position 1 mile. The lion is moving towards the positive direction at 24 mph, so its position at time t is 24t.\n",
      "\n",
      "The elephant is moving away from the lion, which is at 0, so its position is 1 - 19t.\n",
      "\n",
      "Setting them equal: 24t = 1 - 19t.\n",
      "\n",
      "So, 24t + 19t = 1\n",
      "\n",
      "43t = 1\n",
      "\n",
      "t = 1 / 43 hours.\n",
      "\n",
      "But this is 1 / 43 hours, which is about 1.395 minutes, which is about 1 minute 23.7 seconds.\n",
      "\n",
      "But if I think about the relative speed, the lion is gaining on the elephant at 43 mph, so the time to cover 1 mile is 1 / 43 hours, which is the same as above.\n",
      "\n",
      "Wait, but earlier, I thought that the relative speed was 5 mph, which would give 1 / 5 hours. But that was a mistake.\n",
      "\n",
      "Wait, no, if the lion is moving towards the elephant, and the elephant is moving away, then the relative speed is indeed 24 + 19 = 43 mph. Because the lion is closing the distance at 24 mph, and the elephant is moving away at 19 mph, so the net effect is 24 + 19 = 43 mph towards the elephant.\n",
      "\n",
      "Therefore, the time to cover 1 mile at 43 mph is 1 / 43 hours, which is 60 / 43 minutes, which is approximately 1.395 minutes, which is about 1 minute 24 seconds.\n",
      "\n",
      "But wait, 60 / 43 is approximately 1.395, which is 1 minute and 0.395 * 60 = 23.7 seconds. So, about 1 minute 24 seconds.\n",
      "\n",
      "But earlier, I thought it was 1 / 5 hours, which is 12 minutes. So, which is correct?\n",
      "\n",
      "Wait, let me think about it in terms of distances.\n",
      "\n",
      "If the lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph, then the distance between them is decreasing at a rate of 24 - 19 = 5 mph.\n",
      "\n",
      "Wait, no, that's only if they are moving towards each other. But in this case, the lion is moving towards the elephant, and the elephant is moving away, so their relative speed is 24 + 19 = 43 mph.\n",
      "\n",
      "Wait, no, no, no. Let me think about it again.\n",
      "\n",
      "If two objects are moving towards each other, their relative speed is the sum. If they are moving in the same direction, it's the difference.\n",
      "\n",
      "But in this case, the lion is moving towards the elephant, and the elephant is moving away, so their directions are opposite. Therefore, the relative speed is 24 + 19 = 43 mph.\n",
      "\n",
      "Therefore, the time to cover the initial 1 mile at 43 mph is 1 / 43 hours, which is 60 / 43 minutes, which is approximately 1.395 minutes, which is about 1 minute 24 seconds.\n",
      "\n",
      "But wait, in the position function, I got the same result. So, why was I confused earlier?\n",
      "\n",
      "Wait, maybe I was confusing the relative speed when moving in the same direction. Let me think.\n",
      "\n",
      "If both are moving towards each other, the relative speed is the sum. If both are moving away from each other, the relative speed is the sum. If one is moving towards and the other is moving away, the relative speed is the difference.\n",
      "\n",
      "But in this case, the lion is moving towards the elephant, and the elephant is moving away, so their relative speed is 24 + 19 = 43 mph.\n",
      "\n",
      "Therefore, the time to cover 1 mile at 43 mph is 1 / 43 hours, which is 60 / 43 minutes, which is approximately 1.395 minutes, which is about 1 minute 24 seconds.\n",
      "\n",
      "But wait, in the position function, I got the same result, so that must be correct.\n",
      "\n",
      "Wait, but in the first method, I thought that the relative speed was 5 mph, which is incorrect. The correct relative speed is 43 mph.\n",
      "\n",
      "So, which one is correct? Let me think about it in another way.\n",
      "\n",
      "Let me assume that the lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the distance between them is decreasing at a rate of 24 + 19 = 43 mph.\n",
      "\n",
      "Therefore, the time to cover 1 mile at 43 mph is 1 / 43 hours, which is 60 / 43 minutes, which is approximately 1.395 minutes, which is about 1 minute 24 seconds.\n",
      "\n",
      "Alternatively, if I think about it in terms of how much distance the lion covers relative to the elephant.\n",
      "\n",
      "In the time it takes for the lion to catch the elephant, the lion will have moved 24t miles, and the elephant will have moved 19t miles away from the lion. Since they are moving towards each other, the total distance covered by both is 24t + 19t = 43t miles, which equals the initial distance of 1 mile.\n",
      "\n",
      "Therefore, 43t = 1, so t = 1 / 43 hours.\n",
      "\n",
      "Therefore, the time is 1 / 43 hours, which is 60 / 43 minutes, which is approximately 1.395 minutes, which is about 1 minute 24 seconds.\n",
      "\n",
      "But wait, 60 / 43 is exactly 1.3953488376 minutes, which is 1 minute and 0.3953488376 * 60 = 23.72093026 seconds, so approximately 1 minute 23.7 seconds.\n",
      "\n",
      "But the question is asking for the time in minutes, so I need to convert 1 / 43 hours to minutes.\n",
      "\n",
      "Since 1 hour is 60 minutes, 1 / 43 hours is 60 / 43 minutes, which is approximately 1.395 minutes, or 1 minute and about 24 seconds.\n",
      "\n",
      "But the problem is probably expecting an exact value, not an approximate. So, 60 / 43 minutes is the exact time.\n",
      "\n",
      "But 60 / 43 is equal to 1 and 17/43 minutes, since 43 * 1 = 43, 60 - 43 = 17.\n",
      "\n",
      "So, 17/43 minutes is the fractional part.\n",
      "\n",
      "But the question is asking for the time in minutes, so I can leave it as 60 / 43 minutes, or convert it to minutes and seconds.\n",
      "\n",
      "But 60 / 43 is approximately 1.395 minutes, which is 1 minute and about 23.7 seconds. Since the question doesn't specify, but it says to put it in minutes, maybe as a fraction.\n",
      "\n",
      "But let me double-check the initial methods.\n",
      "\n",
      "Method 1: Relative speed is 5 mph, time is 1 / 5 hours, which is 12 minutes.\n",
      "\n",
      "Method 2: Position functions, time is 1 / 43 hours, which is 60 / 43 minutes, which is approximately 1.395 minutes.\n",
      "\n",
      "Method 3: Distance covered by lion minus distance covered by elephant equals initial distance.\n",
      "\n",
      "In t hours, lion covers 24t, elephant covers 19t, so 24t - 19t = 5t = 1, so t = 1 / 5 hours, which is 12 minutes.\n",
      "\n",
      "Wait, hold on, this is conflicting with the position function.\n",
      "\n",
      "Wait, in the position function, I set the lion's position as 24t and the elephant's position as 1 - 19t, and set them equal, getting t = 1 / 43 hours.\n",
      "\n",
      "But in the third method, I thought of the lion moving away from the elephant at 24 - 19 = 5 mph, so time is 1 / 5 hours.\n",
      "\n",
      "But that seems incorrect because the elephant is moving away, so the relative speed is actually 24 + 19\n",
      "4096 800 4496 2797\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "2957 800 3357 3845\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "3396 800 3796 3024\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "620 800 1020 1765\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1203 800 1603 1870\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1216 800 1616 2777\n",
      "===\n",
      "===\n",
      "===\n",
      "WRONG \\text{odd}\n",
      "6} = -\\frac{4}{6} = -\\frac{2}{3}.\n",
      "\n",
      "Compare \\( f(-1) \\) with \\( -f(1) \\):\n",
      "\n",
      "\\( -f(1) = -\\frac{2}{3} \\), which is equal to \\( f(-1) \\). So, it checks out for \\( x = 1 \\).\n",
      "\n",
      "Let me try another value, say \\( x = 2 \\):\n",
      "\n",
      "Compute \\( f(2) = \\frac{5^2 - 1}{5^2 + 1} = \\frac{25 - 1}{25 + 1} = \\frac{24}{26} = \\frac{12}{13} \\).\n",
      "\n",
      "Compute \\( f(-2) = \\frac{5^{-2} - 1}{5^{-2} + 1} = \\frac{\\frac{1}{25} - 1}{\\frac{1}{25} + 1} = \\frac{-\\frac{24}{25}}{\\frac{26}{25}} = -\\frac{24}{26} = -\\frac{12}{13} \\).\n",
      "\n",
      "Which is equal to \\( -f(2) \\), since \\( f(2) = \\frac{12}{13} \\). So, again, it holds.\n",
      "\n",
      "Maybe I should test a negative exponent to see if the pattern continues. Let's try \\( x = -1 \\):\n",
      "\n",
      "Wait, I already did \\( x = -1 \\). Let me try \\( x = 0 \\):\n",
      "\n",
      "Compute \\( f(0) = \\frac{5^0 - 1}{5^0 + 1} = \\frac{1 - 1}{1 + 1} = \\frac{0}{2} = 0 \\).\n",
      "\n",
      "Compute \\( f(-0) = f(0) = 0 \\). So, that's consistent.\n",
      "\n",
      "Compute \\( -f(0) = -0 = 0 \\), which equals \\( f(-0) \\). So, that's okay.\n",
      "\n",
      "Let me try \\( x = \\frac{1}{2} \\):\n",
      "\n",
      "Compute \\( f(\\frac{1}{2}) = \\frac{5^{1/2} - 1}{5^{1/2} + 1} = \\frac{\\sqrt{5} - 1}{\\sqrt{5} + 1} \\).\n",
      "\n",
      "Compute \\( f(-\\frac{1}{2}) = \\frac{5^{-1/2} - 1}{5^{-1/2} + 1} = \\frac{\\frac{1}{\\sqrt{5}} - 1}{\\frac{1}{\\sqrt{5}} + 1} \\).\n",
      "\n",
      "Let me rationalize the numerator and denominator of \\( f(-\\frac{1}{2}) \\):\n",
      "\n",
      "Multiply numerator and denominator by \\( \\sqrt{5} \\):\n",
      "\n",
      "Numerator: \\( 1 - \\sqrt{5} \\).\n",
      "\n",
      "Denominator: \\( 1 + \\sqrt{5} \\).\n",
      "\n",
      "So, \\( f(-\\frac{1}{2}) = \\frac{1 - \\sqrt{5}}{1 + \\sqrt{5}} \\).\n",
      "\n",
      "Now, let's rationalize this expression by multiplying numerator and denominator by \\( 1 - \\sqrt{5} \\):\n",
      "\n",
      "Numerator: \\( (1 - \\sqrt{5})(1 - \\sqrt{5}) = 1 - 2\\sqrt{5} + 5 = 6 - 2\\sqrt{5} \\).\n",
      "\n",
      "Denominator: \\( (1 + \\sqrt{5})(1 - \\sqrt{5}) = 1 - 5 = -4 \\).\n",
      "\n",
      "So, \\( f(-\\frac{1}{2}) = \\frac{6 - 2\\sqrt{5}}{-4} = \\frac{-6 + 2\\sqrt{5}}{4} = \\frac{-3 + \\sqrt{5}}{2} \\).\n",
      "\n",
      "Now, let's compute \\( -f(\\frac{1}{2}) \\):\n",
      "\n",
      "\\( -f(\\frac{1}{2}) = - \\times \\frac{\\sqrt{5} - 1}{\\sqrt{5} + 1} = \\frac{1 - \\sqrt{5}}{\\sqrt{5} + 1} \\).\n",
      "\n",
      "Wait, that's the same as \\( f(-\\frac{1}{2}) \\). So, \\( f(-\\frac{1}{2}) = -f(\\frac{1}{2}) \\), which confirms the odd function property.\n",
      "\n",
      "Therefore, after computing \\( f(-x) \\) and simplifying, it's clear that \\( f(-x) = -f(x) \\), so the function is odd.\n",
      "\n",
      "I think that's thorough enough. I can't find any mistakes in my computations, and the examples I tested satisfy the condition for an odd function. So, I'm confident that the function is odd.\n",
      "\n",
      "**Final Answer**\n",
      "The function is \\boxed{odd}.\n",
      "</think>\n",
      "\n",
      "To determine whether the function \\( f(x) = \\frac{5^x - 1}{5^x + 1} \\) is even, odd, or neither, we need to check the conditions for even and odd functions. An even function satisfies \\( f(-x) = f(x) \\) for all \\( x \\), and an odd function satisfies \\( f(-x) = -f(x) \\).\n",
      "\n",
      "First, we compute \\( f(-x) \\):\n",
      "\n",
      "\\[\n",
      "f(-x) = \\frac{5^{-x} - 1}{5^{-x} + 1}\n",
      "\\]\n",
      "\n",
      "Since \\( 5^{-x} = \\frac{1}{5^x} \\), we substitute this into the expression:\n",
      "\n",
      "\\[\n",
      "f(-x) = \\frac{\\frac{1}{5^x} - 1}{\\frac{1}{5^x} + 1}\n",
      "\\]\n",
      "\n",
      "Next, we simplify by multiplying the numerator and the denominator by \\( 5^x \\):\n",
      "\n",
      "\\[\n",
      "f(-x) = \\frac{\\left( \\frac{1}{5^x} - 1 \\right) \\cdot 5^x}{\\left( \\frac{1}{5^x} + 1 \\right) \\cdot 5^x} = \\frac{1 - 5^x}{1 + 5^x}\n",
      "\\]\n",
      "\n",
      "This simplifies to:\n",
      "\n",
      "\\[\n",
      "f(-x) = -\\frac{5^x - 1}{5^x + 1} = -f(x)\n",
      "\\]\n",
      "\n",
      "Thus, we have shown that \\( f(-x) = -f(x) \\), which means the function is odd.\n",
      "\n",
      "To confirm, we tested specific values of \\( x \\) and found that the function satisfies the condition for odd functions. Therefore, the function is odd.\n",
      "\n",
      "\\[\n",
      "\\boxed{odd}\n",
      "\\]<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "1441 800 1841 2310\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "914 800 1314 1614\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1874 800 2274 2299\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1784 800 2184 1780\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1022 800 1422 1501\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "704 800 1104 2213\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1808 800 2208 2935\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1309 800 1709 3175\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "350 800 750 1158\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1226 800 1626 1181\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "2325 800 2725 2669\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "2454 800 2854 2217\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "2526 800 2926 2824\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "327 800 727 1136\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1312 800 1712 2187\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "706 800 1106 1456\n",
      "===\n",
      "===\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    WINDOW_SIZE,\n",
    "    VISION_START,\n",
    "    VISION_END,\n",
    "    EOS_TOKEN_ID,\n",
    ")\n",
    "import torch\n",
    "import json\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "correct_items = 0\n",
    "torch.manual_seed(0)\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "evaluation_dataset = []\n",
    "\n",
    "for dataset_pos in tqdm(range(len(correct_dataset))):\n",
    "    # dataset_pos = 11\n",
    "    input_ids = correct_dataset[dataset_pos][\"problem\"]\n",
    "    # print(input_ids)\n",
    "    # print(\"===\")\n",
    "    # print(\"===\")\n",
    "\n",
    "    input_ids = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": base_prompt.format(question=input_ids),\n",
    "                },\n",
    "            ],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # input_ids = dataset_item[\"input_ids\"]\n",
    "    # generated_ids = dataset_item[\"generated_ids\"]\n",
    "    generated_ids = [\n",
    "        tokenizer.encode(\n",
    "            correct_dataset[dataset_pos][\"model_answer\"],\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "            torch.tensor([[VISION_START]], device=\"cuda\")\n",
    "        )\n",
    "        end_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "            torch.tensor([[VISION_END]], device=\"cuda\")\n",
    "        )\n",
    "        input_ids = torch.tensor(input_ids).cuda()\n",
    "        input_ids_embeds = model.get_input_embeddings()(input_ids)\n",
    "        # windows_amount = 100\n",
    "        # windows_amount = 200\n",
    "        # windows_amount = 300\n",
    "        windows_amount = 400\n",
    "        # windows_amount = 500\n",
    "        # windows_amount = 2\n",
    "        generated_tokens_amount = WINDOW_SIZE * windows_amount\n",
    "        original_total_len = torch.tensor(generated_ids).shape[1]\n",
    "        if generated_tokens_amount > original_total_len:\n",
    "            windows_amount = original_total_len // WINDOW_SIZE\n",
    "            generated_tokens_amount = WINDOW_SIZE * windows_amount\n",
    "\n",
    "        next_true_tokens = torch.tensor(generated_ids, device=\"cuda\")[\n",
    "            :, :generated_tokens_amount\n",
    "        ]\n",
    "\n",
    "        # next_true_tokens = torch.tensor(next_true_tokens, device=\"cuda\")\n",
    "\n",
    "        original_embeds = (model.get_input_embeddings()(next_true_tokens)).to(\n",
    "            torch.bfloat16\n",
    "        )\n",
    "\n",
    "        # compressed_part = model.base_model.embed_pooler(new_embeds_for_compression)\n",
    "        new_embeds_for_compression = original_embeds.reshape(\n",
    "            windows_amount, WINDOW_SIZE, -1\n",
    "        )\n",
    "        compressed_part = model.base_model.embed_pooler(new_embeds_for_compression)\n",
    "        compressed_part = compressed_part.reshape(1, windows_amount, -1)\n",
    "        # start_embed = torch.rand_like(start_embed)\n",
    "        # compressed_part = torch.rand_like(compressed_part)\n",
    "        # end_embed = torch.rand_like(end_embed)\n",
    "        generated_embeds = torch.cat(\n",
    "            [\n",
    "                input_ids_embeds,\n",
    "                start_embed,\n",
    "                compressed_part,\n",
    "                end_embed,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(\"COMPRESSED PART\", tokenizer.decode(next_true_tokens[-1]))\n",
    "        # print(\"===\")\n",
    "        generated_ids_compressed = model.generate(\n",
    "            inputs_embeds=generated_embeds,\n",
    "            # attention_mask=torch.ones_like(generated_embeds),\n",
    "            attention_mask=torch.ones(\n",
    "                generated_embeds.shape[:2],\n",
    "                device=\"cuda\",\n",
    "            ).long(),\n",
    "            max_new_tokens=4096,\n",
    "            # max_new_tokens=5,\n",
    "            do_sample=False,\n",
    "            # do_sample=True,\n",
    "            # temperature=0.6,\n",
    "            # top_p=0.95,\n",
    "        )\n",
    "        # break\n",
    "    generated_result = tokenizer.decode(generated_ids_compressed[-1])\n",
    "    # print()\n",
    "    gold_answer = correct_dataset[dataset_pos][\"answer\"]\n",
    "    answer = dataset_answer_filter(gold_answer)\n",
    "    model_answer = model_answer_filter(generated_result)\n",
    "    if is_equiv(answer, model_answer):\n",
    "        correct_items += 1\n",
    "        print(\"CORRECT\")\n",
    "    else:\n",
    "        print(\"WRONG\", gold_answer)\n",
    "        print(generated_result)\n",
    "    compressed_total_len = generated_ids_compressed.shape[1] + compressed_part.shape[1]\n",
    "    print(\n",
    "        generated_ids_compressed.shape[1],\n",
    "        next_true_tokens.shape[1],\n",
    "        compressed_total_len,\n",
    "        torch.tensor(generated_ids).shape[1],\n",
    "    )\n",
    "    evaluation_dataset.append(\n",
    "        {\n",
    "            **correct_dataset[dataset_pos],\n",
    "            \"compressed_input_part\": tokenizer.decode(next_true_tokens[-1]),\n",
    "            \"compressed_output_generation\": generated_result,\n",
    "            \"compressed_compression_size\": generated_tokens_amount,\n",
    "            \"original_total_len\": torch.tensor(generated_ids).shape[1],\n",
    "            \"compressed_total_len\": compressed_total_len,\n",
    "        }\n",
    "    )\n",
    "    print(\"===\")\n",
    "    print(\"===\")\n",
    "    print(\"===\")\n",
    "    # break\n",
    "    # embeds_generation_tokens = generated_tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 656])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_embeds.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(\n",
    "    generated_embeds.shape[:2],\n",
    "    device=\"cuda\",\n",
    ").long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 719, 1536])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(next_true_tokens[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8908045977011494, 0.7816091954022989)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_dataset) / len(dataset), correct_items / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.base_model_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Below is a magic square, meaning that the sum of the numbers in each row, in each column, and in each of the $2$ main diagonals are equal. What is the value of $n$?\\n\\n[asy]size(125);\\nfor(int i = 0; i<4; ++i)\\n{\\n\\ndraw((0,i)--(3,i),linewidth(1));\\n}\\n\\nfor(int j = 0; j<4; ++j)\\n{\\n\\ndraw((j,0)--(j,3),linewidth(1));\\n}\\n\\nlabel(\"$n-3$\",(.5,.5));\\nlabel(\"3\",(.5,1.5));\\nlabel(\"$n+1$\",(.5,2.5));\\n\\nlabel(\"$n+2$\",(1.5,.5));\\nlabel(\"$2n-9$\",(1.5,1.5));\\nlabel(\"$1$\",(1.5,2.5));\\n\\nlabel(\"$2$\",(2.5,.5));\\nlabel(\"$n$\",(2.5,1.5));\\nlabel(\"$n-1$\",(2.5,2.5));\\n[/asy]',\n",
       " 'solution': 'First, we can evaluate the sum across the first row, which gives $(n+1)+1+(n-1)=2n+1$.  Evaluate the sum of the entries across the second row, $3+(2n-9)+n=3n-6$. Now, since we have a magic square, these two sums are equal.  So $2n+1=3n-6$. Isolating $n$, we obtain $n = \\\\boxed{7}$.\\n\\nThe square will look like: [asy] size(2cm);\\ndraw((0,0)--(3,0)--(3,3)--(0,3)--cycle,linewidth(1));\\ndraw((1,0)--(1,3),linewidth(1));\\ndraw((2,0)--(2,3),linewidth(1));\\ndraw((0,1)--(3,1),linewidth(1));\\ndraw((0,2)--(3,2),linewidth(1));\\nlabel(\"8\",(.5,2.5));\\nlabel(\"1\",(1.5,2.5));\\nlabel(\"6\",(2.5,2.5));\\nlabel(\"3\",(.5,1.5));\\nlabel(\"5\",(1.5,1.5));\\nlabel(\"7\",(2.5,1.5));\\nlabel(\"4\",(.5,.5));\\nlabel(\"9\",(1.5,.5));\\nlabel(\"2\",(2.5,.5));\\n[/asy]',\n",
       " 'answer': '7',\n",
       " 'subject': 'Prealgebra',\n",
       " 'level': 5,\n",
       " 'unique_id': 'test/prealgebra/1930.json',\n",
       " 'model_answer': \"Okay, so I have this magic square problem here, and I need to find the value of \\\\( n \\\\). A magic square is one where the sums of the numbers in each row, each column, and both main diagonals are all equal. The Asymptote code is provided, which I can visualize. Let me try to reconstruct the magic square based on the labels given.\\n\\nFirst, I'll try to draw it out mentally. The Asymptote code is creating a 3x3 grid. Each cell is labeled with some expression involving \\\\( n \\\\). Let me write down the positions and the expressions.\\n\\nLooking at the Asymptote code:\\n\\n- The first row (top row) is labeled at positions (0.5, 0.5), (0.5, 1.5), and (0.5, 2.5), so that's the top row. The labels are:\\n  - \\\\( n - 3 \\\\) at the left,\\n  - \\\\( 3 \\\\) in the center,\\n  - \\\\( n + 1 \\\\) at the right.\\n  \\nSo, the top row is: \\\\( n - 3 \\\\), \\\\( 3 \\\\), \\\\( n + 1 \\\\).\\n\\n- The second row (middle row) is labeled at positions (1.5, 0.5), (1.5, 1.5), and (1.5, 2.5). The labels are:\\n  - \\\\( n + 2 \\\\) on the left,\\n  - \\\\( 2n - 9 \\\\) in the center,\\n  - \\\\( 1 \\\\) on the right.\\n  \\nSo, the middle row is: \\\\( n + 2 \\\\), \\\\( 2n - 9 \\\\), \\\\( 1 \\\\).\\n\\n- The third row (bottom row) is labeled at positions (2.5, 0.5), (2.5, 1.5), and (2.5, 2.5). The labels are:\\n  - \\\\( 2 \\\\) on the left,\\n  - \\\\( n \\\\) in the center,\\n  - \\\\( n - 1 \\\\) on the right.\\n  \\nSo, the bottom row is: \\\\( 2 \\\\), \\\\( n \\\\), \\\\( n - 1 \\\\).\\n\\nLet me write this down as a table for clarity:\\n\\n\\\\[\\n\\\\begin{array}{|c|c|c|}\\n\\\\hline\\nn - 3 & 3 & n + 1 \\\\\\\\\\n\\\\hline\\nn + 2 & 2n - 9 & 1 \\\\\\\\\\n\\\\hline\\n2 & n & n - 1 \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n\\\\]\\n\\nAlright, so now I need to find \\\\( n \\\\) such that the sums of each row, column, and both main diagonals are equal. Let's denote the magic constant (the common sum) as \\\\( S \\\\). So, I need to express \\\\( S \\\\) in terms of \\\\( n \\\\) and set up equations accordingly.\\n\\nFirst, let me compute the sum of the first row:\\n\\n\\\\( (n - 3) + 3 + (n + 1) \\\\)\\n\\nSimplify that:\\n\\n\\\\( n - 3 + 3 + n + 1 = 2n + 1 \\\\)\\n\\nSo, the sum of the first row is \\\\( 2n + 1 \\\\). That gives me one equation.\\n\\nNext, let me check the sum of the second row:\\n\\n\\\\( (n + 2) + (2n - 9) + 1 \\\\)\\n\\nSimplify:\\n\\n\\\\( n + 2 + 2n - 9 + 1 = 3n - 6 \\\\)\\n\\nSo, the sum of the second row is \\\\( 3n - 6 \\\\). Since it's a magic square, this should also equal \\\\( S \\\\). Therefore, I have:\\n\\n\\\\( 2n + 1 = 3n - 6 \\\\)\\n\\nLet me solve this equation for \\\\( n \\\\):\\n\\nSubtract \\\\( 2n \\\\) from both sides:\\n\\n\\\\( 1 = n - 6 \\\\)\\n\\nAdd 6 to both sides:\\n\\n\\\\( 7 = n \\\\)\\n\\nSo, \\\\( n = 7 \\\\). Hmm, let me check if this is consistent with the other rows, columns, and diagonals.\\n\\nLet's compute the third row:\\n\\n\\\\( 2 + 7 + (7 - 1) = 2 + 7 + 6 = 15 \\\\)\\n\\nSo, the sum is 15. Let's verify if this holds with the columns and diagonals.\\n\\nFirst, the first column:\\n\\n\\\\( (n - 3) + (n + 2) + 2 \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (7 - 3) + (7 + 2) + 2 = 4 + 9 + 2 = 15 \\\\)\\n\\nGood, that's 15.\\n\\nSecond column:\\n\\n\\\\( 3 + (2n - 9) + n \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( 3 + (14 - 9) + 7 = 3 + 5 + 7 = 15 \\\\)\\n\\nAlso 15.\\n\\nThird column:\\n\\n\\\\( (n + 1) + 1 + (n - 1) \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (8) + 1 + 6 = 15 \\\\)\\n\\nGood.\\n\\nNow, the main diagonals.\\n\\nFirst diagonal (top-left to bottom-right):\\n\\n\\\\( (n - 3) + (2n - 9) + (n - 1) \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (4) + (14 - 9) + 6 = 4 + 5 + 6 = 15 \\\\)\\n\\nSecond diagonal (top-right to bottom-left):\\n\\n\\\\( (n + 1) + (2n - 9) + 2 \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (8) + (14 - 9) + 2 = 8 + 5 + 2 = 15 \\\\)\\n\\nPerfect, both diagonals sum to 15.\\n\\nSo, all rows, columns, and diagonals sum to 15 when \\\\( n = 7 \\\\).\\n\\nWait, but let me double-check the initial row sum. When \\\\( n = 7 \\\\), the first row sum was \\\\( 2n + 1 = 14 + 1 = 15 \\\\). Yep, that's consistent.\\n\\nSo, it seems that \\\\( n = 7 \\\\) is the correct value.\\n\\nBut just to be thorough, let me check the second row sum when \\\\( n = 7 \\\\):\\n\\nSecond row: \\\\( (7 + 2) + (14 - 9) + 1 = 9 + 5 + 1 = 15 \\\\). Yep, that's correct.\\n\\nSimilarly, the third row was 2 + 7 + 6 = 15.\\n\\nAll columns, diagonals, and rows check out.\\n\\nSo, I think \\\\( n = 7 \\\\) is the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( n \\\\) is \\\\boxed{7}.\\n</think>\\n\\nGiven a magic square where the sum of the numbers in each row, column, and the two main diagonals are equal, we need to find the value of \\\\( n \\\\).\\n\\nThe magic square is reconstructed as follows:\\n\\n\\\\[\\n\\\\begin{array}{|c|c|c|}\\n\\\\hline\\nn - 3 & 3 & n + 1 \\\\\\\\\\n\\\\hline\\nn + 2 & 2n - 9 & 1 \\\\\\\\\\n\\\\hline\\n2 & n & n - 1 \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n\\\\]\\n\\nWe denote the magic constant (the common sum) as \\\\( S \\\\).\\n\\n1. Calculate the sum of the first row:\\n   \\\\[\\n   (n - 3) + 3 + (n + 1) = 2n + 1\\n   \\\\]\\n   This gives us the equation:\\n   \\\\[\\n   2n + 1 = S\\n   \\\\]\\n\\n2. Calculate the sum of the second row:\\n   \\\\[\\n   (n + 2) + (2n - 9) + 1 = 3n - 6\\n   \\\\]\\n   This gives us the equation:\\n   \\\\[\\n   3n - 6 = S\\n   \\\\]\\n\\n3. Equate the two expressions for \\\\( S \\\\):\\n   \\\\[\\n   2n + 1 = 3n - 6\\n   \\\\]\\n   Solving for \\\\( n \\\\):\\n   \\\\[\\n   1 = n - 6 \\\\implies n = 7\\n   \\\\]\\n\\n4. Verify the value of \\\\( n \\\\) by checking the sums of the third row, columns, and diagonals:\\n   - Third row: \\\\( 2 + 7 + 6 = 15 \\\\)\\n   - First column: \\\\( 4 + 9 + 2 = 15 \\\\)\\n   - Second column: \\\\( 3 + 5 + 7 = 15 \\\\)\\n   - Third column: \\\\( 8 + 1 + 6 = 15 \\\\)\\n   - First diagonal: \\\\( 4 + 5 + 6 = 15 \\\\)\\n   - Second diagonal: \\\\( 8 + 5 + 2 = 15 \\\\)\\n\\nAll sums are consistent and equal to 15 when \\\\( n = 7 \\\\).\\n\\nThus, the value of \\\\( n \\\\) is \\\\(\\\\boxed{7}\\\\).\",\n",
       " 'compressed_input_part': \"Okay, so I have this magic square problem here, and I need to find the value of \\\\( n \\\\). A magic square is one where the sums of the numbers in each row, each column, and both main diagonals are all equal. The Asymptote code is provided, which I can visualize. Let me try to reconstruct the magic square based on the labels given.\\n\\nFirst, I'll try to draw it out mentally. The Asymptote code is creating a 3x3 grid. Each cell is labeled with some expression involving \\\\( n \\\\). Let me write down the positions and the expressions.\\n\\nLooking at the Asymptote code:\\n\\n- The first row (top row) is labeled at positions (0.5, 0.5), (0.5, 1.5), and (0.5, 2.5), so that's the top row. The labels are:\\n  - \\\\( n - 3 \\\\) at the left,\\n  - \\\\( 3 \\\\) in the center,\\n  - \\\\( n + 1 \\\\) at the right.\\n  \\nSo, the top row is: \\\\( n - 3 \\\\), \\\\( 3 \\\\), \\\\( n + 1 \\\\).\\n\\n- The second row (middle row) is labeled at positions (1.5, 0.5), (1.5, 1.5), and (1.5, 2.5). The labels are:\\n  - \\\\( n + 2 \\\\) on the left,\\n  - \\\\( 2n - 9 \\\\) in the center,\\n  - \\\\( 1 \\\\) on the right.\\n  \\nSo, the middle row is: \\\\( n + 2 \\\\), \\\\( 2n - 9 \\\\), \\\\( 1 \\\\).\\n\\n- The third row (bottom row) is labeled at positions (2.5, 0.5), (2.5, 1.5),\",\n",
       " 'compressed_output_generation': \" (2.5, 2.5). The labels are:\\n  - \\\\( 2 \\\\) on the left,\\n  - \\\\( n \\\\) in the center,\\n  - \\\\( n - 1 \\\\) on the right.\\n\\nSo, the bottom row is: \\\\( 2 \\\\), \\\\( n \\\\), \\\\( n - 1 \\\\).\\n\\nNow, let me summarize all the labels:\\n\\n- Row 1 (Top): \\\\( n - 3 \\\\), \\\\( 3 \\\\), \\\\( n + 1 \\\\)\\n- Row 2 (Middle): \\\\( n + 2 \\\\), \\\\( 2n - 9 \\\\), \\\\( 1 \\\\)\\n- Row 3 (Bottom): \\\\( 2 \\\\), \\\\( n \\\\), \\\\( n - 1 \\\\)\\n\\nI can calculate the sums of the rows and set them equal to each other since it's a magic square.\\n\\nLet me compute the sum of Row 1: \\\\( (n - 3) + 3 + (n + 1) \\\\). Simplify that: \\\\( n - 3 + 3 + n + 1 = 2n + 1 \\\\).\\n\\nSum of Row 2: \\\\( (n + 2) + (2n - 9) + 1 \\\\). Simplify: \\\\( n + 2 + 2n - 9 + 1 = 3n - 6 \\\\).\\n\\nSum of Row 3: \\\\( 2 + n + (n - 1) \\\\). Simplify: \\\\( 2 + n + n - 1 = 2n + 1 \\\\).\\n\\nSo, Row 1 and Row 3 both sum to \\\\( 2n + 1 \\\\). Row 2 sums to \\\\( 3n - 6 \\\\). Since it's a magic square, all rows must sum to the same value. Therefore, I can set Row 1 equal to Row 2:\\n\\n\\\\( 2n + 1 = 3n - 6 \\\\)\\n\\nSolving for \\\\( n \\\\):\\n\\nSubtract \\\\( 2n \\\\) from both sides: \\\\( 1 = n - 6 \\\\)\\n\\nAdd 6 to both sides: \\\\( n = 7 \\\\)\\n\\nWait, is that it? Let me check if this value of \\\\( n \\\\) works for the columns as well.\\n\\nLet me compute the sums of the columns to verify.\\n\\nFirst, label the magic square based on \\\\( n = 7 \\\\):\\n\\n- Row 1: \\\\( 7 - 3 = 4 \\\\), \\\\( 3 \\\\), \\\\( 7 + 1 = 8 \\\\)\\n- Row 2: \\\\( 7 + 2 = 9 \\\\), \\\\( 2*7 - 9 = 14 - 9 = 5 \\\\), \\\\( 1 \\\\)\\n- Row 3: \\\\( 2 \\\\), \\\\( 7 \\\\), \\\\( 7 - 1 = 6 \\\\)\\n\\nSo, the matrix is:\\n\\n4 | 3 | 8\\n\\n9 | 5 | 1\\n\\n2 | 7 | 6\\n\\nNow, let's check the columns:\\n\\n- Column 1: 4 + 9 + 2 = 15\\n- Column 2: 3 + 5 + 7 = 15\\n- Column 3: 8 + 1 + 6 = 15\\n\\nGood, all columns sum to 15.\\n\\nNow, let's check the diagonals.\\n\\nMain diagonal (top-left to bottom-right): 4, 5, 6. Sum: 4 + 5 + 6 = 15.\\n\\nOther diagonal (top-right to bottom-left): 8, 5, 2. Sum: 8 + 5 + 2 = 15.\\n\\nPerfect, all rows, columns, and diagonals sum to 15 when \\\\( n = 7 \\\\).\\n\\nTherefore, the value of \\\\( n \\\\) is 7.\\n\\n**Final Answer**\\nThe value of \\\\( n \\\\) is \\\\boxed{7}.\\n</think>\\n\\nTo find the value of \\\\( n \\\\) in the given magic square, we start by noting the labels of each cell based on the Asymptote code. The matrix is reconstructed as follows:\\n\\n- Row 1 (Top): \\\\( n - 3 \\\\), \\\\( 3 \\\\), \\\\( n + 1 \\\\)\\n- Row 2 (Middle): \\\\( n + 2 \\\\), \\\\( 2n - 9 \\\\), \\\\( 1 \\\\)\\n- Row 3 (Bottom): \\\\( 2 \\\\), \\\\( n \\\\), \\\\( n - 1 \\\\)\\n\\nWe calculate the sums of the rows:\\n\\n- Sum of Row 1: \\\\( (n - 3) + 3 + (n + 1) = 2n + 1 \\\\)\\n- Sum of Row 2: \\\\( (n + 2) + (2n - 9) + 1 = 3n - 6 \\\\)\\n- Sum of Row 3: \\\\( 2 + n + (n - 1) = 2n + 1 \\\\)\\n\\nSince it is a magic square, all rows must sum to the same value. Setting the sum of Row 1 equal to Row 2:\\n\\n\\\\[ 2n + 1 = 3n - 6 \\\\]\\n\\nSolving for \\\\( n \\\\):\\n\\n\\\\[ 1 = n - 6 \\\\]\\n\\\\[ n = 7 \\\\]\\n\\nTo verify, we check the columns and diagonals with \\\\( n = 7 \\\\):\\n\\n- The matrix becomes:\\n  \\\\[\\n  \\\\begin{array}{ccc}\\n  4 & 3 & 8 \\\\\\\\\\n  9 & 5 & 1 \\\\\\\\\\n  2 & 7 & 6 \\\\\\\\\\n  \\\\end{array}\\n  \\\\]\\n\\n- Column sums: 4 + 9 + 2 = 15, 3 + 5 + 7 = 15, 8 + 1 + 6 = 15\\n- Diagonals: 4 + 5 + 6 = 15, 8 + 5 + 2 = 15\\n\\nAll sums are equal to 15, confirming the magic square. Therefore, the value of \\\\( n \\\\) is \\\\(\\\\boxed{7}\\\\).<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\",\n",
       " 'compressed_compression_size': 400,\n",
       " 'original_total_len': 2019,\n",
       " 'compressed_total_len': 1502}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "base_path = \"hidden_capacity_reasoning/evaluation/math_500/evals/compression_tests\"\n",
    "save_id = f\"test_correct_dataset_{len(correct_dataset)}_compressed_window_amount=400,window=2__2025_04_22_01_43_43_347583__10000\"\n",
    "with open(f\"{base_path}/{save_id}.json\", \"w\") as f:\n",
    "    json.dump(evaluation_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\n",
    "    f\"hidden_capacity_reasoning/evaluation/math_500/evals/compression_tests/correct_dataset_155_compressed_window_amount=400,window=2__2025_04_22_01_43_43_347583__90000.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    evaluation_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350015, 338450, 0.9669585589189035)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_total_len = 0\n",
    "compressed_total_len = 0\n",
    "for item in evaluation_dataset:\n",
    "    original_total_len += item[\"original_total_len\"]\n",
    "    compressed_total_len += item[\"compressed_total_len\"]\n",
    "original_total_len, compressed_total_len, compressed_total_len / original_total_len"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
