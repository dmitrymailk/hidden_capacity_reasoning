{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ef7a7bb00246348e4b40a104ceaa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f085db131e654d64964a7040ecfed546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/312 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/datasets/dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B/resolve/82cf73531b7b8daaee327e4813aa774abbc0fcc4/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B.py",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/load.py:1580\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1580\u001b[0m     dataset_script_path \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _require_custom_configs \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/hf_api.py:5475\u001b[0m, in \u001b[0;36mHfApi.hf_hub_download\u001b[0;34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   5473\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m-> 5475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5478\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5487\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5491\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:961\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1024\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1024\u001b[0m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1484\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1401\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:285\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 285\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    308\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 309\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:420\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    419\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(EntryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-68128578-0ea274e125dba3cb1d1ac5b6;c8d72666-fbda-4390-b0c3-c9d217194594)\n\nEntry Not Found for url: https://huggingface.co/datasets/dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B/resolve/82cf73531b7b8daaee327e4813aa774abbc0fcc4/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B.py.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 89\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# model = model.to(torch.bfloat16)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# temp_model = Qwen2ModelEmbedPoolerV3.from_pretrained(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# gc.collect()\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     91\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/load.py:2062\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2057\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2058\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2059\u001b[0m )\n\u001b[1;32m   2061\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2062\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/load.py:1782\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1781\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1782\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/load.py:1629\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1620\u001b[0m         use_exported_dataset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_exported_dataset_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_exported_dataset_infos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1631\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is a gated dataset on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/load.py:1019\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1018\u001b[0m     patterns \u001b[38;5;241m=\u001b[39m get_data_patterns(base_path, download_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config)\n\u001b[0;32m-> 1019\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_ALLOWED_EXTENSIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m module_name, default_builder_kwargs \u001b[38;5;241m=\u001b[39m infer_module_for_data_files(\n\u001b[1;32m   1026\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1027\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1028\u001b[0m     download_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config,\n\u001b[1;32m   1029\u001b[0m )\n\u001b[1;32m   1030\u001b[0m data_files \u001b[38;5;241m=\u001b[39m data_files\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m   1031\u001b[0m     extensions\u001b[38;5;241m=\u001b[39m_MODULE_TO_EXTENSIONS[module_name], file_names\u001b[38;5;241m=\u001b[39m_MODULE_TO_METADATA_FILE_NAMES[module_name]\n\u001b[1;32m   1032\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/data_files.py:690\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    685\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    687\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    688\u001b[0m         patterns_for_key\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[0;32m--> 690\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m     )\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/data_files.py:583\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 583\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m         )\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/datasets/data_files.py:361\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mHF_HUB_VERSION \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# 10 times faster glob with detail=True (ignores costly info like lastCommit)\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     glob_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand_info\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    359\u001b[0m matched_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    360\u001b[0m     filepath \u001b[38;5;28;01mif\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mstartswith(protocol_prefix) \u001b[38;5;28;01melse\u001b[39;00m protocol_prefix \u001b[38;5;241m+\u001b[39m filepath\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filepath, info \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mglob_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mislink\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(filepath))))\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (xbasename(filepath) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m files_to_ignore)\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_inside_unrequested_special_dir(filepath, fs_pattern)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir(filepath, fs_pattern)\n\u001b[1;32m    366\u001b[0m ]  \u001b[38;5;66;03m# ignore .ipynb and __pycache__, but keep /../\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     out \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    369\u001b[0m         filepath\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m matched_paths\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix \u001b[38;5;129;01min\u001b[39;00m allowed_extensions \u001b[38;5;28;01mfor\u001b[39;00m suffix \u001b[38;5;129;01min\u001b[39;00m xbasename(filepath)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    372\u001b[0m     ]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:521\u001b[0m, in \u001b[0;36mHfFileSystem.glob\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    520\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39munresolve()\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/spec.py:609\u001b[0m, in \u001b[0;36mAbstractFileSystem.glob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m         depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m allpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwithdirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m pattern \u001b[38;5;241m=\u001b[39m glob_translate(path \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ends_with_sep \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    612\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:556\u001b[0m, in \u001b[0;36mHfFileSystem.find\u001b[0;34m(self, path, maxdepth, withdirs, detail, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;124;03mList all files below path.\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03m    `Union[List[str], Dict[str, Dict[str, Any]]]`: List of paths or dict of file information.\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxdepth:\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwithdirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwithdirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetail\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m resolved_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[1;32m    560\u001b[0m path \u001b[38;5;241m=\u001b[39m resolved_path\u001b[38;5;241m.\u001b[39munresolve()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/spec.py:500\u001b[0m, in \u001b[0;36mAbstractFileSystem.find\u001b[0;34m(self, path, maxdepth, withdirs, detail, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Add the root directory if withdirs is requested\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# This is needed for posix glob compliance\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m withdirs \u001b[38;5;129;01mand\u001b[39;00m path \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[0;32m--> 500\u001b[0m     out[path] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, dirs, files \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwalk(path, maxdepth, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m withdirs:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:719\u001b[0m, in \u001b[0;36mHfFileSystem.info\u001b[0;34m(self, path, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m     out \u001b[38;5;241m=\u001b[39m out1[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refresh \u001b[38;5;129;01mor\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (expand_info \u001b[38;5;129;01mand\u001b[39;00m out \u001b[38;5;129;01mand\u001b[39;00m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_commit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 719\u001b[0m     paths_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_paths_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths_info:\n\u001b[1;32m    727\u001b[0m         _raise_file_not_found(path, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3354\u001b[0m, in \u001b[0;36mHfApi.get_paths_info\u001b[0;34m(self, repo_id, paths, expand, revision, repo_type, token)\u001b[0m\n\u001b[1;32m   3351\u001b[0m revision \u001b[38;5;241m=\u001b[39m quote(revision, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mDEFAULT_REVISION\n\u001b[1;32m   3352\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_hf_headers(token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m-> 3354\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3355\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrepo_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43ms/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrepo_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/paths-info/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrevision\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m   3357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3358\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m   3363\u001b[0m paths_info \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"hidden_capacity_reasoning\"\n",
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from trl import (\n",
    "    ModelConfig,\n",
    "    ScriptArguments,\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    "    TrlParser,\n",
    "    get_kbit_device_map,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    generate_train_examples,\n",
    "    pad_train_examples,\n",
    "    tokenize_single_turn,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "import types\n",
    "\n",
    "# need for auto SFTTrainer patch(possible increase speed)\n",
    "from unsloth import is_bfloat16_supported\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    EOS_TOKEN_ID,\n",
    "    TEXT_TOKEN_ID,\n",
    "    WINDOW_SIZE,\n",
    "    VISION_START,\n",
    "    VISION_END,\n",
    "    find_all_linear_names_v3,\n",
    ")\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from hidden_capacity_reasoning.models import (\n",
    "    Qwen2ForCausalLMCompressionV1,\n",
    "    Qwen2ModelEmbedPoolerV1,\n",
    "    Qwen2ForCausalLMCompressionV2,\n",
    "    Qwen2ModelEmbedPoolerV2,\n",
    "    Qwen2ForCausalLMCompressionV3,\n",
    "    Qwen2ModelEmbedPoolerV3,\n",
    ")\n",
    "\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_name = \"my_r1_model_v3\"\n",
    "model = Qwen2ForCausalLMCompressionV3.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.model.requires_grad_(False)\n",
    "# model = model.to(torch.bfloat16)\n",
    "\n",
    "# temp_model = Qwen2ModelEmbedPoolerV3.from_pretrained(\n",
    "#     model_name,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map={\"\": 0},\n",
    "#     # quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n",
    "# )\n",
    "# print(\n",
    "#     model.embed_pooler.load_state_dict(\n",
    "#         temp_model.state_dict(),\n",
    "#         strict=False,\n",
    "#     ),\n",
    "# )\n",
    "# temp_model = temp_model.cpu()\n",
    "# del temp_model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "dataset = load_dataset(\"dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=10, seed=42)\n",
    "\n",
    "# test pass\n",
    "tokenize_single_turn(\n",
    "    question=dataset[\"train\"][0][\"question\"],\n",
    "    answer=dataset[\"train\"][0][\"answer\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "train_examples = [\n",
    "    tokenize_single_turn(tokenizer=tokenizer, **item)\n",
    "    for item in tqdm(dataset[\"train\"].to_list()[:3])\n",
    "]\n",
    "\n",
    "prepared_train_examples = []\n",
    "for item in tqdm(train_examples):\n",
    "    for example in generate_train_examples(\n",
    "        dataset_batch=[item],\n",
    "        window_size=WINDOW_SIZE,\n",
    "    ):\n",
    "        prepared_train_examples.append(example)\n",
    "\n",
    "print(\n",
    "    \"max_len\",\n",
    "    max([len(item[\"original_tokens\"]) for item in prepared_train_examples]),\n",
    ")\n",
    "\n",
    "new_dataset = Dataset.from_list(prepared_train_examples)\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    padded_batch = pad_train_examples(\n",
    "        train_examples=batch,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    padded_batch = {\n",
    "        \"replaced_original_tokens\": padded_batch[\"replaced_original_tokens\"][\n",
    "            \"input_ids\"\n",
    "        ],\n",
    "        \"compressed_input_ids\": padded_batch[\"compressed_input_ids\"][\"input_ids\"],\n",
    "        \"original_tokens\": padded_batch[\"original_tokens\"][\"input_ids\"],\n",
    "        \"attention_mask\": padded_batch[\"compressed_input_ids\"][\"attention_mask\"],\n",
    "        \"labels\": padded_batch[\"compressed_input_ids\"][\"input_ids\"],\n",
    "        \"content_compression_mask\": padded_batch[\"content_compression_mask\"][\n",
    "            \"input_ids\"\n",
    "        ],\n",
    "    }\n",
    "    for key in padded_batch.keys():\n",
    "        padded_batch[key] = torch.tensor(padded_batch[key])\n",
    "    skip_ids = [\n",
    "        TEXT_TOKEN_ID,\n",
    "        EOS_TOKEN_ID,\n",
    "        VISION_START,\n",
    "        VISION_END,\n",
    "    ]\n",
    "    for skip_id in skip_ids:\n",
    "        padded_batch[\"labels\"][padded_batch[\"labels\"] == skip_id] = -100\n",
    "    #    \n",
    "    last_index = (padded_batch[\"content_compression_mask\"] == 1).long().nonzero()[-1][1]\n",
    "    padded_batch[\"labels\"][:, :last_index][\n",
    "        padded_batch[\"content_compression_mask\"][:, :last_index] == 1\n",
    "    ] = -100\n",
    "    # print(padded_batch)\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    target_modules=find_all_linear_names_v3(model=model),\n",
    "    modules_to_save=[\n",
    "        \"embed_pooler.model.embed_tokens\",\n",
    "        \"embed_pooler.weight_pooler\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "formatted_date = datetime.fromtimestamp(time.time()).strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "model.embed_pooler = prepare_model_for_kbit_training(model.embed_pooler)\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=new_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=1,  # 90,  # Set this for 1 full training run.\n",
    "        # num_train_epochs=90,  # Set this for 1 full training run.\n",
    "        # max_steps=10000,\n",
    "        learning_rate=1e-4,\n",
    "        bf16=True,\n",
    "        # fp16=model.dtype == torch.float16,\n",
    "        logging_steps=8,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=f\"outputs/{formatted_date}\",\n",
    "        # report_to=\"wandb\",\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "        # gradient_checkpointing=True,\n",
    "        save_steps=10000,\n",
    "        run_name=formatted_date,\n",
    "    ),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'An elephant and a lion are currently 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour.  How many minutes will it take for the lion to catch the elephant?',\n",
       " 'solution': 'Every hour, the lion runs 24 miles while the elephant runs 19.  Thus, the distance between the two animals closes at a rate of 5 miles every hour.  The lion catches the elephant after this distance has closed 1 mile, which takes $\\\\frac{1}{5}$ hours to do, or $\\\\frac{1}{5}\\\\cdot 60 = \\\\boxed{12}$ minutes.',\n",
       " 'answer': '12',\n",
       " 'subject': 'Prealgebra',\n",
       " 'level': 5,\n",
       " 'unique_id': 'test/prealgebra/1820.json',\n",
       " 'model_answer': \"Okay, so I have this problem here where an elephant and a lion are one mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, sounds like a relative speed problem. Let me think about how to approach this.\\n\\nFirst, let me visualize the situation. There's a lion and an elephant, one mile apart. The lion is running towards the elephant, and the elephant is running away from the lion. Since both are moving towards or away from each other, their speeds will affect the time it takes for the lion to catch up.\\n\\nI remember that when two objects are moving towards each other, their relative speed is the sum of their individual speeds. But in this case, the lion is moving towards the elephant, and the elephant is moving away from the lion, so their speeds are in opposite directions. Wait, no, actually, the lion is moving towards the elephant, so their relative speed is the lion's speed minus the elephant's speed, because they are moving in the same direction. Wait, no, hold on. Let me make sure.\\n\\nThe lion is moving towards the elephant, which is moving away. So from the lion's perspective, the elephant is moving away at 19 mph. So the relative speed between the lion and the elephant is the lion's speed minus the elephant's speed because they're moving in the same direction. Wait, no, actually, if both are moving, the relative speed is the difference when moving in the same direction. Wait, maybe I'm getting confused.\\n\\nLet me recall the formula: when two objects are moving towards each other, their relative speed is the sum of their speeds. But when moving in the same direction, it's the difference. But in this case, the lion is chasing the elephant. So if the lion is moving towards the elephant, and the elephant is moving away, their speeds are in the same direction. So the relative speed is lion's speed minus elephant's speed.\\n\\nWait, hold on. Let me think in terms of distance. The initial distance is 1 mile. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So relative to the lion, the elephant is approaching at (24 - 19) mph, which is 5 mph. So the lion is gaining on the elephant at 5 miles per hour.\\n\\nTherefore, the time it takes for the lion to catch the elephant would be the initial distance divided by the relative speed. So 1 mile divided by 5 mph, which is 1/5 hours. To convert that into minutes, since 1 hour is 60 minutes, 1/5 hours is 12 minutes.\\n\\nWait, that seems straightforward, but let me verify. Let me think of it in terms of equations.\\n\\nLets denote the time it takes for the lion to catch the elephant as t hours.\\n\\nIn t hours, the lion will cover a distance of 24t miles.\\n\\nIn the same t hours, the elephant will cover a distance of 19t miles, but since the elephant is moving away, the distance between the lion and the elephant will decrease at a rate of (24 - 19) mph, which is 5 mph.\\n\\nSo the distance between them decreases by 5 miles every hour. Since they start 1 mile apart, the time it takes for the lion to catch the elephant is 1 / 5 hours, which is 12 minutes. So that seems consistent.\\n\\nAlternatively, I can model the positions of the lion and the elephant as functions of time and find when they meet.\\n\\nLets assume the lion starts at position 0, and the elephant starts at position 1 mile.\\n\\nLets let t be the time in hours. Then, the position of the lion after t hours is 0 + 24t = 24t miles.\\n\\nThe position of the elephant after t hours is 1 - 19t miles.\\n\\nThey meet when their positions are equal, so:\\n\\n24t = 1 - 19t\\n\\nAdding 19t to both sides:\\n\\n24t + 19t = 1\\n\\n43t = 1\\n\\nt = 1/43 hours.\\n\\nWait, hold on, this is different. Earlier, I thought it was 1/5 hours, but now I have 1/43 hours. Which one is correct?\\n\\nWait, that's a big difference. Which approach is correct?\\n\\nLet me check my equations again. Position of lion is 24t, position of elephant is 1 - 19t. So when they meet, 24t = 1 - 19t.\\n\\nSo 24t + 19t = 1\\n\\n43t = 1\\n\\nt = 1/43 hours.\\n\\nHmm, so that's approximately 1.395 minutes, which is about 83.7 seconds. But that seems too fast. Wait, but 1/43 of an hour is roughly 1.395 minutes, which is 83.7 seconds, but 1/5 is 12 minutes, which is 720 seconds. That seems like a huge difference.\\n\\nSo there's a discrepancy here. Which one is right?\\n\\nWait, maybe I made a mistake in setting up the equations. Let me think.\\n\\nWait, if the lion is moving towards the elephant, which is moving away, so the distance between them is decreasing at a rate of (24 - 19) mph, which is 5 mph. So the time should be 1 mile / 5 mph = 1/5 hours, which is 12 minutes.\\n\\nBut in the equation above, I set the positions equal, which gives me t = 1/43 hours. So which is correct?\\n\\nWait, no, the position of the lion is 24t, and the position of the elephant is 1 - 19t. So when they meet, 24t = 1 - 19t.\\n\\nWait, that would mean that the lion is moving towards the elephant, and the elephant is moving away, so the distance between them is decreasing. So the correct equation should be 24t = 1 - 19t.\\n\\nBut solving that gives t = 1/43 hours, which is approximately 1.395 minutes, which seems contradictory.\\n\\nWait, that can't be. Because if the lion is moving at 24 mph and the elephant is moving away at 19 mph, the lion is closing the distance at 5 mph.\\n\\nBut in the equation, the lion is moving at 24t, which is 24 miles per hour, but the elephant is moving away at 19 mph, so the relative speed is 24 - 19 = 5 mph.\\n\\nWait, so maybe the correct equation is that the lion is moving towards the elephant at 5 mph, so the time is 1 mile / 5 mph = 1/5 hours, which is 12 minutes.\\n\\nBut then why does the position equation give me a different answer?\\n\\nWait, perhaps I have a sign error in the position equation.\\n\\nLet me think about the coordinate system. Let's say the lion starts at position 0, and the elephant starts at position 1.\\n\\nSo the lion is moving towards the positive direction at 24 mph, so its position at time t is 24t.\\n\\nThe elephant is moving away from the lion, so its position at time t is 1 + 19t.\\n\\nWait, hold on, if the elephant is moving away from the lion, which is at position 0, then yes, the elephant's position is 1 + 19t.\\n\\nBut wait, if the lion is moving towards the elephant, which is at 1 mile, so if the lion is at position 0, and the elephant is at position 1, and the lion is moving towards the elephant, so the distance between them is decreasing.\\n\\nWait, so if the lion is moving towards the elephant, then the elephant is moving away from the lion, so the distance between them is decreasing at a rate of (24 - 19) mph, which is 5 mph.\\n\\nTherefore, the relative speed is 5 mph, so the time is 1 mile / 5 mph = 1/5 hours = 12 minutes.\\n\\nBut if I model the positions, the lion is at 24t, the elephant is at 1 + 19t. Setting them equal:\\n\\n24t = 1 + 19t\\n\\n24t - 19t = 1\\n\\n5t = 1\\n\\nt = 1/5 hours, which is 12 minutes. So that's consistent.\\n\\nWait, so earlier, I thought the position of the elephant was 1 - 19t, but that was incorrect. The correct position of the elephant is 1 + 19t because it's moving away from the lion.\\n\\nSo my initial setup was wrong. I think I confused the direction. So the correct equation is 24t = 1 + 19t, leading to t = 1/5 hours, which is 12 minutes.\\n\\nTherefore, the lion will catch the elephant in 12 minutes.\\n\\nWait, so that contradicts my initial thought that the relative speed is 5 mph. But why was I getting confused earlier?\\n\\nI think the confusion was in the direction of the elephant's movement. If the elephant is moving away from the lion, then the distance between them is increasing at 19 mph, so the relative speed at which the distance is decreasing is 24 - 19 = 5 mph.\\n\\nBut in the position equation, if the lion is moving towards the elephant, and the elephant is moving away, then the distance between them is decreasing at 5 mph. Therefore, the time to close 1 mile is 1 / 5 hours, which is 12 minutes.\\n\\nSo in summary, the correct answer is 12 minutes.\\n\\nBut let me just double-check the equations.\\n\\nLets denote t as the time in hours.\\n\\nPosition of the lion: 24t.\\n\\nPosition of the elephant: 1 + 19t.\\n\\nThey meet when 24t = 1 + 19t.\\n\\n24t - 19t = 1\\n\\n5t = 1\\n\\nt = 1/5 hours = 12 minutes.\\n\\nYes, that seems correct. So the initial mistake was in the position equation for the elephant, thinking it was moving towards, but it's actually moving away. So the correct equation is 24t = 1 + 19t.\\n\\nSo the answer is 12 minutes.\\n\\nI think that makes sense. Let me think about it another way.\\n\\nIf both were moving towards each other, their relative speed would be 24 + 19 = 43 mph, and the time would be 1/43 hours, which is about 1.395 minutes, but that's if they were moving towards each other. But in this case, they are moving away from each other in the same direction, so the relative speed is 24 - 19 = 5 mph, hence 1/5 hours.\\n\\nAlternatively, if I think about the distance between them decreasing at 5 mph, so 1 mile at 5 mph is 1/5 hours, which is 12 minutes.\\n\\nYes, that seems consistent.\\n\\nSo I think my initial setup was wrong because I thought the elephant was moving towards the lion, but it's actually moving away. So the correct answer is 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe problem involves an elephant and a lion that are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. We need to determine how many minutes it will take for the lion to catch the elephant.\\n\\nFirst, we recognize that the lion is moving towards the elephant, and the elephant is moving away. Therefore, the relative speed at which the lion is approaching the elephant is the difference between their speeds: \\\\(24 - 19 = 5\\\\) miles per hour.\\n\\nThe time it takes for the lion to catch the elephant can be calculated by dividing the initial distance between them by their relative speed. The initial distance is 1 mile, and the relative speed is 5 miles per hour. Thus, the time \\\\(t\\\\) in hours is given by:\\n\\n\\\\[\\nt = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = \\\\frac{1}{5} \\\\text{ hours}\\n\\\\]\\n\\nTo convert this time into minutes, we multiply by 60:\\n\\n\\\\[\\n\\\\frac{1}{5} \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes}\\n\\\\]\\n\\nThus, the lion will catch the elephant in \\\\(\\\\boxed{12}\\\\) minutes.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"my_r1_model_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my_r1_model_v3/tokenizer_config.json',\n",
       " 'my_r1_model_v3/special_tokens_map.json',\n",
       " 'my_r1_model_v3/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"my_r1_model_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.base_model.embed_pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1536])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch.bfloat16)\n",
    "model.embed_pooler(\n",
    "    torch.randn(\n",
    "        2,\n",
    "        2,\n",
    "        1536,\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.bfloat16,\n",
    "    )\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embed_pooler.model.embed_tokens.modules_to_save.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.weight_pooler.modules_to_save.default.weight, Requires Gradient: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, Requires Gradient: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.model.embed_pooler.model.embed_tokens.modules_to_save.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.0.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.1.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.2.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.3.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.4.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.5.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.6.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.7.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.8.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.9.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.10.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.11.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.12.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.13.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.14.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.15.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.16.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.17.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.18.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.19.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.20.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.21.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.22.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.23.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.24.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.25.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.26.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.o_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.self_attn.o_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.gate_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.gate_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.up_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.up_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.down_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: base_model.model.embed_pooler.model.layers.27.mlp.down_proj.lora_B.default.weight, Requires Gradient: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in trainer.model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, Requires Gradient: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      " Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9746fee89c0401bb5c4977bc51f5dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from peft import PeftModel\n",
    "from hidden_capacity_reasoning.models import (\n",
    "    Qwen2ForCausalLMCompressionV1,\n",
    "    Qwen2ModelEmbedPoolerV1,\n",
    "    Qwen2ForCausalLMCompressionV2,\n",
    "    Qwen2ModelEmbedPoolerV2,\n",
    ")\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_name = \"r1_compressor_v2\"\n",
    "model = Qwen2ForCausalLMCompressionV2.from_pretrained(\n",
    "    model_name,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    # \"outputs/2025_04_19_17_17_34_493839/checkpoint-210000\",\n",
    "    # \"outputs/2025_04_19_17_17_34_493839/checkpoint-50000\",\n",
    "    # \"outputs/2025_04_21_19_23_11_642509/checkpoint-10000\",\n",
    "    # \"outputs/2025_04_22_01_43_43_347583/checkpoint-90000\",\n",
    "    # \"outputs/2025_04_22_01_43_43_347583/checkpoint-10000\",\n",
    "    # 'outputs/2025_04_23_00_46_10_062896/checkpoint-2794'\n",
    "    # \"outputs/2025_04_29_21_24_17_071961/checkpoint-588\",\n",
    "    # \"outputs/2025_04_30_01_16_22_692562/checkpoint-140000\",\n",
    "    # \"outputs/2025_04_30_01_16_22_692562/checkpoint-10000\",\n",
    "    \"outputs/2025_05_01_14_33_36_096886/checkpoint-10000\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "209 185 0.8851674641148325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    # \"dim/hendrycks_math_train_12k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096\"\n",
    "    # \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    \"dim/hendrycks_math_train_1k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    # \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    ")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    # test_size=250,\n",
    "    test_size=350,\n",
    "    # test_size=1,\n",
    "    seed=42,\n",
    ")\n",
    "dataset = dataset[\"test\"].filter(lambda x: x[\"model_answer\"].count(\"</think>\") == 1)\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "\n",
    "correct_dataset = []\n",
    "\n",
    "for pos, item in enumerate(dataset):\n",
    "    try:\n",
    "        answer = dataset_answer_filter(item[\"answer\"])\n",
    "        model_answer = model_answer_filter(item[\"model_answer\"])\n",
    "        # print(answer, model_answer)\n",
    "        # break\n",
    "        if is_equiv(answer, model_answer):\n",
    "            correct_dataset.append(item)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(dataset), len(correct_dataset), len(correct_dataset) / len(dataset))\n",
    "\n",
    "correct_dataset = correct_dataset[:30]\n",
    "len(correct_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<beginofsentence><User>Problem: If $A=2+i$, $O=-4$, $P=-i$, and $S=2+4i$, find $A-O+P+S$.\n",
      "\n",
      "Please reason step by step, and put your final answer within \\boxed{}.<Assistant><think>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this problem here where I need to find the value of A minus O plus P plus S. The given values are A = 2 + i, O = -4, P = -i, and S = 2 + 4i. Hmm, let me write that down to make it clearer.\\n\\nFirst, let me write down each term:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nAnd the expression I need to compute is A - O + P + S. Hmm, okay. So, I think I need to substitute these values into the expression and then simplify it step by step.\\n\\nLet me start by substituting the values:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nWait, hold on. Let me make sure I substitute correctly. So, A is 2 + i, O is -4, P is -i, and S is 2 + 4i. So, replacing each term:\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nOkay, that looks right. Now, let me simplify this step by step. I think I should handle the operations one at a time.\\n\\nFirst, let's handle the subtraction of O, which is -(-4). Subtracting a negative is the same as adding a positive, so:\\n\\n(2 + i) - (-4) = (2 + i) + 4\\n\\nSo, that simplifies to 2 + 4 + i, which is 6 + i.\\n\\nAlright, so now the expression becomes:\\n\\n6 + i + (-i) + (2 + 4i)\\n\\nWait, let me write that again:\\n\\n= (6 + i) + (-i) + (2 + 4i)\\n\\nNow, let's combine the real parts and the imaginary parts separately.\\n\\nFirst, the real parts: 6, 0 (since there's no real part in -i), and 2.\\n\\nSo, 6 + 0 + 2 = 8.\\n\\nNow, the imaginary parts: i, -i, and 4i.\\n\\nSo, let's add those up:\\n\\ni - i + 4i\\n\\nHmm, i minus i is 0, so that leaves us with 4i.\\n\\nSo, putting it all together, the real part is 8 and the imaginary part is 4i, so the result is 8 + 4i.\\n\\nWait, let me double-check that. Maybe I made a mistake in the signs somewhere.\\n\\nStarting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, handle the subtraction:\\n\\n(2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P: 6 + i + (-i) = 6 + i - i = 6\\n\\nThen, add S: 6 + (2 + 4i) = 8 + 4i\\n\\nYes, that seems consistent. So, the final result is 8 + 4i.\\n\\nWait, hold on, let me make sure I didn't skip any steps or make a mistake in the signs.\\n\\nSo, starting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, compute (2 + i) - (-4):\\n\\nSubtracting a negative is adding, so 2 + i + 4 = 6 + i.\\n\\nThen, add P, which is -i:\\n\\n6 + i + (-i) = 6 + (i - i) = 6 + 0 = 6.\\n\\nThen, add S, which is 2 + 4i:\\n\\n6 + (2 + 4i) = (6 + 2) + 4i = 8 + 4i.\\n\\nYes, that seems correct. So, the final answer is 8 + 4i.\\n\\nAlternatively, maybe I can approach this problem by grouping the real and imaginary parts differently.\\n\\nLet me try that.\\n\\nGiven:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nSo, A - O + P + S can be rewritten as:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nLet me group the real numbers and the imaginary numbers separately.\\n\\nReal parts: 2, 0 (since O is -4, which is real), -0 (since P is -i, which is imaginary), and 2.\\n\\nImaginary parts: 1 (from A), 0 (from O), -1 (from P), and 4 (from S).\\n\\nSo, real parts: 2 + 0 + 0 + 2 = 4\\n\\nImaginary parts: 1 + 0 - 1 + 4 = 4\\n\\nWait, hold on, that doesn't seem right. Because when I grouped them earlier, I had 8 + 4i, but now grouping separately, I get 4 + 4i. That's a discrepancy. Hmm, so I must have made a mistake in my grouping.\\n\\nWait, let me check again.\\n\\nWait, no, actually, in the expression A - O + P + S, O is a real number, so it's just -(-4) which is +4. So, in the real parts, we have 2 (from A) + 4 (from -O) + 0 (from P) + 2 (from S). So, 2 + 4 + 2 = 8.\\n\\nImaginary parts: 1 (from A) + 0 (from -O) -1 (from P) + 4 (from S). So, 1 -1 + 4 = 4.\\n\\nSo, that gives 8 + 4i, which matches my previous result.\\n\\nWait, so why did I get 4 + 4i when I grouped them? Because I think I miscounted the real parts.\\n\\nWait, let me write it step by step.\\n\\nReal parts:\\n\\nFrom A: 2\\n\\nFrom -O: -(-4) = +4\\n\\nFrom P: 0 (since P is -i, which is purely imaginary)\\n\\nFrom S: 2\\n\\nSo, total real parts: 2 + 4 + 0 + 2 = 8\\n\\nImaginary parts:\\n\\nFrom A: 1\\n\\nFrom -O: 0 (since O is real)\\n\\nFrom P: -1\\n\\nFrom S: 4\\n\\nTotal imaginary parts: 1 + 0 -1 + 4 = 4\\n\\nSo, 8 + 4i. That's correct.\\n\\nSo, my initial grouping was wrong because I didn't account for all the real parts correctly. So, the correct answer is 8 + 4i.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = A + (-O) + P + S\\n\\nSo, substituting:\\n\\n= (2 + i) + (4) + (-i) + (2 + 4i)\\n\\nThen, combine like terms:\\n\\nReal parts: 2 + 4 + 2 = 8\\n\\nImaginary parts: i - i + 4i = 4i\\n\\nSo, 8 + 4i.\\n\\nYes, that's consistent.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = (A + P) + (-O + S)\\n\\nCompute A + P first:\\n\\nA = 2 + i\\n\\nP = -i\\n\\nSo, A + P = 2 + i + (-i) = 2 + 0i = 2\\n\\nThen, compute -O + S:\\n\\nO = -4, so -O = 4\\n\\nS = 2 + 4i\\n\\nSo, -O + S = 4 + (2 + 4i) = 6 + 4i\\n\\nNow, add A + P and -O + S:\\n\\n2 + (6 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, regardless of the method, I get 8 + 4i.\\n\\nTherefore, I think that's the correct answer.\\n\\nWait, just to make sure, let me compute each step numerically.\\n\\nCompute A - O:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nSo, A - O = (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P:\\n\\n6 + i + (-i) = 6 + 0i = 6\\n\\nThen, add S:\\n\\n6 + (2 + 4i) = 8 + 4i\\n\\nYep, same result.\\n\\nAlternatively, if I compute A - O + P + S step by step:\\n\\nStart with A: 2 + i\\n\\nSubtract O: 2 + i - (-4) = 2 + i + 4 = 6 + i\\n\\nAdd P: 6 + i + (-i) = 6 + 0i = 6\\n\\nAdd S: 6 + (2 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, I think I can confidently say that the answer is 8 + 4i.\\n\\nJust to recap, the steps are:\\n\\n1. Substitute each term into the expression.\\n\\n2. Simplify the expression by handling the operations step by step.\\n\\n3. Combine like terms (real and imaginary parts separately).\\n\\n4. Verify the result using different methods to ensure accuracy.\\n\\nSo, yeah, 8 + 4i is the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( A - O + P + S \\\\) is \\\\boxed{8 + 4i}.\\n</think>\\n\\nGiven the values \\\\( A = 2 + i \\\\), \\\\( O = -4 \\\\), \\\\( P = -i \\\\), and \\\\( S = 2 + 4i \\\\), we need to find the value of \\\\( A - O + P + S \\\\).\\n\\nFirst, substitute the given values into the expression:\\n\\n\\\\[\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\\\]\\n\\nSimplify each term step by step:\\n\\n1. Subtract \\\\( O \\\\) from \\\\( A \\\\):\\n   \\\\[\\n   (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n   \\\\]\\n\\n2. Add \\\\( P \\\\) to the result:\\n   \\\\[\\n   6 + i + (-i) = 6 + 0i = 6\\n   \\\\]\\n\\n3. Add \\\\( S \\\\) to the result:\\n   \\\\[\\n   6 + (2 + 4i) = 8 + 4i\\n   \\\\]\\n\\nThus, the value of \\\\( A - O + P + S \\\\) is \\\\(\\\\boxed{8 + 4i}\\\\).\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\"\n",
    "# prompt = \"how many wings has a bird?\"\n",
    "prompt = correct_dataset[0][\"problem\"]\n",
    "\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": base_prompt.format(question=prompt)},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "model_inputs = tokenizer(\n",
    "    [text],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"longest\",\n",
    "    truncation=False,\n",
    "    add_special_tokens=False,\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # generated_ids = model.generate(\n",
    "    #     model_inputs.input_ids,\n",
    "    #     max_new_tokens=1,\n",
    "    #     do_sample=False,\n",
    "    # )\n",
    "    generated_ids = model.generate(\n",
    "        # **model_inputs.input_ids,\n",
    "        **model_inputs,\n",
    "        max_new_tokens=4096,\n",
    "        do_sample=False,\n",
    "        # do_sample=not False,\n",
    "        # temperature=0.6,\n",
    "        # top_p=0.95,\n",
    "    )\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'An elephant and a lion are currently 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour.  How many minutes will it take for the lion to catch the elephant?',\n",
       " 'solution': 'Every hour, the lion runs 24 miles while the elephant runs 19.  Thus, the distance between the two animals closes at a rate of 5 miles every hour.  The lion catches the elephant after this distance has closed 1 mile, which takes $\\\\frac{1}{5}$ hours to do, or $\\\\frac{1}{5}\\\\cdot 60 = \\\\boxed{12}$ minutes.',\n",
       " 'answer': '12',\n",
       " 'subject': 'Prealgebra',\n",
       " 'level': 5,\n",
       " 'unique_id': 'test/prealgebra/1820.json',\n",
       " 'model_answer': \"Okay, so I have this problem here where an elephant and a lion are one mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, sounds like a relative speed problem. Let me think about how to approach this.\\n\\nFirst, let me visualize the situation. There's a lion and an elephant, one mile apart. The lion is running towards the elephant, and the elephant is running away from the lion. Since both are moving towards or away from each other, their speeds will affect the time it takes for the lion to catch up.\\n\\nI remember that when two objects are moving towards each other, their relative speed is the sum of their individual speeds. But in this case, the lion is moving towards the elephant, and the elephant is moving away from the lion, so their speeds are in opposite directions. Wait, no, actually, the lion is moving towards the elephant, so their relative speed is the lion's speed minus the elephant's speed, because they are moving in the same direction. Wait, no, hold on. Let me make sure.\\n\\nThe lion is moving towards the elephant, which is moving away. So from the lion's perspective, the elephant is moving away at 19 mph. So the relative speed between the lion and the elephant is the lion's speed minus the elephant's speed because they're moving in the same direction. Wait, no, actually, if both are moving, the relative speed is the difference when moving in the same direction. Wait, maybe I'm getting confused.\\n\\nLet me recall the formula: when two objects are moving towards each other, their relative speed is the sum of their speeds. But when moving in the same direction, it's the difference. But in this case, the lion is chasing the elephant. So if the lion is moving towards the elephant, and the elephant is moving away, their speeds are in the same direction. So the relative speed is lion's speed minus elephant's speed.\\n\\nWait, hold on. Let me think in terms of distance. The initial distance is 1 mile. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So relative to the lion, the elephant is approaching at (24 - 19) mph, which is 5 mph. So the lion is gaining on the elephant at 5 miles per hour.\\n\\nTherefore, the time it takes for the lion to catch the elephant would be the initial distance divided by the relative speed. So 1 mile divided by 5 mph, which is 1/5 hours. To convert that into minutes, since 1 hour is 60 minutes, 1/5 hours is 12 minutes.\\n\\nWait, that seems straightforward, but let me verify. Let me think of it in terms of equations.\\n\\nLets denote the time it takes for the lion to catch the elephant as t hours.\\n\\nIn t hours, the lion will cover a distance of 24t miles.\\n\\nIn the same t hours, the elephant will cover a distance of 19t miles, but since the elephant is moving away, the distance between the lion and the elephant will decrease at a rate of (24 - 19) mph, which is 5 mph.\\n\\nSo the distance between them decreases by 5 miles every hour. Since they start 1 mile apart, the time it takes for the lion to catch the elephant is 1 / 5 hours, which is 12 minutes. So that seems consistent.\\n\\nAlternatively, I can model the positions of the lion and the elephant as functions of time and find when they meet.\\n\\nLets assume the lion starts at position 0, and the elephant starts at position 1 mile.\\n\\nLets let t be the time in hours. Then, the position of the lion after t hours is 0 + 24t = 24t miles.\\n\\nThe position of the elephant after t hours is 1 - 19t miles.\\n\\nThey meet when their positions are equal, so:\\n\\n24t = 1 - 19t\\n\\nAdding 19t to both sides:\\n\\n24t + 19t = 1\\n\\n43t = 1\\n\\nt = 1/43 hours.\\n\\nWait, hold on, this is different. Earlier, I thought it was 1/5 hours, but now I have 1/43 hours. Which one is correct?\\n\\nWait, that's a big difference. Which approach is correct?\\n\\nLet me check my equations again. Position of lion is 24t, position of elephant is 1 - 19t. So when they meet, 24t = 1 - 19t.\\n\\nSo 24t + 19t = 1\\n\\n43t = 1\\n\\nt = 1/43 hours.\\n\\nHmm, so that's approximately 1.395 minutes, which is about 83.7 seconds. But that seems too fast. Wait, but 1/43 of an hour is roughly 1.395 minutes, which is 83.7 seconds, but 1/5 is 12 minutes, which is 720 seconds. That seems like a huge difference.\\n\\nSo there's a discrepancy here. Which one is right?\\n\\nWait, maybe I made a mistake in setting up the equations. Let me think.\\n\\nWait, if the lion is moving towards the elephant, which is moving away, so the distance between them is decreasing at a rate of (24 - 19) mph, which is 5 mph. So the time should be 1 mile / 5 mph = 1/5 hours, which is 12 minutes.\\n\\nBut in the equation above, I set the positions equal, which gives me t = 1/43 hours. So which is correct?\\n\\nWait, no, the position of the lion is 24t, and the position of the elephant is 1 - 19t. So when they meet, 24t = 1 - 19t.\\n\\nWait, that would mean that the lion is moving towards the elephant, and the elephant is moving away, so the distance between them is decreasing. So the correct equation should be 24t = 1 - 19t.\\n\\nBut solving that gives t = 1/43 hours, which is approximately 1.395 minutes, which seems contradictory.\\n\\nWait, that can't be. Because if the lion is moving at 24 mph and the elephant is moving away at 19 mph, the lion is closing the distance at 5 mph.\\n\\nBut in the equation, the lion is moving at 24t, which is 24 miles per hour, but the elephant is moving away at 19 mph, so the relative speed is 24 - 19 = 5 mph.\\n\\nWait, so maybe the correct equation is that the lion is moving towards the elephant at 5 mph, so the time is 1 mile / 5 mph = 1/5 hours, which is 12 minutes.\\n\\nBut then why does the position equation give me a different answer?\\n\\nWait, perhaps I have a sign error in the position equation.\\n\\nLet me think about the coordinate system. Let's say the lion starts at position 0, and the elephant starts at position 1.\\n\\nSo the lion is moving towards the positive direction at 24 mph, so its position at time t is 24t.\\n\\nThe elephant is moving away from the lion, so its position at time t is 1 + 19t.\\n\\nWait, hold on, if the elephant is moving away from the lion, which is at position 0, then yes, the elephant's position is 1 + 19t.\\n\\nBut wait, if the lion is moving towards the elephant, which is at 1 mile, so if the lion is at position 0, and the elephant is at position 1, and the lion is moving towards the elephant, so the distance between them is decreasing.\\n\\nWait, so if the lion is moving towards the elephant, then the elephant is moving away from the lion, so the distance between them is decreasing at a rate of (24 - 19) mph, which is 5 mph.\\n\\nTherefore, the relative speed is 5 mph, so the time is 1 mile / 5 mph = 1/5 hours = 12 minutes.\\n\\nBut if I model the positions, the lion is at 24t, the elephant is at 1 + 19t. Setting them equal:\\n\\n24t = 1 + 19t\\n\\n24t - 19t = 1\\n\\n5t = 1\\n\\nt = 1/5 hours, which is 12 minutes. So that's consistent.\\n\\nWait, so earlier, I thought the position of the elephant was 1 - 19t, but that was incorrect. The correct position of the elephant is 1 + 19t because it's moving away from the lion.\\n\\nSo my initial setup was wrong. I think I confused the direction. So the correct equation is 24t = 1 + 19t, leading to t = 1/5 hours, which is 12 minutes.\\n\\nTherefore, the lion will catch the elephant in 12 minutes.\\n\\nWait, so that contradicts my initial thought that the relative speed is 5 mph. But why was I getting confused earlier?\\n\\nI think the confusion was in the direction of the elephant's movement. If the elephant is moving away from the lion, then the distance between them is increasing at 19 mph, so the relative speed at which the distance is decreasing is 24 - 19 = 5 mph.\\n\\nBut in the position equation, if the lion is moving towards the elephant, and the elephant is moving away, then the distance between them is decreasing at 5 mph. Therefore, the time to close 1 mile is 1 / 5 hours, which is 12 minutes.\\n\\nSo in summary, the correct answer is 12 minutes.\\n\\nBut let me just double-check the equations.\\n\\nLets denote t as the time in hours.\\n\\nPosition of the lion: 24t.\\n\\nPosition of the elephant: 1 + 19t.\\n\\nThey meet when 24t = 1 + 19t.\\n\\n24t - 19t = 1\\n\\n5t = 1\\n\\nt = 1/5 hours = 12 minutes.\\n\\nYes, that seems correct. So the initial mistake was in the position equation for the elephant, thinking it was moving towards, but it's actually moving away. So the correct equation is 24t = 1 + 19t.\\n\\nSo the answer is 12 minutes.\\n\\nI think that makes sense. Let me think about it another way.\\n\\nIf both were moving towards each other, their relative speed would be 24 + 19 = 43 mph, and the time would be 1/43 hours, which is about 1.395 minutes, but that's if they were moving towards each other. But in this case, they are moving away from each other in the same direction, so the relative speed is 24 - 19 = 5 mph, hence 1/5 hours.\\n\\nAlternatively, if I think about the distance between them decreasing at 5 mph, so 1 mile at 5 mph is 1/5 hours, which is 12 minutes.\\n\\nYes, that seems consistent.\\n\\nSo I think my initial setup was wrong because I thought the elephant was moving towards the lion, but it's actually moving away. So the correct answer is 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe problem involves an elephant and a lion that are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. We need to determine how many minutes it will take for the lion to catch the elephant.\\n\\nFirst, we recognize that the lion is moving towards the elephant, and the elephant is moving away. Therefore, the relative speed at which the lion is approaching the elephant is the difference between their speeds: \\\\(24 - 19 = 5\\\\) miles per hour.\\n\\nThe time it takes for the lion to catch the elephant can be calculated by dividing the initial distance between them by their relative speed. The initial distance is 1 mile, and the relative speed is 5 miles per hour. Thus, the time \\\\(t\\\\) in hours is given by:\\n\\n\\\\[\\nt = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = \\\\frac{1}{5} \\\\text{ hours}\\n\\\\]\\n\\nTo convert this time into minutes, we multiply by 60:\\n\\n\\\\[\\n\\\\frac{1}{5} \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes}\\n\\\\]\\n\\nThus, the lion will catch the elephant in \\\\(\\\\boxed{12}\\\\) minutes.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response == correct_dataset[0][\"model_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this problem here where I need to find the value of A minus O plus P plus S. The given values are A = 2 + i, O = -4, P = -i, and S = 2 + 4i. Hmm, let me write that down to make it clearer.\\n\\nFirst, let me write down each term:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nAnd the expression I need to compute is A - O + P + S. Hmm, okay. So, I think I need to substitute these values into the expression and then simplify it step by step.\\n\\nLet me start by substituting the values:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nWait, hold on. Let me make sure I substitute correctly. So, A is 2 + i, O is -4, P is -i, and S is 2 + 4i. So, replacing each term:\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nOkay, that looks right. Now, let me simplify this step by step. I think I should handle the operations one at a time.\\n\\nFirst, let's handle the subtraction of O, which is -(-4). Subtracting a negative is the same as adding a positive, so:\\n\\n(2 + i) - (-4) = (2 + i) + 4\\n\\nSo, that simplifies to 2 + 4 + i, which is 6 + i.\\n\\nAlright, so now the expression becomes:\\n\\n6 + i + (-i) + (2 + 4i)\\n\\nWait, let me write that again:\\n\\n= (6 + i) + (-i) + (2 + 4i)\\n\\nNow, let's combine the real parts and the imaginary parts separately.\\n\\nFirst, the real parts: 6, 0 (since there's no real part in -i), and 2.\\n\\nSo, 6 + 0 + 2 = 8.\\n\\nNow, the imaginary parts: i, -i, and 4i.\\n\\nSo, let's add those up:\\n\\ni - i + 4i\\n\\nHmm, i minus i is 0, so that leaves us with 4i.\\n\\nSo, putting it all together, the real part is 8 and the imaginary part is 4i, so the result is 8 + 4i.\\n\\nWait, let me double-check that. Maybe I made a mistake in the signs somewhere.\\n\\nStarting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, handle the subtraction:\\n\\n(2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P: 6 + i + (-i) = 6 + i - i = 6\\n\\nThen, add S: 6 + (2 + 4i) = 8 + 4i\\n\\nYes, that seems consistent. So, the final result is 8 + 4i.\\n\\nWait, hold on, let me make sure I didn't skip any steps or make a mistake in the signs.\\n\\nSo, starting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, compute (2 + i) - (-4):\\n\\nSubtracting a negative is adding, so 2 + i + 4 = 6 + i.\\n\\nThen, add P, which is -i:\\n\\n6 + i + (-i) = 6 + (i - i) = 6 + 0 = 6.\\n\\nThen, add S, which is 2 + 4i:\\n\\n6 + (2 + 4i) = (6 + 2) + 4i = 8 + 4i.\\n\\nYes, that seems correct. So, the final answer is 8 + 4i.\\n\\nAlternatively, maybe I can approach this problem by grouping the real and imaginary parts differently.\\n\\nLet me try that.\\n\\nGiven:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nSo, A - O + P + S can be rewritten as:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nLet me group the real numbers and the imaginary numbers separately.\\n\\nReal parts: 2, 0 (since O is -4, which is real), -0 (since P is -i, which is imaginary), and 2.\\n\\nImaginary parts: 1 (from A), 0 (from O), -1 (from P), and 4 (from S).\\n\\nSo, real parts: 2 + 0 + 0 + 2 = 4\\n\\nImaginary parts: 1 + 0 - 1 + 4 = 4\\n\\nWait, hold on, that doesn't seem right. Because when I grouped them earlier, I had 8 + 4i, but now grouping separately, I get 4 + 4i. That's a discrepancy. Hmm, so I must have made a mistake in my grouping.\\n\\nWait, let me check again.\\n\\nWait, no, actually, in the expression A - O + P + S, O is a real number, so it's just -(-4) which is +4. So, in the real parts, we have 2 (from A) + 4 (from -O) + 0 (from P) + 2 (from S). So, 2 + 4 + 2 = 8.\\n\\nImaginary parts: 1 (from A) + 0 (from -O) -1 (from P) + 4 (from S). So, 1 -1 + 4 = 4.\\n\\nSo, that gives 8 + 4i, which matches my previous result.\\n\\nWait, so why did I get 4 + 4i when I grouped them? Because I think I miscounted the real parts.\\n\\nWait, let me write it step by step.\\n\\nReal parts:\\n\\nFrom A: 2\\n\\nFrom -O: -(-4) = +4\\n\\nFrom P: 0 (since P is -i, which is purely imaginary)\\n\\nFrom S: 2\\n\\nSo, total real parts: 2 + 4 + 0 + 2 = 8\\n\\nImaginary parts:\\n\\nFrom A: 1\\n\\nFrom -O: 0 (since O is real)\\n\\nFrom P: -1\\n\\nFrom S: 4\\n\\nTotal imaginary parts: 1 + 0 -1 + 4 = 4\\n\\nSo, 8 + 4i. That's correct.\\n\\nSo, my initial grouping was wrong because I didn't account for all the real parts correctly. So, the correct answer is 8 + 4i.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = A + (-O) + P + S\\n\\nSo, substituting:\\n\\n= (2 + i) + (4) + (-i) + (2 + 4i)\\n\\nThen, combine like terms:\\n\\nReal parts: 2 + 4 + 2 = 8\\n\\nImaginary parts: i - i + 4i = 4i\\n\\nSo, 8 + 4i.\\n\\nYes, that's consistent.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = (A + P) + (-O + S)\\n\\nCompute A + P first:\\n\\nA = 2 + i\\n\\nP = -i\\n\\nSo, A + P = 2 + i + (-i) = 2 + 0i = 2\\n\\nThen, compute -O + S:\\n\\nO = -4, so -O = 4\\n\\nS = 2 + 4i\\n\\nSo, -O + S = 4 + (2 + 4i) = 6 + 4i\\n\\nNow, add A + P and -O + S:\\n\\n2 + (6 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, regardless of the method, I get 8 + 4i.\\n\\nTherefore, I think that's the correct answer.\\n\\nWait, just to make sure, let me compute each step numerically.\\n\\nCompute A - O:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nSo, A - O = (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P:\\n\\n6 + i + (-i) = 6 + 0i = 6\\n\\nThen, add S:\\n\\n6 + (2 + 4i) = 8 + 4i\\n\\nYep, same result.\\n\\nAlternatively, if I compute A - O + P + S step by step:\\n\\nStart with A: 2 + i\\n\\nSubtract O: 2 + i - (-4) = 2 + i + 4 = 6 + i\\n\\nAdd P: 6 + i + (-i) = 6 + 0i = 6\\n\\nAdd S: 6 + (2 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, I think I can confidently say that the answer is 8 + 4i.\\n\\nJust to recap, the steps are:\\n\\n1. Substitute each term into the expression.\\n\\n2. Simplify the expression by handling the operations step by step.\\n\\n3. Combine like terms (real and imaginary parts separately).\\n\\n4. Verify the result using different methods to ensure accuracy.\\n\\nSo, yeah, 8 + 4i is the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( A - O + P + S \\\\) is \\\\boxed{8 + 4i}.\\n</think>\\n\\nGiven the values \\\\( A = 2 + i \\\\), \\\\( O = -4 \\\\), \\\\( P = -i \\\\), and \\\\( S = 2 + 4i \\\\), we need to find the value of \\\\( A - O + P + S \\\\).\\n\\nFirst, substitute the given values into the expression:\\n\\n\\\\[\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\\\]\\n\\nSimplify each term step by step:\\n\\n1. Subtract \\\\( O \\\\) from \\\\( A \\\\):\\n   \\\\[\\n   (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n   \\\\]\\n\\n2. Add \\\\( P \\\\) to the result:\\n   \\\\[\\n   6 + i + (-i) = 6 + 0i = 6\\n   \\\\]\\n\\n3. Add \\\\( S \\\\) to the result:\\n   \\\\[\\n   6 + (2 + 4i) = 8 + 4i\\n   \\\\]\\n\\nThus, the value of \\\\( A - O + P + S \\\\) is \\\\(\\\\boxed{8 + 4i}\\\\).\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"model_answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<beginofsentence><User>Problem: An elephant and a lion are currently 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour.  How many minutes will it take for the lion to catch the elephant?\n",
      "\n",
      "Please reason step by step, and put your final answer within \\boxed{}.<Assistant><think>\n",
      "Okay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\n",
      "\n",
      "First, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards it at 24 mph. So, their relative speed is the difference between their speeds because they're moving towards each other. Wait, no, actually, since the elephant is moving away, the lion has to cover the distance that the elephant is moving away plus the distance the elephant covers while the lion is moving towards it.\n",
      "\n",
      "Let me clarify. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the lion is closing the gap at a rate of 24 mph minus 19 mph, which is 5 mph. That makes sense because if two objects are moving towards each other, their relative speed is the sum of their speeds, but in this case, one is moving away and the other is moving towards, so it's the difference.\n",
      "\n",
      "So, the initial distance between them is 1 mile. The lion is closing the gap at 5 mph. To find the time it takes to catch up, I can use the formula:\n",
      "\n",
      "Time = Distance / Speed\n",
      "\n",
      "So, plugging in the numbers, the time should be 1 mile divided by 5 mph. That gives me 0.2 hours. But the question asks for the time in minutes, so I need to convert 0.2 hours to minutes. Since 1 hour is 60 minutes, 0.2 hours is 0.2 * 60 = 12 minutes.\n",
      "\n",
      "Wait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, the relative speed is 24 - 19 = 5 mph. So, yes, the lion is gaining on the elephant at 5 mph. So, 1 mile divided by 5 mph is indeed 0.2 hours, which is 12 minutes. That seems right.\n",
      "\n",
      "But just to make sure I didn't make a mistake, let me think about it another way. Maybe set up an equation for their positions as functions of time and see if I get the same result.\n",
      "\n",
      "Let's denote t as the time in hours it takes for the lion to catch the elephant. In that time, the elephant will have moved a distance of 19t miles away from the starting point, and the lion will have moved 24t miles towards the elephant. Since they start 1 mile apart, the distance between them when the lion catches the elephant will be zero.\n",
      "\n",
      "So, the distance the lion covers plus the distance the elephant covers should equal the initial distance between them. Wait, no, actually, the lion is moving towards the elephant, so the distance the lion covers is 24t, and the distance the elephant covers is 19t. But since the elephant is moving away, the total distance between them when the lion catches up is 24t - 19t = 5t. This should equal the initial distance, which is 1 mile.\n",
      "\n",
      "So, 5t = 1 mile. Solving for t, we get t = 1/5 hours, which is 0.2 hours. Converting that to minutes, 0.2 * 60 = 12 minutes. Yep, same result. So, that seems consistent.\n",
      "\n",
      "Alternatively, maybe I can think about it in terms of how much distance the lion needs to cover relative to the elephant. Since the lion is moving faster, it's gaining on the elephant at 5 mph. So, the lion needs to cover the 1 mile gap at a relative speed of 5 mph. So, time = distance / speed = 1 / 5 hours, which is 12 minutes. Yep, same answer.\n",
      "\n",
      "I think that's solid. So, the lion will catch the elephant in 12 minutes.\n",
      "\n",
      "**Final Answer**\n",
      "The lion will catch the elephant in \\boxed{12} minutes.\n",
      "</think>\n",
      "\n",
      "The elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. \n",
      "\n",
      "To find the time it takes for the lion to catch the elephant, we first determine their relative speed. Since the lion is moving towards the elephant and the elephant is moving away, their relative speed is the difference between their speeds:\n",
      "\n",
      "\\[\n",
      "24 \\text{ mph} - 19 \\text{ mph} = 5 \\text{ mph}\n",
      "\\]\n",
      "\n",
      "The initial distance between them is 1 mile. Using the formula for time, which is distance divided by speed, we get:\n",
      "\n",
      "\\[\n",
      "\\text{Time} = \\frac{1 \\text{ mile}}{5 \\text{ mph}} = 0.2 \\text{ hours}\n",
      "\\]\n",
      "\n",
      "Converting 0.2 hours to minutes:\n",
      "\n",
      "\\[\n",
      "0.2 \\text{ hours} \\times 60 \\text{ minutes per hour} = 12 \\text{ minutes}\n",
      "\\]\n",
      "\n",
      "Thus, the lion will catch the elephant in \\boxed{12} minutes.<endofsentence><beginofsentence>\n",
      "\n",
      "To determine how long it will take for the lion to catch the elephant, we need to consider their relative speed. The elephant is running away at \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\"\n",
    "prompt = correct_dataset[:5][0][\"problem\"]\n",
    "\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": base_prompt.format(question=prompt)},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "# initial_len = generated_tokens.shape\n",
    "with torch.no_grad():\n",
    "    generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "    generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "    max_steps = 1200\n",
    "    past_key_values = None\n",
    "    for step in range(max_steps):\n",
    "        if step == 0:\n",
    "            logits = model(\n",
    "                inputs_embeds=generated_embeds,\n",
    "                attention_mask=torch.ones(1, generated_embeds.shape[1]).long().cuda(),\n",
    "                position_ids=torch.arange(generated_embeds.shape[1])\n",
    "                .cuda()\n",
    "                .unsqueeze(0),\n",
    "                use_cache=True,\n",
    "                past_key_values=None,\n",
    "            )\n",
    "            past_key_values = logits.past_key_values\n",
    "            logits = logits.logits[:, -1, :].clone().float()\n",
    "        else:\n",
    "            logits = model(\n",
    "                # input_ids=generated_tokens[-1][-1:].unsqueeze(0),\n",
    "                inputs_embeds=generated_embeds[:, -1:, :],\n",
    "                attention_mask=torch.ones(1, generated_embeds.shape[1]).long().cuda(),\n",
    "                position_ids=torch.tensor(generated_embeds.shape[1] - 1)\n",
    "                .reshape(1, 1)\n",
    "                .cuda(),\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "            )\n",
    "            past_key_values = logits.past_key_values\n",
    "\n",
    "            logits = logits.logits[:, -1, :].clone().float()\n",
    "\n",
    "        top_token = logits.argmax(-1)[-1]\n",
    "        top_token_embed = model.get_input_embeddings()(top_token)\n",
    "        # print(top)\n",
    "        generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "        generated_embeds = torch.cat(\n",
    "            [generated_embeds, top_token_embed.reshape(1, 1, -1)],\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(step, tokenizer.decode(generated_tokens[-1]))\n",
    "    # break\n",
    "print(tokenizer.decode(generated_tokens[-1]))\n",
    "# break\n",
    "embeds_generation_tokens = generated_tokens[-1]\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<beginofsentence><User>Problem: An elephant and a lion are currently 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour.  How many minutes will it take for the lion to catch the elephant?\\n\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<Assistant><think>\\nOkay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards it at 24 mph. So, their relative speed is the difference between their speeds because they're moving towards each other. Wait, no, actually, since the elephant is moving away, the lion has to cover the distance that the elephant is moving away plus the distance the elephant covers while the lion is moving towards it.\\n\\nLet me clarify. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the lion is closing the gap at a rate of 24 mph minus 19 mph, which is 5 mph. That makes sense because if two objects are moving towards each other, their relative speed is the sum of their speeds, but in this case, one is moving away and the other is moving towards, so it's the difference.\\n\\nSo, the initial distance between them is 1 mile. The lion is closing the gap at 5 mph. To find the time it takes to catch up, I can use the formula:\\n\\nTime = Distance / Speed\\n\\nSo, plugging in the numbers, the time should be 1 mile divided by 5 mph. That gives me 0.2 hours. But the question asks for the time in minutes, so I need to convert 0.2 hours to minutes. Since 1 hour is 60 minutes, 0.2 hours is 0.2 * 60 = 12 minutes.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, the relative speed is 24 - 19 = 5 mph. So, yes, the lion is gaining on the elephant at 5 mph. So, 1 mile divided by 5 mph is indeed 0.2 hours, which is 12 minutes. That seems right.\\n\\nBut just to make sure I didn't make a mistake, let me think about it another way. Maybe set up an equation for their positions as functions of time and see if I get the same result.\\n\\nLet's denote t as the time in hours it takes for the lion to catch the elephant. In that time, the elephant will have moved a distance of 19t miles away from the starting point, and the lion will have moved 24t miles towards the elephant. Since they start 1 mile apart, the distance between them when the lion catches the elephant will be zero.\\n\\nSo, the distance the lion covers plus the distance the elephant covers should equal the initial distance between them. Wait, no, actually, the lion is moving towards the elephant, so the distance the lion covers is 24t, and the distance the elephant covers is 19t. But since the elephant is moving away, the total distance between them when the lion catches up is 24t - 19t = 5t. This should equal the initial distance, which is 1 mile.\\n\\nSo, 5t = 1 mile. Solving for t, we get t = 1/5 hours, which is 0.2 hours. Converting that to minutes, 0.2 * 60 = 12 minutes. Yep, same result. So, that seems consistent.\\n\\nAlternatively, maybe I can think about it in terms of how much distance the lion needs to cover relative to the elephant. Since the lion is moving faster, it's gaining on the elephant at 5 mph. So, the lion needs to cover the 1 mile gap at a relative speed of 5 mph. So, time = distance / speed = 1 / 5 hours, which is 12 minutes. Yep, same answer.\\n\\nI think that's solid. So, the lion will catch the elephant in 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. \\n\\nTo find the time it takes for the lion to catch the elephant, we first determine their relative speed. Since the lion is moving towards the elephant and the elephant is moving away, their relative speed is the difference between their speeds:\\n\\n\\\\[\\n24 \\\\text{ mph} - 19 \\\\text{ mph} = 5 \\\\text{ mph}\\n\\\\]\\n\\nThe initial distance between them is 1 mile. Using the formula for time, which is distance divided by speed, we get:\\n\\n\\\\[\\n\\\\text{Time} = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = 0.2 \\\\text{ hours}\\n\\\\]\\n\\nConverting 0.2 hours to minutes:\\n\\n\\\\[\\n0.2 \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes}\\n\\\\]\\n\\nThus, the lion will catch the elephant in \\\\boxed{12} minutes.<endofsentence><beginofsentence>\\n\\nTo determine how long it will take for the lion to catch the elephant, we need to consider their relative speed. The elephant is running away at \""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "from transformers.generation import utils\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\"\n",
    "dataset_pos = 0\n",
    "prompt = correct_dataset[dataset_pos][\"problem\"]\n",
    "\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": base_prompt.format(question=prompt),\n",
    "        },\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "with torch.no_grad():\n",
    "    generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "    generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        inputs_embeds=generated_embeds,\n",
    "        max_new_tokens=4096,\n",
    "        # max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this problem here where I need to find the value of A minus O plus P plus S. The given values are A = 2 + i, O = -4, P = -i, and S = 2 + 4i. Hmm, let me write that down to make it clearer.\\n\\nFirst, let me write down each term:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nAnd the expression I need to compute is A - O + P + S. Hmm, okay. So, I think I need to substitute these values into the expression and then simplify it step by step.\\n\\nLet me start by substituting the values:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nWait, hold on. Let me make sure I substitute correctly. So, A is 2 + i, O is -4, P is -i, and S is 2 + 4i. So, replacing each term:\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nOkay, that looks right. Now, let me simplify this step by step. I think I should handle the operations one at a time.\\n\\nFirst, let's handle the subtraction of O, which is -(-4). Subtracting a negative is the same as adding a positive, so:\\n\\n(2 + i) - (-4) = (2 + i) + 4\\n\\nSo, that simplifies to 2 + 4 + i, which is 6 + i.\\n\\nAlright, so now the expression becomes:\\n\\n6 + i + (-i) + (2 + 4i)\\n\\nWait, let me write that again:\\n\\n= (6 + i) + (-i) + (2 + 4i)\\n\\nNow, let's combine the real parts and the imaginary parts separately.\\n\\nFirst, the real parts: 6, 0 (since there's no real part in -i), and 2.\\n\\nSo, 6 + 0 + 2 = 8.\\n\\nNow, the imaginary parts: i, -i, and 4i.\\n\\nSo, let's add those up:\\n\\ni - i + 4i\\n\\nHmm, i minus i is 0, so that leaves us with 4i.\\n\\nSo, putting it all together, the real part is 8 and the imaginary part is 4i, so the result is 8 + 4i.\\n\\nWait, let me double-check that. Maybe I made a mistake in the signs somewhere.\\n\\nStarting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, handle the subtraction:\\n\\n(2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P: 6 + i + (-i) = 6 + i - i = 6\\n\\nThen, add S: 6 + (2 + 4i) = 8 + 4i\\n\\nYes, that seems consistent. So, the final result is 8 + 4i.\\n\\nWait, hold on, let me make sure I didn't skip any steps or make a mistake in the signs.\\n\\nSo, starting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, compute (2 + i) - (-4):\\n\\nSubtracting a negative is adding, so 2 + i + 4 = 6 + i.\\n\\nThen, add P, which is -i:\\n\\n6 + i + (-i) = 6 + (i - i) = 6 + 0 = 6.\\n\\nThen, add S, which is 2 + 4i:\\n\\n6 + (2 + 4i) = (6 + 2) + 4i = 8 + 4i.\\n\\nYes, that seems correct. So, the final answer is 8 + 4i.\\n\\nAlternatively, maybe I can approach this problem by grouping the real and imaginary parts differently.\\n\\nLet me try that.\\n\\nGiven:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nSo, A - O + P + S can be rewritten as:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nLet me group the real numbers and the imaginary numbers separately.\\n\\nReal parts: 2, 0 (since O is -4, which is real), -0 (since P is -i, which is imaginary), and 2.\\n\\nImaginary parts: 1 (from A), 0 (from O), -1 (from P), and 4 (from S).\\n\\nSo, real parts: 2 + 0 + 0 + 2 = 4\\n\\nImaginary parts: 1 + 0 - 1 + 4 = 4\\n\\nWait, hold on, that doesn't seem right. Because when I grouped them earlier, I had 8 + 4i, but now grouping separately, I get 4 + 4i. That's a discrepancy. Hmm, so I must have made a mistake in my grouping.\\n\\nWait, let me check again.\\n\\nWait, no, actually, in the expression A - O + P + S, O is a real number, so it's just -(-4) which is +4. So, in the real parts, we have 2 (from A) + 4 (from -O) + 0 (from P) + 2 (from S). So, 2 + 4 + 2 = 8.\\n\\nImaginary parts: 1 (from A) + 0 (from -O) -1 (from P) + 4 (from S). So, 1 -1 + 4 = 4.\\n\\nSo, that gives 8 + 4i, which matches my previous result.\\n\\nWait, so why did I get 4 + 4i when I grouped them? Because I think I miscounted the real parts.\\n\\nWait, let me write it step by step.\\n\\nReal parts:\\n\\nFrom A: 2\\n\\nFrom -O: -(-4) = +4\\n\\nFrom P: 0 (since P is -i, which is purely imaginary)\\n\\nFrom S: 2\\n\\nSo, total real parts: 2 + 4 + 0 + 2 = 8\\n\\nImaginary parts:\\n\\nFrom A: 1\\n\\nFrom -O: 0 (since O is real)\\n\\nFrom P: -1\\n\\nFrom S: 4\\n\\nTotal imaginary parts: 1 + 0 -1 + 4 = 4\\n\\nSo, 8 + 4i. That's correct.\\n\\nSo, my initial grouping was wrong because I didn't account for all the real parts correctly. So, the correct answer is 8 + 4i.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = A + (-O) + P + S\\n\\nSo, substituting:\\n\\n= (2 + i) + (4) + (-i) + (2 + 4i)\\n\\nThen, combine like terms:\\n\\nReal parts: 2 + 4 + 2 = 8\\n\\nImaginary parts: i - i + 4i = 4i\\n\\nSo, 8 + 4i.\\n\\nYes, that's consistent.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = (A + P) + (-O + S)\\n\\nCompute A + P first:\\n\\nA = 2 + i\\n\\nP = -i\\n\\nSo, A + P = 2 + i + (-i) = 2 + 0i = 2\\n\\nThen, compute -O + S:\\n\\nO = -4, so -O = 4\\n\\nS = 2 + 4i\\n\\nSo, -O + S = 4 + (2 + 4i) = 6 + 4i\\n\\nNow, add A + P and -O + S:\\n\\n2 + (6 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, regardless of the method, I get 8 + 4i.\\n\\nTherefore, I think that's the correct answer.\\n\\nWait, just to make sure, let me compute each step numerically.\\n\\nCompute A - O:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nSo, A - O = (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P:\\n\\n6 + i + (-i) = 6 + 0i = 6\\n\\nThen, add S:\\n\\n6 + (2 + 4i) = 8 + 4i\\n\\nYep, same result.\\n\\nAlternatively, if I compute A - O + P + S step by step:\\n\\nStart with A: 2 + i\\n\\nSubtract O: 2 + i - (-4) = 2 + i + 4 = 6 + i\\n\\nAdd P: 6 + i + (-i) = 6 + 0i = 6\\n\\nAdd S: 6 + (2 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, I think I can confidently say that the answer is 8 + 4i.\\n\\nJust to recap, the steps are:\\n\\n1. Substitute each term into the expression.\\n\\n2. Simplify the expression by handling the operations step by step.\\n\\n3. Combine like terms (real and imaginary parts separately).\\n\\n4. Verify the result using different methods to ensure accuracy.\\n\\nSo, yeah, 8 + 4i is the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( A - O + P + S \\\\) is \\\\boxed{8 + 4i}.\\n</think>\\n\\nGiven the values \\\\( A = 2 + i \\\\), \\\\( O = -4 \\\\), \\\\( P = -i \\\\), and \\\\( S = 2 + 4i \\\\), we need to find the value of \\\\( A - O + P + S \\\\).\\n\\nFirst, substitute the given values into the expression:\\n\\n\\\\[\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\\\]\\n\\nSimplify each term step by step:\\n\\n1. Subtract \\\\( O \\\\) from \\\\( A \\\\):\\n   \\\\[\\n   (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n   \\\\]\\n\\n2. Add \\\\( P \\\\) to the result:\\n   \\\\[\\n   6 + i + (-i) = 6 + 0i = 6\\n   \\\\]\\n\\n3. Add \\\\( S \\\\) to the result:\\n   \\\\[\\n   6 + (2 + 4i) = 8 + 4i\\n   \\\\]\\n\\nThus, the value of \\\\( A - O + P + S \\\\) is \\\\(\\\\boxed{8 + 4i}\\\\).<endofsentence>\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this problem here where I need to find the value of A minus O plus P plus S. The given values are A = 2 + i, O = -4, P = -i, and S = 2 + 4i. Hmm, let me write that down to make it clearer.\\n\\nFirst, let me write down each term:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nAnd the expression I need to compute is A - O + P + S. Hmm, okay. So, I think I need to substitute these values into the expression and then simplify it step by step.\\n\\nLet me start by substituting the values:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nWait, hold on. Let me make sure I substitute correctly. So, A is 2 + i, O is -4, P is -i, and S is 2 + 4i. So, replacing each term:\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nOkay, that looks right. Now, let me simplify this step by step. I think I should handle the operations one at a time.\\n\\nFirst, let's handle the subtraction of O, which is -(-4). Subtracting a negative is the same as adding a positive, so:\\n\\n(2 + i) - (-4) = (2 + i) + 4\\n\\nSo, that simplifies to 2 + 4 + i, which is 6 + i.\\n\\nAlright, so now the expression becomes:\\n\\n6 + i + (-i) + (2 + 4i)\\n\\nWait, let me write that again:\\n\\n= (6 + i) + (-i) + (2 + 4i)\\n\\nNow, let's combine the real parts and the imaginary parts separately.\\n\\nFirst, the real parts: 6, 0 (since there's no real part in -i), and 2.\\n\\nSo, 6 + 0 + 2 = 8.\\n\\nNow, the imaginary parts: i, -i, and 4i.\\n\\nSo, let's add those up:\\n\\ni - i + 4i\\n\\nHmm, i minus i is 0, so that leaves us with 4i.\\n\\nSo, putting it all together, the real part is 8 and the imaginary part is 4i, so the result is 8 + 4i.\\n\\nWait, let me double-check that. Maybe I made a mistake in the signs somewhere.\\n\\nStarting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, handle the subtraction:\\n\\n(2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P: 6 + i + (-i) = 6 + i - i = 6\\n\\nThen, add S: 6 + (2 + 4i) = 8 + 4i\\n\\nYes, that seems consistent. So, the final result is 8 + 4i.\\n\\nWait, hold on, let me make sure I didn't skip any steps or make a mistake in the signs.\\n\\nSo, starting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, compute (2 + i) - (-4):\\n\\nSubtracting a negative is adding, so 2 + i + 4 = 6 + i.\\n\\nThen, add P, which is -i:\\n\\n6 + i + (-i) = 6 + (i - i) = 6 + 0 = 6.\\n\\nThen, add S, which is 2 + 4i:\\n\\n6 + (2 + 4i) = (6 + 2) + 4i = 8 + 4i.\\n\\nYes, that seems correct. So, the final answer is 8 + 4i.\\n\\nAlternatively, maybe I can approach this problem by grouping the real and imaginary parts differently.\\n\\nLet me try that.\\n\\nGiven:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nSo, A - O + P + S can be rewritten as:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nLet me group the real numbers and the imaginary numbers separately.\\n\\nReal parts: 2, 0 (since O is -4, which is real), -0 (since P is -i, which is imaginary), and 2.\\n\\nImaginary parts: 1 (from A), 0 (from O), -1 (from P), and 4 (from S).\\n\\nSo, real parts: 2 + 0 + 0 + 2 = 4\\n\\nImaginary parts: 1 + 0 - 1 + 4 = 4\\n\\nWait, hold on, that doesn't seem right. Because when I grouped them earlier, I had 8 + 4i, but now grouping separately, I get 4 + 4i. That's a discrepancy. Hmm, so I must have made a mistake in my grouping.\\n\\nWait, let me check again.\\n\\nWait, no, actually, in the expression A - O + P + S, O is a real number, so it's just -(-4) which is +4. So, in the real parts, we have 2 (from A) + 4 (from -O) + 0 (from P) + 2 (from S). So, 2 + 4 + 2 = 8.\\n\\nImaginary parts: 1 (from A) + 0 (from -O) -1 (from P) + 4 (from S). So, 1 -1 + 4 = 4.\\n\\nSo, that gives 8 + 4i, which matches my previous result.\\n\\nWait, so why did I get 4 + 4i when I grouped them? Because I think I miscounted the real parts.\\n\\nWait, let me write it step by step.\\n\\nReal parts:\\n\\nFrom A: 2\\n\\nFrom -O: -(-4) = +4\\n\\nFrom P: 0 (since P is -i, which is purely imaginary)\\n\\nFrom S: 2\\n\\nSo, total real parts: 2 + 4 + 0 + 2 = 8\\n\\nImaginary parts:\\n\\nFrom A: 1\\n\\nFrom -O: 0 (since O is real)\\n\\nFrom P: -1\\n\\nFrom S: 4\\n\\nTotal imaginary parts: 1 + 0 -1 + 4 = 4\\n\\nSo, 8 + 4i. That's correct.\\n\\nSo, my initial grouping was wrong because I didn't account for all the real parts correctly. So, the correct answer is 8 + 4i.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = A + (-O) + P + S\\n\\nSo, substituting:\\n\\n= (2 + i) + (4) + (-i) + (2 + 4i)\\n\\nThen, combine like terms:\\n\\nReal parts: 2 + 4 + 2 = 8\\n\\nImaginary parts: i - i + 4i = 4i\\n\\nSo, 8 + 4i.\\n\\nYes, that's consistent.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = (A + P) + (-O + S)\\n\\nCompute A + P first:\\n\\nA = 2 + i\\n\\nP = -i\\n\\nSo, A + P = 2 + i + (-i) = 2 + 0i = 2\\n\\nThen, compute -O + S:\\n\\nO = -4, so -O = 4\\n\\nS = 2 + 4i\\n\\nSo, -O + S = 4 + (2 + 4i) = 6 + 4i\\n\\nNow, add A + P and -O + S:\\n\\n2 + (6 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, regardless of the method, I get 8 + 4i.\\n\\nTherefore, I think that's the correct answer.\\n\\nWait, just to make sure, let me compute each step numerically.\\n\\nCompute A - O:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nSo, A - O = (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P:\\n\\n6 + i + (-i) = 6 + 0i = 6\\n\\nThen, add S:\\n\\n6 + (2 + 4i) = 8 + 4i\\n\\nYep, same result.\\n\\nAlternatively, if I compute A - O + P + S step by step:\\n\\nStart with A: 2 + i\\n\\nSubtract O: 2 + i - (-4) = 2 + i + 4 = 6 + i\\n\\nAdd P: 6 + i + (-i) = 6 + 0i = 6\\n\\nAdd S: 6 + (2 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, I think I can confidently say that the answer is 8 + 4i.\\n\\nJust to recap, the steps are:\\n\\n1. Substitute each term into the expression.\\n\\n2. Simplify the expression by handling the operations step by step.\\n\\n3. Combine like terms (real and imaginary parts separately).\\n\\n4. Verify the result using different methods to ensure accuracy.\\n\\nSo, yeah, 8 + 4i is the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( A - O + P + S \\\\) is \\\\boxed{8 + 4i}.\\n</think>\\n\\nGiven the values \\\\( A = 2 + i \\\\), \\\\( O = -4 \\\\), \\\\( P = -i \\\\), and \\\\( S = 2 + 4i \\\\), we need to find the value of \\\\( A - O + P + S \\\\).\\n\\nFirst, substitute the given values into the expression:\\n\\n\\\\[\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\\\]\\n\\nSimplify each term step by step:\\n\\n1. Subtract \\\\( O \\\\) from \\\\( A \\\\):\\n   \\\\[\\n   (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n   \\\\]\\n\\n2. Add \\\\( P \\\\) to the result:\\n   \\\\[\\n   6 + i + (-i) = 6 + 0i = 6\\n   \\\\]\\n\\n3. Add \\\\( S \\\\) to the result:\\n   \\\\[\\n   6 + (2 + 4i) = 8 + 4i\\n   \\\\]\\n\\nThus, the value of \\\\( A - O + P + S \\\\) is \\\\(\\\\boxed{8 + 4i}\\\\).\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_dataset[dataset_pos][\"model_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] == correct_dataset[\n",
    "    dataset_pos\n",
    "][\"model_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': \"Alicia's average on her five tests is 88 points. The score range for each test is 0 points to 100 points, inclusive. What is the lowest possible score that Alicia could have earned on one of the five tests?\",\n",
       " 'solution': \"If Alicia's average score on her five tests is 88 points, then the sum of her scores must be $88 \\\\times 5 = 440$ points. If she earned   100 points on four of the tests, then she could have earned a score as low as $\\\\boxed{40\\\\text{ points}}$ on the other test.\",\n",
       " 'answer': '40\\\\text{ points}',\n",
       " 'subject': 'Prealgebra',\n",
       " 'level': 4,\n",
       " 'unique_id': 'test/prealgebra/1697.json',\n",
       " 'model_answer': \"Okay, so I need to figure out the lowest possible score Alicia could have earned on one of her five tests. She has an average of 88 points, and each test is scored between 0 and 100, inclusive. Hmm, let me think about this step by step.\\n\\nFirst, I know that an average is calculated by taking the sum of all the test scores and dividing it by the number of tests. In this case, she has five tests, and her average is 88. So, to find the total points she has across all five tests, I can multiply the average by the number of tests.\\n\\nLet me write that down:\\n\\nTotal points = Average  Number of tests\\nTotal points = 88  5\\n\\nLet me calculate that. 88 times 5 is... 440. So, the total points she has across all five tests is 440.\\n\\nNow, the question is asking for the lowest possible score she could have earned on one of these tests. To minimize one of the test scores, I need to maximize the other scores because the total is fixed at 440.\\n\\nBut there are constraints: each test can only have a score between 0 and 100, inclusive. So, the maximum score on any test is 100. If I try to maximize the other four test scores, each being 100, that would give me the maximum possible sum for four tests.\\n\\nLet me calculate that:\\n\\nMaximizing four tests:\\nMax sum = 100  4 = 400\\n\\nSo, if four tests are 100 each, that's 400 points. Then, the fifth test must make up the remaining points to reach the total of 440.\\n\\nLet me find the score for the fifth test:\\n\\nScore on fifth test = Total points - Max sum of four tests\\nScore = 440 - 400\\nScore = 40\\n\\nWait, so that means if four tests are 100 each, the fifth test must be 40? That seems low, but it's mathematically correct because 100 + 100 + 100 + 100 + 40 = 440.\\n\\nBut hold on, the question is asking for the lowest possible score on one of the tests. So, if I set the other four tests as high as possible (100 each), the fifth test would be 40. But is there a way to get a lower score? Let me think.\\n\\nIf I make one of the other tests lower than 100, does that allow the fifth test to be lower than 40? Wait, no, because if I lower one test, I have to raise the others to compensate, but they can't go above 100. So, actually, to minimize one test, you need to maximize the others as much as possible, which is 100 each. So, 40 is indeed the minimal possible score.\\n\\nBut just to double-check, let's think about it differently. Suppose I try to set four tests to 100, which gives me 400. The fifth test is 40. If I tried to make the fifth test lower, say 30, then the sum of the other four tests would have to be 440 - 30 = 410. But if I have four tests, each can be at most 100, so 100  4 is 400. 410 is more than 400, which isn't possible. So, it's not possible to have a lower score on the fifth test than 40 because the other four tests can't exceed 400.\\n\\nTherefore, 40 is the lowest possible score Alicia could have earned on one of the five tests.\\n\\nWait, but another thought: what if one test is 0? Would that allow the others to be higher? Let me see. If one test is 0, then the total of the other four tests would have to be 440 - 0 = 440. But each of those four tests can only go up to 100, so 100  4 is 400. But 440 is more than 400, which is impossible. Therefore, having a 0 on one test would require the others to sum to 440, which isn't possible since the maximum they can sum to is 400. So, 0 is too low.\\n\\nSo, 40 is indeed the lowest possible score because it's the only score that allows the other four tests to be 100 each without exceeding the maximum.\\n\\nAnother angle: Let's think about the formula for the average. The average is the total divided by the number of tests. So, if I want to minimize one test, I have to maximize the others, but they can't exceed 100. So, the maximum sum of four tests is 400, as we had before. Therefore, the minimal fifth test is 40.\\n\\nWait, let me make sure I'm not missing any other constraints. The problem says each test is between 0 and 100, inclusive. So, 0 is allowed, but in this case, if we tried to set four tests to 100, the fifth must be 40. If we set one test to 100, the others could be higher, but that doesn't help in minimizing a single test. Alternatively, if I set two tests to 100, the others can be higher, but again, that doesn't help.\\n\\nTherefore, the only way to minimize a single test score is to maximize the other four, which is 100 each. So, the minimal possible score is 40.\\n\\nI think that's solid reasoning. I don't see any other way to lower one test score without violating the test score range or the total points.\\n\\n**Final Answer**\\nThe lowest possible score Alicia could have earned on one of the five tests is \\\\boxed{40}.\\n</think>\\n\\nAlicia's average score on her five tests is 88 points. To find the total points she has across all five tests, we multiply the average by the number of tests:\\n\\n\\\\[\\n\\\\text{Total points} = 88 \\\\times 5 = 440\\n\\\\]\\n\\nTo find the lowest possible score on one of the tests, we need to maximize the scores of the other four tests. Since each test can score a maximum of 100, the maximum sum of four tests is:\\n\\n\\\\[\\n\\\\text{Max sum of four tests} = 100 \\\\times 4 = 400\\n\\\\]\\n\\nThe score on the fifth test must then be:\\n\\n\\\\[\\n\\\\text{Score on fifth test} = 440 - 400 = 40\\n\\\\]\\n\\nThus, the lowest possible score Alicia could have earned on one of the five tests is \\\\boxed{40}.\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_dataset[dataset_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "example_id = correct_dataset[dataset_pos][\"unique_id\"].replace(\"/\", \"_\")\n",
    "with open(f\"./temp/{example_id}\", \"w\") as f:\n",
    "    temp = {\n",
    "        \"generated_ids\": generated_ids.tolist(),\n",
    "        \"input_ids\": generated_tokens.tolist(),\n",
    "    }\n",
    "    json.dump(temp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\n",
    "    correct_dataset[dataset_pos][\"model_answer\"],\n",
    "    add_special_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_ids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(generated_ids[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(generated_tokens[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<beginofsentence><User>An elephant and a lion are currently 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour.  How many minutes will it take for the lion to catch the elephant?<Assistant><think>\\nOkay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards the elephant at 24 mph. So, their speeds are different, which means the distance between them will be changing over time. I need to find the time it takes for the lion to close that 1 mile gap.\\n\\nI remember that when two objects are moving towards each other, their relative speed is the sum of their individual speeds. But in this case, the elephant is moving away, so it's like the lion is chasing the elephant, but the elephant is moving away. So, maybe I should think of the relative speed as the lion's speed minus the elephant's speed? Wait, no, that might not be right.\\n\\nLet me clarify. If the lion is moving towards the elephant at 24 mph, and the elephant is moving away from the lion at 19 mph, then the lion is effectively closing the gap at a rate of 24 mph minus 19 mph. That makes sense because the lion is moving towards the elephant, but the elephant is moving away. So, the net speed at which the distance between them is decreasing is 24 - 19 = 5 mph.\\n\\nSo, the distance between them is decreasing at 5 miles per hour. They start 1 mile apart, so I can use the formula:\\n\\nTime = Distance / Speed\\n\\nHere, the distance is 1 mile, and the speed is 5 mph. So, time = 1 / 5 hours. But the question asks for the time in minutes, so I need to convert hours to minutes. There are 60 minutes in an hour, so 1/5 hours * 60 minutes/hour = 12 minutes.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph towards the elephant, and the elephant is moving away at 19 mph, then the relative speed is indeed 24 - 19 = 5 mph. So, the lion is gaining on the elephant at 5 miles per hour. Since they start 1 mile apart, the time it takes to close that distance is 1 / 5 hours, which is 12 minutes. That seems right.\\n\\nBut just to make sure, let me think about it another way. Maybe set up an equation for their positions as functions of time and see when they meet.\\n\\nLet me denote t as the time in hours. The position of the elephant as a function of time would be starting from some point and moving away at 19 mph. Let's say the initial position of the elephant is at position 0, and the lion is at position 1 mile. Then, the position of the elephant at time t is 0 + 19t, and the position of the lion is 1 - 24t. They meet when their positions are equal, so:\\n\\n19t = 1 - 24t\\n\\nAdding 24t to both sides:\\n\\n19t + 24t = 1\\n\\n43t = 1\\n\\nt = 1/43 hours\\n\\nWait, that's different from what I got before. Hmm, so which one is correct? 12 minutes or 1/43 hours?\\n\\nWait, 1/43 hours is approximately 1.395 minutes, which is about 1 minute and 23 seconds. But earlier, I got 12 minutes. That's a big difference. So, I must have made a mistake somewhere.\\n\\nLet me go back. When I thought about the relative speed, I considered the lion moving towards the elephant and the elephant moving away, so the relative speed is 24 - 19 = 5 mph. So, the time should be 1/5 hours, which is 12 minutes. But when I set up the equations, I got t = 1/43 hours, which is about 1.395 minutes. That's conflicting.\\n\\nWait, maybe I messed up the initial positions. Let me define the positions more carefully. Let's say the lion starts at position 0, and the elephant starts at position 1 mile. Then, the lion is moving towards the elephant, so its position as a function of time is 0 + 24t. The elephant is moving away from the lion, so its position is 1 + 19t. They meet when their positions are equal:\\n\\n24t = 1 + 19t\\n\\nSubtracting 19t from both sides:\\n\\n5t = 1\\n\\nt = 1/5 hours, which is 12 minutes. Okay, that makes sense. So, why did I get 1/43 hours earlier?\\n\\nWait, in my second approach, I set the lion's position as 1 - 24t, but that might be incorrect. If the lion starts at position 0, moving towards the elephant, which is at position 1, then the lion's position is 24t. The elephant is moving away from the lion, so its position is 1 + 19t. So, setting them equal:\\n\\n24t = 1 + 19t\\n\\nWhich gives t = 1/5 hours, which is 12 minutes. So, that seems correct.\\n\\nWait, so why did I get 1/43 hours earlier? Maybe I confused the starting positions. Let me re-examine that.\\n\\nIn the second approach, I set the lion's position as 1 - 24t, which would be incorrect because if the lion is moving towards the elephant, starting from position 0, its position should be increasing, not decreasing. So, that was my mistake. I should have set the lion's position as 24t, not 1 - 24t.\\n\\nSo, correcting that, the correct equation is 24t = 1 + 19t, leading to t = 1/5 hours, which is 12 minutes. So, that's consistent with the first method.\\n\\nTherefore, the correct answer is 12 minutes.\\n\\nWait, but just to make sure, let me think about it in another way. Suppose the lion is moving at 24 mph towards the elephant, and the elephant is\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\\n\\nFirst, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards the elephant at 24 mph. So, their speeds are different, which means the distance between them is changing over time. I need to find the time it takes for the lion to close that 1 mile gap.\\n\\nI remember that when two objects are moving towards each other, their relative speed is the sum of their individual speeds. But in this case, the elephant is moving away, so it's like the lion is chasing the elephant, but the elephant is moving away. So, maybe I should consider the relative speed between the lion and the elephant.\\n\\nWait, actually, since the elephant is moving away, the lion has to cover the initial distance plus any additional distance the elephant might cover while the lion is chasing. But since the lion is faster, it should eventually catch up. So, maybe I can model this as a relative speed problem.\\n\\nLet me denote the speed of the lion as \\\\( v_l = 24 \\\\) mph and the speed of the elephant as \\\\( v_e = 19 \\\\) mph. The initial distance between them is \\\\( d = 1 \\\\) mile.\\n\\nSince the lion is moving towards the elephant and the elephant is moving away, the relative speed at which the distance between them is decreasing is the difference between the lion's speed and the elephant's speed. So, the relative speed \\\\( v_{relative} = v_l - v_e = 24 - 19 = 5 \\\\) mph.\\n\\nWait, is that right? If the lion is moving towards the elephant and the elephant is moving away, then the lion is effectively closing the gap at a rate of 5 mph. So, the time it takes to close the 1 mile gap would be the initial distance divided by the relative speed.\\n\\nSo, time \\\\( t = \\\\frac{d}{v_{relative}} = \\\\frac{1}{5} \\\\) hours. But the question asks for the time in minutes, so I need to convert that.\\n\\nSince 1 hour is 60 minutes, \\\\( \\\\frac{1}{5} \\\\) hours is \\\\( \\\\frac{1}{5} \\\\times 60 = 12 \\\\) minutes. So, it should take 12 minutes for the lion to catch the elephant.\\n\\nWait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, then the lion is gaining on the elephant at a rate of 5 mph. So, to cover the 1 mile gap, it would take \\\\( \\\\frac{1}{5} \\\\) hours, which is indeed 12 minutes. That seems correct.\\n\\nAlternatively, I can model this with equations. Let me set up a coordinate system where at time \\\\( t \\\\) hours, the position of the lion is \\\\( 24t \\\\) miles from its starting point, and the position of the elephant is \\\\( 1 + 19t \\\\) miles from the starting point of the lion. Wait, actually, hold on. If the elephant is moving away from the lion, then the distance between them is increasing. So, the position of the elephant is \\\\( 1 + 19t \\\\) miles from the starting point of the lion, and the position of the lion is \\\\( 24t \\\\) miles from the same starting point.\\n\\nWait, but initially, they are 1 mile apart. So, if the lion starts at position 0 and the elephant starts at position 1 mile, then the distance between them at time \\\\( t \\\\) is \\\\( |24t - (1 + 19t)| \\\\). We want this distance to be 0 when the lion catches the elephant.\\n\\nSo, setting up the equation:\\n\\n\\\\( 24t - (1 + 19t) = 0 \\\\)\\n\\nSimplify:\\n\\n\\\\( 24t - 19t - 1 = 0 \\\\)\\n\\n\\\\( 5t - 1 = 0 \\\\)\\n\\n\\\\( 5t = 1 \\\\)\\n\\n\\\\( t = \\\\frac{1}{5} \\\\) hours, which is 12 minutes. So, that confirms my earlier result.\\n\\nAlternatively, I can think about it in terms of distance covered. The lion needs to cover the initial 1 mile plus whatever distance the elephant covers in that time. But since the lion is faster, the time is determined by the relative speed.\\n\\nWait, another way to think about it is using the concept of relative velocity. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the relative speed is 24 - 19 = 5 mph. So, the lion is closing the gap at 5 mph. Therefore, the time to close 1 mile is 1/5 hours, which is 12 minutes.\\n\\nI think that's solid. I can't see any mistakes in this reasoning. So, I think the answer is 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. To determine how long it will take for the lion to catch the elephant, we need to consider their relative speed.\\n\\nThe relative speed at which the lion is closing the gap is the difference between their speeds:\\n\\\\[ v_{\\\\text{relative}} = v_l - v_e = 24 \\\\text{ mph} - 19 \\\\text{ mph} = 5 \\\\text{ mph} \\\\]\\n\\nThe time it takes to close the 1 mile gap is calculated by dividing the initial distance by the relative speed:\\n\\\\[ t = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = \\\\frac{1}{5} \\\\text{ hours} \\\\]\\n\\nConverting this time into minutes:\\n\\\\[ \\\\frac{1}{5} \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes} \\\\]\\n\\nThus, the lion will catch the elephant in \\\\boxed{12} minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If $A=2+i$, $O=-4$, $P=-i$, and $S=2+4i$, find $A-O+P+S$.\n",
      "===\n",
      "===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPRESSED PART Okay, so I have this problem here where I need to find the value of A minus O plus P plus S. The given values are A = 2 + i, O = -4, P = -i, and S = 2 + 4i. Hmm, let me write that down to make it clearer.\n",
      "\n",
      "First, let me write down each term:\n",
      "\n",
      "A = 2 + i\n",
      "\n",
      "O = -4\n",
      "\n",
      "P = -i\n",
      "\n",
      "S = 2 + 4i\n",
      "\n",
      "And the expression I need to compute is A - O + P + S. Hmm, okay. So, I think I need to substitute these values into the expression and then simplify it step by step.\n",
      "\n",
      "Let me start by substituting the values:\n",
      "\n",
      "A - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\n",
      "\n",
      "Wait, hold on. Let me make sure I substitute correctly. So, A is 2\n",
      "===\n",
      " + i, then subtract O which is -4, then add P which is -i, then add S which is 2 + 4i.\n",
      "\n",
      "So, writing it out: (2 + i) - (-4) + (-i) + (2 + 4i)\n",
      "\n",
      "Now, let's simplify this step by step. I think the best way is to handle the real parts and the imaginary parts separately. That is, combine all the constants without the i and then combine all the terms with i.\n",
      "\n",
      "First, let's handle the real parts:\n",
      "\n",
      "We have 2 (from A), then -(-4) which is +4, then 2 (from S). So, 2 + 4 + 2.\n",
      "\n",
      "Let me calculate that: 2 + 4 is 6, plus 2 is 8. So, the real part is 8.\n",
      "\n",
      "Now, the imaginary parts:\n",
      "\n",
      "From A, we have i, from O, we have -i, and from S, we have 4i. So, let's write that out:\n",
      "\n",
      "i - i + 4i\n",
      "\n",
      "Let me simplify that:\n",
      "\n",
      "i - i is 0, and then 0 + 4i is 4i. So, the imaginary part is 4i.\n",
      "\n",
      "Putting it all together, the expression simplifies to 8 + 4i.\n",
      "\n",
      "Wait, let me double-check my steps to make sure I didn't make a mistake.\n",
      "\n",
      "Starting with A - O + P + S:\n",
      "\n",
      "A is 2 + i, O is -4, P is -i, S is 2 + 4i.\n",
      "\n",
      "So, substituting:\n",
      "\n",
      "(2 + i) - (-4) + (-i) + (2 + 4i)\n",
      "\n",
      "Simplify each term:\n",
      "\n",
      "First term: 2 + i\n",
      "\n",
      "Second term: -(-4) is +4\n",
      "\n",
      "Third term: -i\n",
      "\n",
      "Fourth term: 2 + 4i\n",
      "\n",
      "Now, combine all the real parts: 2 + 4 + 2 = 8\n",
      "\n",
      "Combine all the imaginary parts: i - i + 4i = (1 - 1 + 4)i = 4i\n",
      "\n",
      "So, 8 + 4i is the result.\n",
      "\n",
      "Wait, but let me make sure I didn't miss any signs. Sometimes, when subtracting a negative, it's easy to flip the sign. So, A - O is (2 + i) - (-4). Subtracting a negative is adding the positive, so that becomes 2 + i + 4, which is 6 + i.\n",
      "\n",
      "Then, adding P which is -i: 6 + i - i = 6 + 0i.\n",
      "\n",
      "Then, adding S which is 2 + 4i: 6 + 0i + 2 + 4i = 8 + 4i.\n",
      "\n",
      "Yes, that's the same result as before. So, that seems consistent.\n",
      "\n",
      "Alternatively, I can think of it as:\n",
      "\n",
      "A - O + P + S = (A + P) + (S - O)\n",
      "\n",
      "Let me try that approach to see if I get the same result.\n",
      "\n",
      "First, compute A + P:\n",
      "\n",
      "A is 2 + i, P is -i.\n",
      "\n",
      "So, 2 + i + (-i) = 2 + 0i = 2\n",
      "\n",
      "Then, compute S - O:\n",
      "\n",
      "S is 2 + 4i, O is -4.\n",
      "\n",
      "So, 2 + 4i - (-4) = 2 + 4i + 4 = 6 + 4i\n",
      "\n",
      "Now, add the two results: 2 + (6 + 4i) = 8 + 4i\n",
      "\n",
      "Same result. Good, that's consistent.\n",
      "\n",
      "Another way to check is to convert all the complex numbers to their standard form and then perform the operations.\n",
      "\n",
      "A is 2 + i\n",
      "\n",
      "O is -4 + 0i\n",
      "\n",
      "P is 0 - i\n",
      "\n",
      "S is 2 + 4i\n",
      "\n",
      "So, substituting:\n",
      "\n",
      "(2 + i) - (-4 + 0i) + (0 - i) + (2 + 4i)\n",
      "\n",
      "Simplify each term:\n",
      "\n",
      "First term: 2 + i\n",
      "\n",
      "Second term: -(-4 + 0i) = 4 + 0i\n",
      "\n",
      "Third term: 0 - i\n",
      "\n",
      "Fourth term: 2 + 4i\n",
      "\n",
      "Now, combine all the real parts: 2 + 4 + 0 + 2 = 8\n",
      "\n",
      "Combine all the imaginary parts: i + 0i - i + 4i = (1 - 1 + 4)i = 4i\n",
      "\n",
      "So, again, 8 + 4i.\n",
      "\n",
      "Okay, so regardless of the method I use, I keep getting 8 + 4i. That gives me confidence that the answer is correct.\n",
      "\n",
      "Just to be absolutely sure, let me write down each step again:\n",
      "\n",
      "1. Substitute the given values into the expression:\n",
      "\n",
      "A - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\n",
      "\n",
      "2. Simplify each term:\n",
      "\n",
      "- (2 + i) becomes +4\n",
      "- (-i) remains as -i\n",
      "- (2 + 4i) remains as is\n",
      "\n",
      "So, the expression becomes:\n",
      "\n",
      "2 + i + 4 - i + 2 + 4i\n",
      "\n",
      "3. Combine like terms:\n",
      "\n",
      "Real parts: 2 + 4 + 2 = 8\n",
      "\n",
      "Imaginary parts: i - i + 4i = 4i\n",
      "\n",
      "4. Combine them: 8 + 4i\n",
      "\n",
      "Yep, that's consistent.\n",
      "\n",
      "I think I've checked it enough ways. So, the final answer should be 8 + 4i.\n",
      "\n",
      "**Final Answer**\n",
      "The value of \\( A - O + P + S \\) is \\boxed{8 + 4i}.\n",
      "</think>\n",
      "\n",
      "Given the complex numbers \\( A = 2 + i \\), \\( O = -4 \\), \\( P = -i \\), and \\( S = 2 + 4i \\), we need to find the value of \\( A - O + P + S \\).\n",
      "\n",
      "First, substitute the given values into the expression:\n",
      "\n",
      "\\[\n",
      "A - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\n",
      "\\]\n",
      "\n",
      "Simplify each term:\n",
      "\n",
      "- \\( (2 + i) \\) remains as is.\n",
      "- \\( -(-4) \\) becomes \\( +4 \\).\n",
      "- \\( -i \\) remains as is.\n",
      "- \\( (2 + 4i) \\) remains as is.\n",
      "\n",
      "So, the expression becomes:\n",
      "\n",
      "\\[\n",
      "(2 + i) + 4 - i + (2 + 4i)\n",
      "\\]\n",
      "\n",
      "Combine the real parts and the imaginary parts separately:\n",
      "\n",
      "- Real parts: \\( 2 + 4 + 2 = 8 \\)\n",
      "- Imaginary parts: \\( i - i + 4i = 4i \\)\n",
      "\n",
      "Putting it all together, the expression simplifies to:\n",
      "\n",
      "\\[\n",
      "8 + 4i\n",
      "\\]\n",
      "\n",
      "Thus, the value of \\( A - O + P + S \\) is \\(\\boxed{8 + 4i}\\).<endofsentence>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    WINDOW_SIZE,\n",
    "    VISION_START,\n",
    "    VISION_END,\n",
    "    EOS_TOKEN_ID,\n",
    ")\n",
    "import torch\n",
    "import json\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "dataset_pos = 0\n",
    "input_ids = correct_dataset[dataset_pos][\"problem\"]\n",
    "print(input_ids)\n",
    "print(\"===\")\n",
    "print(\"===\")\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "input_ids = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": base_prompt.format(question=input_ids),\n",
    "            },\n",
    "        ],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "# input_ids = dataset_item[\"input_ids\"]\n",
    "# generated_ids = dataset_item[\"generated_ids\"]\n",
    "generated_ids = [\n",
    "    tokenizer.encode(\n",
    "        correct_dataset[dataset_pos][\"model_answer\"],\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\"\n",
    "\n",
    "# generated_tokens = tokenizer.apply_chat_template(\n",
    "#     [\n",
    "#         # {\"role\": \"user\", \"content\": \"how many wings has a bird?\"},\n",
    "#         {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "#     ],\n",
    "#     tokenize=True,\n",
    "#     add_generation_prompt=True,\n",
    "# )\n",
    "generated_tokens = input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "        # start_embed = model.base_model.embed_pooler.model.embed_tokens.modules_to_save.default(\n",
    "        torch.tensor([[VISION_START]], device=\"cuda\")\n",
    "    )\n",
    "    end_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "        # end_embed = model.base_model.embed_pooler.model.embed_tokens.modules_to_save.default(\n",
    "        torch.tensor([[VISION_END]], device=\"cuda\")\n",
    "    )\n",
    "    input_ids = torch.tensor(input_ids).cuda()\n",
    "    input_ids_embeds = model.get_input_embeddings()(input_ids)\n",
    "    windows_amount = 100\n",
    "    # windows_amount = 200\n",
    "    # windows_amount = 300\n",
    "    # windows_amount = 400\n",
    "    # windows_amount = 500\n",
    "    # windows_amount = 2\n",
    "    next_true_tokens = torch.tensor(generated_ids, device=\"cuda\")[\n",
    "        :, : WINDOW_SIZE * windows_amount\n",
    "    ]\n",
    "\n",
    "    # next_true_tokens = torch.tensor(next_true_tokens, device=\"cuda\")\n",
    "\n",
    "    original_embeds = (\n",
    "        # model.base_model.embed_pooler.model.get_input_embeddings()(next_true_tokens)\n",
    "        # model.base_model.model.get_input_embeddings()(next_true_tokens)\n",
    "        model.get_input_embeddings()(next_true_tokens)\n",
    "    ).to(torch.float32)\n",
    "    # ).to(torch.bfloat16)\n",
    "\n",
    "    # compressed_part = model.base_model.embed_pooler(new_embeds_for_compression)\n",
    "    new_embeds_for_compression = original_embeds.reshape(\n",
    "        windows_amount, WINDOW_SIZE, -1\n",
    "    )\n",
    "    compressed_part = model.base_model.embed_pooler(new_embeds_for_compression)\n",
    "    compressed_part = compressed_part.reshape(1, windows_amount, -1)\n",
    "    # start_embed = torch.rand_like(start_embed)\n",
    "    # compressed_part = torch.rand_like(compressed_part)\n",
    "    # end_embed = torch.rand_like(end_embed)\n",
    "    generated_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds,\n",
    "            start_embed,\n",
    "            compressed_part,\n",
    "            end_embed,\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    print(\"COMPRESSED PART\", tokenizer.decode(next_true_tokens[-1]))\n",
    "    print(\"===\")\n",
    "    generated_ids_compressed = model.generate(\n",
    "        inputs_embeds=generated_embeds,\n",
    "        max_new_tokens=2800,\n",
    "        # max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "        # do_sample=True,\n",
    "        # temperature=0.6,\n",
    "        # top_p=0.95,\n",
    "    )\n",
    "    # break\n",
    "print(tokenizer.decode(generated_ids_compressed[-1]))\n",
    "# break\n",
    "# embeds_generation_tokens = generated_tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'If $A=2+i$, $O=-4$, $P=-i$, and $S=2+4i$, find $A-O+P+S$.',\n",
       " 'solution': 'Adding real parts and imaginary parts separately, we have $(2-(-4)+0+2)+(1+0-1+4)i=\\\\boxed{8+4i}$.',\n",
       " 'answer': '8+4i',\n",
       " 'subject': 'Algebra',\n",
       " 'level': 3,\n",
       " 'unique_id': 'train/algebra/3.json',\n",
       " 'model_answer': \"Okay, so I have this problem here where I need to find the value of A minus O plus P plus S. The given values are A = 2 + i, O = -4, P = -i, and S = 2 + 4i. Hmm, let me write that down to make it clearer.\\n\\nFirst, let me write down each term:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nAnd the expression I need to compute is A - O + P + S. Hmm, okay. So, I think I need to substitute these values into the expression and then simplify it step by step.\\n\\nLet me start by substituting the values:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nWait, hold on. Let me make sure I substitute correctly. So, A is 2 + i, O is -4, P is -i, and S is 2 + 4i. So, replacing each term:\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nOkay, that looks right. Now, let me simplify this step by step. I think I should handle the operations one at a time.\\n\\nFirst, let's handle the subtraction of O, which is -(-4). Subtracting a negative is the same as adding a positive, so:\\n\\n(2 + i) - (-4) = (2 + i) + 4\\n\\nSo, that simplifies to 2 + 4 + i, which is 6 + i.\\n\\nAlright, so now the expression becomes:\\n\\n6 + i + (-i) + (2 + 4i)\\n\\nWait, let me write that again:\\n\\n= (6 + i) + (-i) + (2 + 4i)\\n\\nNow, let's combine the real parts and the imaginary parts separately.\\n\\nFirst, the real parts: 6, 0 (since there's no real part in -i), and 2.\\n\\nSo, 6 + 0 + 2 = 8.\\n\\nNow, the imaginary parts: i, -i, and 4i.\\n\\nSo, let's add those up:\\n\\ni - i + 4i\\n\\nHmm, i minus i is 0, so that leaves us with 4i.\\n\\nSo, putting it all together, the real part is 8 and the imaginary part is 4i, so the result is 8 + 4i.\\n\\nWait, let me double-check that. Maybe I made a mistake in the signs somewhere.\\n\\nStarting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, handle the subtraction:\\n\\n(2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P: 6 + i + (-i) = 6 + i - i = 6\\n\\nThen, add S: 6 + (2 + 4i) = 8 + 4i\\n\\nYes, that seems consistent. So, the final result is 8 + 4i.\\n\\nWait, hold on, let me make sure I didn't skip any steps or make a mistake in the signs.\\n\\nSo, starting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, compute (2 + i) - (-4):\\n\\nSubtracting a negative is adding, so 2 + i + 4 = 6 + i.\\n\\nThen, add P, which is -i:\\n\\n6 + i + (-i) = 6 + (i - i) = 6 + 0 = 6.\\n\\nThen, add S, which is 2 + 4i:\\n\\n6 + (2 + 4i) = (6 + 2) + 4i = 8 + 4i.\\n\\nYes, that seems correct. So, the final answer is 8 + 4i.\\n\\nAlternatively, maybe I can approach this problem by grouping the real and imaginary parts differently.\\n\\nLet me try that.\\n\\nGiven:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nSo, A - O + P + S can be rewritten as:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nLet me group the real numbers and the imaginary numbers separately.\\n\\nReal parts: 2, 0 (since O is -4, which is real), -0 (since P is -i, which is imaginary), and 2.\\n\\nImaginary parts: 1 (from A), 0 (from O), -1 (from P), and 4 (from S).\\n\\nSo, real parts: 2 + 0 + 0 + 2 = 4\\n\\nImaginary parts: 1 + 0 - 1 + 4 = 4\\n\\nWait, hold on, that doesn't seem right. Because when I grouped them earlier, I had 8 + 4i, but now grouping separately, I get 4 + 4i. That's a discrepancy. Hmm, so I must have made a mistake in my grouping.\\n\\nWait, let me check again.\\n\\nWait, no, actually, in the expression A - O + P + S, O is a real number, so it's just -(-4) which is +4. So, in the real parts, we have 2 (from A) + 4 (from -O) + 0 (from P) + 2 (from S). So, 2 + 4 + 2 = 8.\\n\\nImaginary parts: 1 (from A) + 0 (from -O) -1 (from P) + 4 (from S). So, 1 -1 + 4 = 4.\\n\\nSo, that gives 8 + 4i, which matches my previous result.\\n\\nWait, so why did I get 4 + 4i when I grouped them? Because I think I miscounted the real parts.\\n\\nWait, let me write it step by step.\\n\\nReal parts:\\n\\nFrom A: 2\\n\\nFrom -O: -(-4) = +4\\n\\nFrom P: 0 (since P is -i, which is purely imaginary)\\n\\nFrom S: 2\\n\\nSo, total real parts: 2 + 4 + 0 + 2 = 8\\n\\nImaginary parts:\\n\\nFrom A: 1\\n\\nFrom -O: 0 (since O is real)\\n\\nFrom P: -1\\n\\nFrom S: 4\\n\\nTotal imaginary parts: 1 + 0 -1 + 4 = 4\\n\\nSo, 8 + 4i. That's correct.\\n\\nSo, my initial grouping was wrong because I didn't account for all the real parts correctly. So, the correct answer is 8 + 4i.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = A + (-O) + P + S\\n\\nSo, substituting:\\n\\n= (2 + i) + (4) + (-i) + (2 + 4i)\\n\\nThen, combine like terms:\\n\\nReal parts: 2 + 4 + 2 = 8\\n\\nImaginary parts: i - i + 4i = 4i\\n\\nSo, 8 + 4i.\\n\\nYes, that's consistent.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = (A + P) + (-O + S)\\n\\nCompute A + P first:\\n\\nA = 2 + i\\n\\nP = -i\\n\\nSo, A + P = 2 + i + (-i) = 2 + 0i = 2\\n\\nThen, compute -O + S:\\n\\nO = -4, so -O = 4\\n\\nS = 2 + 4i\\n\\nSo, -O + S = 4 + (2 + 4i) = 6 + 4i\\n\\nNow, add A + P and -O + S:\\n\\n2 + (6 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, regardless of the method, I get 8 + 4i.\\n\\nTherefore, I think that's the correct answer.\\n\\nWait, just to make sure, let me compute each step numerically.\\n\\nCompute A - O:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nSo, A - O = (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P:\\n\\n6 + i + (-i) = 6 + 0i = 6\\n\\nThen, add S:\\n\\n6 + (2 + 4i) = 8 + 4i\\n\\nYep, same result.\\n\\nAlternatively, if I compute A - O + P + S step by step:\\n\\nStart with A: 2 + i\\n\\nSubtract O: 2 + i - (-4) = 2 + i + 4 = 6 + i\\n\\nAdd P: 6 + i + (-i) = 6 + 0i = 6\\n\\nAdd S: 6 + (2 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, I think I can confidently say that the answer is 8 + 4i.\\n\\nJust to recap, the steps are:\\n\\n1. Substitute each term into the expression.\\n\\n2. Simplify the expression by handling the operations step by step.\\n\\n3. Combine like terms (real and imaginary parts separately).\\n\\n4. Verify the result using different methods to ensure accuracy.\\n\\nSo, yeah, 8 + 4i is the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( A - O + P + S \\\\) is \\\\boxed{8 + 4i}.\\n</think>\\n\\nGiven the values \\\\( A = 2 + i \\\\), \\\\( O = -4 \\\\), \\\\( P = -i \\\\), and \\\\( S = 2 + 4i \\\\), we need to find the value of \\\\( A - O + P + S \\\\).\\n\\nFirst, substitute the given values into the expression:\\n\\n\\\\[\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\\\]\\n\\nSimplify each term step by step:\\n\\n1. Subtract \\\\( O \\\\) from \\\\( A \\\\):\\n   \\\\[\\n   (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n   \\\\]\\n\\n2. Add \\\\( P \\\\) to the result:\\n   \\\\[\\n   6 + i + (-i) = 6 + 0i = 6\\n   \\\\]\\n\\n3. Add \\\\( S \\\\) to the result:\\n   \\\\[\\n   6 + (2 + 4i) = 8 + 4i\\n   \\\\]\\n\\nThus, the value of \\\\( A - O + P + S \\\\) is \\\\(\\\\boxed{8 + 4i}\\\\).\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_dataset[dataset_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_true_tokens.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1621, 200, 1721, torch.Size([1, 2356]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids_compressed.shape[1], next_true_tokens.shape[\n",
    "    1\n",
    "], generated_ids_compressed.shape[1] + compressed_part.shape[1], torch.tensor(\n",
    "    generated_ids\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this problem here where I need to find the value of A minus O plus P plus S. The given values are A = 2 + i, O = -4, P = -i, and S = 2 + 4i. Hmm, let me write that down to make it clearer.\\n\\nFirst, let me write down each term:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nAnd the expression I need to compute is A - O + P + S. Hmm, okay. So, I think I need to substitute these values into the expression and then simplify it step by step.\\n\\nLet me start by substituting the values:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nWait, hold on. Let me make sure I substitute correctly. So, A is 2 + i, O is -4, P is -i, and S is 2 + 4i. So, replacing each term:\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nOkay, that looks right. Now, let me simplify this step by step. I think I should handle the operations one at a time.\\n\\nFirst, let's handle the subtraction of O, which is -(-4). Subtracting a negative is the same as adding a positive, so:\\n\\n(2 + i) - (-4) = (2 + i) + 4\\n\\nSo, that simplifies to 2 + 4 + i, which is 6 + i.\\n\\nAlright, so now the expression becomes:\\n\\n6 + i + (-i) + (2 + 4i)\\n\\nWait, let me write that again:\\n\\n= (6 + i) + (-i) + (2 + 4i)\\n\\nNow, let's combine the real parts and the imaginary parts separately.\\n\\nFirst, the real parts: 6, 0 (since there's no real part in -i), and 2.\\n\\nSo, 6 + 0 + 2 = 8.\\n\\nNow, the imaginary parts: i, -i, and 4i.\\n\\nSo, let's add those up:\\n\\ni - i + 4i\\n\\nHmm, i minus i is 0, so that leaves us with 4i.\\n\\nSo, putting it all together, the real part is 8 and the imaginary part is 4i, so the result is 8 + 4i.\\n\\nWait, let me double-check that. Maybe I made a mistake in the signs somewhere.\\n\\nStarting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, handle the subtraction:\\n\\n(2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P: 6 + i + (-i) = 6 + i - i = 6\\n\\nThen, add S: 6 + (2 + 4i) = 8 + 4i\\n\\nYes, that seems consistent. So, the final result is 8 + 4i.\\n\\nWait, hold on, let me make sure I didn't skip any steps or make a mistake in the signs.\\n\\nSo, starting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, compute (2 + i) - (-4):\\n\\nSubtracting a negative is adding, so 2 + i + 4 = 6 + i.\\n\\nThen, add P, which is -i:\\n\\n6 + i + (-i) = 6 + (i - i) = 6 + 0 = 6.\\n\\nThen, add S, which is 2 + 4i:\\n\\n6 + (2 + 4i) = (6 + 2) + 4i = 8 + 4i.\\n\\nYes, that seems correct. So, the final answer is 8 + 4i.\\n\\nAlternatively, maybe I can approach this problem by grouping the real and imaginary parts differently.\\n\\nLet me try that.\\n\\nGiven:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nSo, A - O + P + S can be rewritten as:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nLet me group the real numbers and the imaginary numbers separately.\\n\\nReal parts: 2, 0 (since O is -4, which is real), -0 (since P is -i, which is imaginary), and 2.\\n\\nImaginary parts: 1 (from A), 0 (from O), -1 (from P), and 4 (from S).\\n\\nSo, real parts: 2 + 0 + 0 + 2 = 4\\n\\nImaginary parts: 1 + 0 - 1 + 4 = 4\\n\\nWait, hold on, that doesn't seem right. Because when I grouped them earlier, I had 8 + 4i, but now grouping separately, I get 4 + 4i. That's a discrepancy. Hmm, so I must have made a mistake in my grouping.\\n\\nWait, let me check again.\\n\\nWait, no, actually, in the expression A - O + P + S, O is a real number, so it's just -(-4) which is +4. So, in the real parts, we have 2 (from A) + 4 (from -O) + 0 (from P) + 2 (from S). So, 2 + 4 + 2 = 8.\\n\\nImaginary parts: 1 (from A) + 0 (from -O) -1 (from P) + 4 (from S). So, 1 -1 + 4 = 4.\\n\\nSo, that gives 8 + 4i, which matches my previous result.\\n\\nWait, so why did I get 4 + 4i when I grouped them? Because I think I miscounted the real parts.\\n\\nWait, let me write it step by step.\\n\\nReal parts:\\n\\nFrom A: 2\\n\\nFrom -O: -(-4) = +4\\n\\nFrom P: 0 (since P is -i, which is purely imaginary)\\n\\nFrom S: 2\\n\\nSo, total real parts: 2 + 4 + 0 + 2 = 8\\n\\nImaginary parts:\\n\\nFrom A: 1\\n\\nFrom -O: 0 (since O is real)\\n\\nFrom P: -1\\n\\nFrom S: 4\\n\\nTotal imaginary parts: 1 + 0 -1 + 4 = 4\\n\\nSo, 8 + 4i. That's correct.\\n\\nSo, my initial grouping was wrong because I didn't account for all the real parts correctly. So, the correct answer is 8 + 4i.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = A + (-O) + P + S\\n\\nSo, substituting:\\n\\n= (2 + i) + (4) + (-i) + (2 + 4i)\\n\\nThen, combine like terms:\\n\\nReal parts: 2 + 4 + 2 = 8\\n\\nImaginary parts: i - i + 4i = 4i\\n\\nSo, 8 + 4i.\\n\\nYes, that's consistent.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = (A + P) + (-O + S)\\n\\nCompute A + P first:\\n\\nA = 2 + i\\n\\nP = -i\\n\\nSo, A + P = 2 + i + (-i) = 2 + 0i = 2\\n\\nThen, compute -O + S:\\n\\nO = -4, so -O = 4\\n\\nS = 2 + 4i\\n\\nSo, -O + S = 4 + (2 + 4i) = 6 + 4i\\n\\nNow, add A + P and -O + S:\\n\\n2 + (6 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, regardless of the method, I get 8 + 4i.\\n\\nTherefore, I think that's the correct answer.\\n\\nWait, just to make sure, let me compute each step numerically.\\n\\nCompute A - O:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nSo, A - O = (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P:\\n\\n6 + i + (-i) = 6 + 0i = 6\\n\\nThen, add S:\\n\\n6 + (2 + 4i) = 8 + 4i\\n\\nYep, same result.\\n\\nAlternatively, if I compute A - O + P + S step by step:\\n\\nStart with A: 2 + i\\n\\nSubtract O: 2 + i - (-4) = 2 + i + 4 = 6 + i\\n\\nAdd P: 6 + i + (-i) = 6 + 0i = 6\\n\\nAdd S: 6 + (2 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, I think I can confidently say that the answer is 8 + 4i.\\n\\nJust to recap, the steps are:\\n\\n1. Substitute each term into the expression.\\n\\n2. Simplify the expression by handling the operations step by step.\\n\\n3. Combine like terms (real and imaginary parts separately).\\n\\n4. Verify the result using different methods to ensure accuracy.\\n\\nSo, yeah, 8 + 4i is the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( A - O + P + S \\\\) is \\\\boxed{8 + 4i}.\\n</think>\\n\\nGiven the values \\\\( A = 2 + i \\\\), \\\\( O = -4 \\\\), \\\\( P = -i \\\\), and \\\\( S = 2 + 4i \\\\), we need to find the value of \\\\( A - O + P + S \\\\).\\n\\nFirst, substitute the given values into the expression:\\n\\n\\\\[\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\\\]\\n\\nSimplify each term step by step:\\n\\n1. Subtract \\\\( O \\\\) from \\\\( A \\\\):\\n   \\\\[\\n   (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n   \\\\]\\n\\n2. Add \\\\( P \\\\) to the result:\\n   \\\\[\\n   6 + i + (-i) = 6 + 0i = 6\\n   \\\\]\\n\\n3. Add \\\\( S \\\\) to the result:\\n   \\\\[\\n   6 + (2 + 4i) = 8 + 4i\\n   \\\\]\\n\\nThus, the value of \\\\( A - O + P + S \\\\) is \\\\(\\\\boxed{8 + 4i}\\\\).\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_dataset[0][\"model_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, so I have this problem here where I need to find the value of A minus O plus P plus S. The given values are A = 2 + i, O = -4, P = -i, and S = 2 + 4i. Hmm, let me write that down to make it clearer.\\n\\nFirst, let me write down each term:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nAnd the expression I need to compute is A - O + P + S. Hmm, okay. So, I think I need to substitute these values into the expression and then simplify it step by step.\\n\\nLet me start by substituting the values:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nWait, hold on. Let me make sure I substitute correctly. So, A is 2'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next_true_tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "# \"Okay, so I have this problem here where I need to find the value of A minus O plus P plus S. The given values are A = 2 + i, O = -4, P = -i, and S = 2 + 4i. Hmm, let me write that down to make it clearer.\\n\\nFirst, let me write down each term:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nAnd the expression I need to compute is A - O + P + S. Hmm, okay. So, I think I need to substitute these values into the expression and then simplify it step by step.\\n\\nLet me start by substituting the values:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nWait, hold on. Let me make sure I substitute correctly. So, A is 2 + i, O is -4, P is -i, and S is 2 + 4i. So, replacing each term:\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nOkay, that looks right. Now, let me simplify this step by step. I think I should handle the operations one at a time.\\n\\nFirst, let's handle the subtraction of O, which is -(-4). Subtracting a negative is the same as adding a positive, so:\\n\\n(2 + i) - (-4) = (2 + i) + 4\\n\\nSo, that simplifies to 2 + 4 + i, which is 6 + i.\\n\\nAlright, so now the expression becomes:\\n\\n6 + i + (-i) + (2 + 4i)\\n\\nWait, let me write that again:\\n\\n= (6 + i) + (-i) + (2 + 4i)\\n\\nNow, let's combine the real parts and the imaginary parts separately.\\n\\nFirst, the real parts: 6, 0 (since there's no real part in -i), and 2.\\n\\nSo, 6 + 0 + 2 = 8.\\n\\nNow, the imaginary parts: i, -i, and 4i.\\n\\nSo, let's add those up:\\n\\ni - i + 4i\\n\\nHmm, i minus i is 0, so that leaves us with 4i.\\n\\nSo, putting it all together, the real part is 8 and the imaginary part is 4i, so the result is 8 + 4i.\\n\\nWait, let me double-check that. Maybe I made a mistake in the signs somewhere.\\n\\nStarting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, handle the subtraction:\\n\\n(2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P: 6 + i + (-i) = 6 + i - i = 6\\n\\nThen, add S: 6 + (2 + 4i) = 8 + 4i\\n\\nYes, that seems consistent. So, the final result is 8 + 4i.\\n\\nWait, hold on, let me make sure I didn't skip any steps or make a mistake in the signs.\\n\\nSo, starting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, compute (2 + i) - (-4):\\n\\nSubtracting a negative is adding, so 2 + i + 4 = 6 + i.\\n\\nThen, add P, which is -i:\\n\\n6 + i + (-i) = 6 + (i - i) = 6 + 0 = 6.\\n\\nThen, add S, which is 2 + 4i:\\n\\n6 + (2 + 4i) = (6 + 2) + 4i = 8 + 4i.\\n\\nYes, that seems correct. So, the final answer is 8 + 4i.\\n\\nAlternatively, maybe I can approach this problem by grouping the real and imaginary parts differently.\\n\\nLet me try that.\\n\\nGiven:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nSo, A - O + P + S can be rewritten as:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nLet me group the real numbers and the imaginary numbers separately.\\n\\nReal parts: 2, 0 (since O is -4, which is real), -0 (since P is -i, which is imaginary), and 2.\\n\\nImaginary parts: 1 (from A), 0 (from O), -1 (from P), and 4 (from S).\\n\\nSo, real parts: 2 + 0 + 0 + 2 = 4\\n\\nImaginary parts: 1 + 0 - 1 + 4 = 4\\n\\nWait, hold on, that doesn't seem right. Because when I grouped them earlier, I had 8 + 4i, but now grouping separately, I get 4 + 4i. That's a discrepancy. Hmm, so I must have made a mistake in my grouping.\\n\\nWait, let me check again.\\n\\nWait, no, actually, in the expression A - O + P + S, O is a real number, so it's just -(-4) which is +4. So, in the real parts, we have 2 (from A) + 4 (from -O) + 0 (from P) + 2 (from S). So, 2 + 4 + 2 = 8.\\n\\nImaginary parts: 1 (from A) + 0 (from -O) -1 (from P) + 4 (from S). So, 1 -1 + 4 = 4.\\n\\nSo, that gives 8 + 4i, which matches my previous result.\\n\\nWait, so why did I get 4 + 4i when I grouped them? Because I think I miscounted the real parts.\\n\\nWait, let me write it step by step.\\n\\nReal parts:\\n\\nFrom A: 2\\n\\nFrom -O: -(-4) = +4\\n\\nFrom P: 0 (since P is -i, which is purely imaginary)\\n\\nFrom S: 2\\n\\nSo, total real parts: 2 + 4 + 0 + 2 = 8\\n\\nImaginary parts:\\n\\nFrom A: 1\\n\\nFrom -O: 0 (since O is real)\\n\\nFrom P: -1\\n\\nFrom S: 4\\n\\nTotal imaginary parts: 1 + 0 -1 + 4 = 4\\n\\nSo, 8 + 4i. That's correct.\\n\\nSo, my initial grouping was wrong because I didn't account for all the real parts correctly. So, the correct answer is 8 + 4i.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = A + (-O) + P + S\\n\\nSo, substituting:\\n\\n= (2 + i) + (4) + (-i) + (2 + 4i)\\n\\nThen, combine like terms:\\n\\nReal parts: 2 + 4 + 2 = 8\\n\\nImaginary parts: i - i + 4i = 4i\\n\\nSo, 8 + 4i.\\n\\nYes, that's consistent.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = (A + P) + (-O + S)\\n\\nCompute A + P first:\\n\\nA = 2 + i\\n\\nP = -i\\n\\nSo, A + P = 2 + i + (-i) = 2 + 0i = 2\\n\\nThen, compute -O + S:\\n\\nO = -4, so -O = 4\\n\\nS = 2 + 4i\\n\\nSo, -O + S = 4 + (2 + 4i) = 6 + 4i\\n\\nNow, add A + P and -O + S:\\n\\n2 + (6 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, regardless of the method, I get 8 + 4i.\\n\\nTherefore, I think that's the correct answer.\\n\\nWait, just to make sure, let me compute each step numerically.\\n\\nCompute A - O:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nSo, A - O = (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P:\\n\\n6 + i + (-i) = 6 + 0i = 6\\n\\nThen, add S:\\n\\n6 + (2 + 4i) = 8 + 4i\\n\\nYep, same result.\\n\\nAlternatively, if I compute A - O + P + S step by step:\\n\\nStart with A: 2 + i\\n\\nSubtract O: 2 + i - (-4) = 2 + i + 4 = 6 + i\\n\\nAdd P: 6 + i + (-i) = 6 + 0i = 6\\n\\nAdd S: 6 + (2 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, I think I can confidently say that the answer is 8 + 4i.\\n\\nJust to recap, the steps are:\\n\\n1. Substitute each term into the expression.\\n\\n2. Simplify the expression by handling the operations step by step.\\n\\n3. Combine like terms (real and imaginary parts separately).\\n\\n4. Verify the result using different methods to ensure accuracy.\\n\\nSo, yeah, 8 + 4i is the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( A - O + P + S \\\\) is \\\\boxed{8 + 4i}.\\n</think>\\n\\nGiven the values \\\\( A = 2 + i \\\\), \\\\( O = -4 \\\\), \\\\( P = -i \\\\), and \\\\( S = 2 + 4i \\\\), we need to find the value of \\\\( A - O + P + S \\\\).\\n\\nFirst, substitute the given values into the expression:\\n\\n\\\\[\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\\\]\\n\\nSimplify each term step by step:\\n\\n1. Subtract \\\\( O \\\\) from \\\\( A \\\\):\\n   \\\\[\\n   (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n   \\\\]\\n\\n2. Add \\\\( P \\\\) to the result:\\n   \\\\[\\n   6 + i + (-i) = 6 + 0i = 6\\n   \\\\]\\n\\n3. Add \\\\( S \\\\) to the result:\\n   \\\\[\\n   6 + (2 + 4i) = 8 + 4i\\n   \\\\]\\n\\nThus, the value of \\\\( A - O + P + S \\\\) is \\\\(\\\\boxed{8 + 4i}\\\\).\"\n",
    "\n",
    "# compressed\n",
    "# 'Okay, so I have this problem here where I need to find the value of A minus O plus P plus S. The given values are A = 2 + i, O = -4, P = -i, and S = 2 + 4i. Hmm, let me write that down to make it clearer.\\n\\nFirst, let me write down each term:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nAnd the expression I need to compute is A - O + P + S. Hmm, okay. So, I think I need to substitute these values into the expression and then simplify it step by step.\\n\\nLet me start by substituting the values:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nWait, hold on. Let me make sure I substitute correctly. So, A is 2' \"\n",
    "# generated\n",
    "# \" + i, then subtract O which is -4, then add P which is -i, then add S which is 2 + 4i.\\n\\nSo, writing it out: (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nNow, let's simplify this step by step. I think the best way is to handle the real parts and the imaginary parts separately. That is, combine all the constants without the i and then combine all the terms with i.\\n\\nFirst, let's handle the real parts:\\n\\nWe have 2 (from A), then -(-4) which is +4, then 2 (from S). So, 2 + 4 + 2.\\n\\nLet me calculate that: 2 + 4 is 6, plus 2 is 8. So, the real part is 8.\\n\\nNow, the imaginary parts:\\n\\nFrom A, we have i, from O, we have -i, and from S, we have 4i. So, let's write that out:\\n\\ni - i + 4i\\n\\nLet me simplify that:\\n\\ni - i is 0, and then 0 + 4i is 4i. So, the imaginary part is 4i.\\n\\nPutting it all together, the expression simplifies to 8 + 4i.\\n\\nWait, let me double-check my steps to make sure I didn't make a mistake.\\n\\nStarting with A - O + P + S:\\n\\nA is 2 + i, O is -4, P is -i, S is 2 + 4i.\\n\\nSo, substituting:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nSimplify each term:\\n\\nFirst term: 2 + i\\n\\nSecond term: -(-4) is +4\\n\\nThird term: -i\\n\\nFourth term: 2 + 4i\\n\\nNow, combine all the real parts: 2 + 4 + 2 = 8\\n\\nCombine all the imaginary parts: i - i + 4i = (1 - 1 + 4)i = 4i\\n\\nSo, 8 + 4i is the result.\\n\\nWait, but let me make sure I didn't miss any signs. Sometimes, when subtracting a negative, it's easy to flip the sign. So, A - O is (2 + i) - (-4). Subtracting a negative is adding the positive, so that becomes 2 + i + 4, which is 6 + i.\\n\\nThen, adding P which is -i: 6 + i - i = 6 + 0i.\\n\\nThen, adding S which is 2 + 4i: 6 + 0i + 2 + 4i = 8 + 4i.\\n\\nYes, that's the same result as before. So, that seems consistent.\\n\\nAlternatively, I can think of it as:\\n\\nA - O + P + S = (A + P) + (S - O)\\n\\nLet me try that approach to see if I get the same result.\\n\\nFirst, compute A + P:\\n\\nA is 2 + i, P is -i.\\n\\nSo, 2 + i + (-i) = 2 + 0i = 2\\n\\nThen, compute S - O:\\n\\nS is 2 + 4i, O is -4.\\n\\nSo, 2 + 4i - (-4) = 2 + 4i + 4 = 6 + 4i\\n\\nNow, add the two results: 2 + (6 + 4i) = 8 + 4i\\n\\nSame result. Good, that's consistent.\\n\\nAnother way to check is to convert all the complex numbers to their standard form and then perform the operations.\\n\\nA is 2 + i\\n\\nO is -4 + 0i\\n\\nP is 0 - i\\n\\nS is 2 + 4i\\n\\nSo, substituting:\\n\\n(2 + i) - (-4 + 0i) + (0 - i) + (2 + 4i)\\n\\nSimplify each term:\\n\\nFirst term: 2 + i\\n\\nSecond term: -(-4 + 0i) = 4 + 0i\\n\\nThird term: 0 - i\\n\\nFourth term: 2 + 4i\\n\\nNow, combine all the real parts: 2 + 4 + 0 + 2 = 8\\n\\nCombine all the imaginary parts: i + 0i - i + 4i = (1 - 1 + 4)i = 4i\\n\\nSo, again, 8 + 4i.\\n\\nOkay, so regardless of the method I use, I keep getting 8 + 4i. That gives me confidence that the answer is correct.\\n\\nJust to be absolutely sure, let me write down each step again:\\n\\n1. Substitute the given values into the expression:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\n2. Simplify each term:\\n\\n- (2 + i) becomes +4\\n- (-i) remains as -i\\n- (2 + 4i) remains as is\\n\\nSo, the expression becomes:\\n\\n2 + i + 4 - i + 2 + 4i\\n\\n3. Combine like terms:\\n\\nReal parts: 2 + 4 + 2 = 8\\n\\nImaginary parts: i - i + 4i = 4i\\n\\n4. Combine them: 8 + 4i\\n\\nYep, that's consistent.\\n\\nI think I've checked it enough ways. So, the final answer should be 8 + 4i.\\n\\n**Final Answer**\\nThe value of \\\\( A - O + P + S \\\\) is \\\\boxed{8 + 4i}.\\n</think>\\n\\nGiven the complex numbers \\\\( A = 2 + i \\\\), \\\\( O = -4 \\\\), \\\\( P = -i \\\\), and \\\\( S = 2 + 4i \\\\), we need to find the value of \\\\( A - O + P + S \\\\).\\n\\nFirst, substitute the given values into the expression:\\n\\n\\\\[\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\\\]\\n\\nSimplify each term:\\n\\n- \\\\( (2 + i) \\\\) remains as is.\\n- \\\\( -(-4) \\\\) becomes \\\\( +4 \\\\).\\n- \\\\( -i \\\\) remains as is.\\n- \\\\( (2 + 4i) \\\\) remains as is.\\n\\nSo, the expression becomes:\\n\\n\\\\[\\n(2 + i) + 4 - i + (2 + 4i)\\n\\\\]\\n\\nCombine the real parts and the imaginary parts separately:\\n\\n- Real parts: \\\\( 2 + 4 + 2 = 8 \\\\)\\n- Imaginary parts: \\\\( i - i + 4i = 4i \\\\)\\n\\nPutting it all together, the expression simplifies to:\\n\\n\\\\[\\n8 + 4i\\n\\\\]\\n\\nThus, the value of \\\\( A - O + P + S \\\\) is \\\\(\\\\boxed{8 + 4i}\\\\).<endofsentence>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this problem here where I need to find the value of A minus O plus P plus S. The given values are A = 2 + i, O = -4, P = -i, and S = 2 + 4i. Hmm, let me write that down to make it clearer.\\n\\nFirst, let me write down each term:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nAnd the expression I need to compute is A - O + P + S. Hmm, okay. So, I think I need to substitute these values into the expression and then simplify it step by step.\\n\\nLet me start by substituting the values:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nWait, hold on. Let me make sure I substitute correctly. So, A is 2 + i, O is -4, P is -i, and S is 2 + 4i. So, replacing each term:\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nOkay, that looks right. Now, let me simplify this step by step. I think I should handle the operations one at a time.\\n\\nFirst, let's handle the subtraction of O, which is -(-4). Subtracting a negative is the same as adding a positive, so:\\n\\n(2 + i) - (-4) = (2 + i) + 4\\n\\nSo, that simplifies to 2 + 4 + i, which is 6 + i.\\n\\nAlright, so now the expression becomes:\\n\\n6 + i + (-i) + (2 + 4i)\\n\\nWait, let me write that again:\\n\\n= (6 + i) + (-i) + (2 + 4i)\\n\\nNow, let's combine the real parts and the imaginary parts separately.\\n\\nFirst, the real parts: 6, 0 (since there's no real part in -i), and 2.\\n\\nSo, 6 + 0 + 2 = 8.\\n\\nNow, the imaginary parts: i, -i, and 4i.\\n\\nSo, let's add those up:\\n\\ni - i + 4i\\n\\nHmm, i minus i is 0, so that leaves us with 4i.\\n\\nSo, putting it all together, the real part is 8 and the imaginary part is 4i, so the result is 8 + 4i.\\n\\nWait, let me double-check that. Maybe I made a mistake in the signs somewhere.\\n\\nStarting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, handle the subtraction:\\n\\n(2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P: 6 + i + (-i) = 6 + i - i = 6\\n\\nThen, add S: 6 + (2 + 4i) = 8 + 4i\\n\\nYes, that seems consistent. So, the final result is 8 + 4i.\\n\\nWait, hold on, let me make sure I didn't skip any steps or make a mistake in the signs.\\n\\nSo, starting over:\\n\\nA - O + P + S\\n\\n= (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, compute (2 + i) - (-4):\\n\\nSubtracting a negative is adding, so 2 + i + 4 = 6 + i.\\n\\nThen, add P, which is -i:\\n\\n6 + i + (-i) = 6 + (i - i) = 6 + 0 = 6.\\n\\nThen, add S, which is 2 + 4i:\\n\\n6 + (2 + 4i) = (6 + 2) + 4i = 8 + 4i.\\n\\nYes, that seems correct. So, the final answer is 8 + 4i.\\n\\nAlternatively, maybe I can approach this problem by grouping the real and imaginary parts differently.\\n\\nLet me try that.\\n\\nGiven:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nSo, A - O + P + S can be rewritten as:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nLet me group the real numbers and the imaginary numbers separately.\\n\\nReal parts: 2, 0 (since O is -4, which is real), -0 (since P is -i, which is imaginary), and 2.\\n\\nImaginary parts: 1 (from A), 0 (from O), -1 (from P), and 4 (from S).\\n\\nSo, real parts: 2 + 0 + 0 + 2 = 4\\n\\nImaginary parts: 1 + 0 - 1 + 4 = 4\\n\\nWait, hold on, that doesn't seem right. Because when I grouped them earlier, I had 8 + 4i, but now grouping separately, I get 4 + 4i. That's a discrepancy. Hmm, so I must have made a mistake in my grouping.\\n\\nWait, let me check again.\\n\\nWait, no, actually, in the expression A - O + P + S, O is a real number, so it's just -(-4) which is +4. So, in the real parts, we have 2 (from A) + 4 (from -O) + 0 (from P) + 2 (from S). So, 2 + 4 + 2 = 8.\\n\\nImaginary parts: 1 (from A) + 0 (from -O) -1 (from P) + 4 (from S). So, 1 -1 + 4 = 4.\\n\\nSo, that gives 8 + 4i, which matches my previous result.\\n\\nWait, so why did I get 4 + 4i when I grouped them? Because I think I miscounted the real parts.\\n\\nWait, let me write it step by step.\\n\\nReal parts:\\n\\nFrom A: 2\\n\\nFrom -O: -(-4) = +4\\n\\nFrom P: 0 (since P is -i, which is purely imaginary)\\n\\nFrom S: 2\\n\\nSo, total real parts: 2 + 4 + 0 + 2 = 8\\n\\nImaginary parts:\\n\\nFrom A: 1\\n\\nFrom -O: 0 (since O is real)\\n\\nFrom P: -1\\n\\nFrom S: 4\\n\\nTotal imaginary parts: 1 + 0 -1 + 4 = 4\\n\\nSo, 8 + 4i. That's correct.\\n\\nSo, my initial grouping was wrong because I didn't account for all the real parts correctly. So, the correct answer is 8 + 4i.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = A + (-O) + P + S\\n\\nSo, substituting:\\n\\n= (2 + i) + (4) + (-i) + (2 + 4i)\\n\\nThen, combine like terms:\\n\\nReal parts: 2 + 4 + 2 = 8\\n\\nImaginary parts: i - i + 4i = 4i\\n\\nSo, 8 + 4i.\\n\\nYes, that's consistent.\\n\\nAlternatively, maybe I can think of it as:\\n\\nA - O + P + S = (A + P) + (-O + S)\\n\\nCompute A + P first:\\n\\nA = 2 + i\\n\\nP = -i\\n\\nSo, A + P = 2 + i + (-i) = 2 + 0i = 2\\n\\nThen, compute -O + S:\\n\\nO = -4, so -O = 4\\n\\nS = 2 + 4i\\n\\nSo, -O + S = 4 + (2 + 4i) = 6 + 4i\\n\\nNow, add A + P and -O + S:\\n\\n2 + (6 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, regardless of the method, I get 8 + 4i.\\n\\nTherefore, I think that's the correct answer.\\n\\nWait, just to make sure, let me compute each step numerically.\\n\\nCompute A - O:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nSo, A - O = (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P:\\n\\n6 + i + (-i) = 6 + 0i = 6\\n\\nThen, add S:\\n\\n6 + (2 + 4i) = 8 + 4i\\n\\nYep, same result.\\n\\nAlternatively, if I compute A - O + P + S step by step:\\n\\nStart with A: 2 + i\\n\\nSubtract O: 2 + i - (-4) = 2 + i + 4 = 6 + i\\n\\nAdd P: 6 + i + (-i) = 6 + 0i = 6\\n\\nAdd S: 6 + (2 + 4i) = 8 + 4i\\n\\nSame result.\\n\\nSo, I think I can confidently say that the answer is 8 + 4i.\\n\\nJust to recap, the steps are:\\n\\n1. Substitute each term into the expression.\\n\\n2. Simplify the expression by handling the operations step by step.\\n\\n3. Combine like terms (real and imaginary parts separately).\\n\\n4. Verify the result using different methods to ensure accuracy.\\n\\nSo, yeah, 8 + 4i is the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( A - O + P + S \\\\) is \\\\boxed{8 + 4i}.\\n</think>\\n\\nGiven the values \\\\( A = 2 + i \\\\), \\\\( O = -4 \\\\), \\\\( P = -i \\\\), and \\\\( S = 2 + 4i \\\\), we need to find the value of \\\\( A - O + P + S \\\\).\\n\\nFirst, substitute the given values into the expression:\\n\\n\\\\[\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\\\]\\n\\nSimplify each term step by step:\\n\\n1. Subtract \\\\( O \\\\) from \\\\( A \\\\):\\n   \\\\[\\n   (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n   \\\\]\\n\\n2. Add \\\\( P \\\\) to the result:\\n   \\\\[\\n   6 + i + (-i) = 6 + 0i = 6\\n   \\\\]\\n\\n3. Add \\\\( S \\\\) to the result:\\n   \\\\[\\n   6 + (2 + 4i) = 8 + 4i\\n   \\\\]\\n\\nThus, the value of \\\\( A - O + P + S \\\\) is \\\\(\\\\boxed{8 + 4i}\\\\).\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_ids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" + i, then subtract O which is -4, then add P which is -i, then add S which is 2 + 4i.\\n\\nSo, writing it out: (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nNow, let's simplify this step by step. I think the best way is to handle the real parts and the imaginary parts separately. That is, combine all the constants without the i and then combine all the terms with i.\\n\\nFirst, let's handle the real parts:\\n\\nWe have 2 (from A), then -(-4) which is +4, then 2 (from S). So, 2 + 4 + 2.\\n\\nLet me calculate that: 2 + 4 is 6, plus 2 is 8. So, the real part is 8.\\n\\nNow, the imaginary parts:\\n\\nFrom A, we have i, from O, we have -i, and from S, we have 4i. So, let's write that out:\\n\\ni - i + 4i\\n\\nLet me simplify that:\\n\\ni - i is 0, and then 0 + 4i is 4i. So, the imaginary part is 4i.\\n\\nPutting it all together, the expression simplifies to 8 + 4i.\\n\\nWait, let me double-check my steps to make sure I didn't make a mistake.\\n\\nStarting with A - O + P + S:\\n\\nA is 2 + i, O is -4, P is -i, S is 2 + 4i.\\n\\nSo, substituting:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nSimplify each term:\\n\\nFirst term: 2 + i\\n\\nSecond term: -(-4) is +4\\n\\nThird term: -i\\n\\nFourth term: 2 + 4i\\n\\nNow, combine all the real parts: 2 + 4 + 2 = 8\\n\\nCombine all the imaginary parts: i - i + 4i = (1 - 1 + 4)i = 4i\\n\\nSo, 8 + 4i is the result.\\n\\nWait, but let me make sure I didn't miss any signs. Sometimes, when subtracting a negative, it's easy to flip the sign. So, A - O is (2 + i) - (-4). Subtracting a negative is adding the positive, so that becomes 2 + i + 4, which is 6 + i.\\n\\nThen, adding P which is -i: 6 + i - i = 6 + 0i.\\n\\nThen, adding S which is 2 + 4i: 6 + 0i + 2 + 4i = 8 + 4i.\\n\\nYes, that's the same result as before. So, that seems consistent.\\n\\nAlternatively, I can think of it as:\\n\\nA - O + P + S = (A + P) + (S - O)\\n\\nLet me try that approach to see if I get the same result.\\n\\nFirst, compute A + P:\\n\\nA is 2 + i, P is -i.\\n\\nSo, 2 + i + (-i) = 2 + 0i = 2\\n\\nThen, compute S - O:\\n\\nS is 2 + 4i, O is -4.\\n\\nSo, 2 + 4i - (-4) = 2 + 4i + 4 = 6 + 4i\\n\\nNow, add the two results: 2 + (6 + 4i) = 8 + 4i\\n\\nSame result. Good, that's consistent.\\n\\nAnother way to check is to convert all the complex numbers to their standard form and then perform the operations.\\n\\nA is 2 + i\\n\\nO is -4 + 0i\\n\\nP is 0 - i\\n\\nS is 2 + 4i\\n\\nSo, substituting:\\n\\n(2 + i) - (-4 + 0i) + (0 - i) + (2 + 4i)\\n\\nSimplify each term:\\n\\nFirst term: 2 + i\\n\\nSecond term: -(-4 + 0i) = 4 + 0i\\n\\nThird term: 0 - i\\n\\nFourth term: 2 + 4i\\n\\nNow, combine all the real parts: 2 + 4 + 0 + 2 = 8\\n\\nCombine all the imaginary parts: i + 0i - i + 4i = (1 - 1 + 4)i = 4i\\n\\nSo, again, 8 + 4i.\\n\\nOkay, so regardless of the method I use, I keep getting 8 + 4i. That gives me confidence that the answer is correct.\\n\\nJust to be absolutely sure, let me write down each step again:\\n\\n1. Substitute the given values into the expression:\\n\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\n2. Simplify each term:\\n\\n- (2 + i) becomes +4\\n- (-i) remains as -i\\n- (2 + 4i) remains as is\\n\\nSo, the expression becomes:\\n\\n2 + i + 4 - i + 2 + 4i\\n\\n3. Combine like terms:\\n\\nReal parts: 2 + 4 + 2 = 8\\n\\nImaginary parts: i - i + 4i = 4i\\n\\n4. Combine them: 8 + 4i\\n\\nYep, that's consistent.\\n\\nI think I've checked it enough ways. So, the final answer should be 8 + 4i.\\n\\n**Final Answer**\\nThe value of \\\\( A - O + P + S \\\\) is \\\\boxed{8 + 4i}.\\n</think>\\n\\nGiven the complex numbers \\\\( A = 2 + i \\\\), \\\\( O = -4 \\\\), \\\\( P = -i \\\\), and \\\\( S = 2 + 4i \\\\), we need to find the value of \\\\( A - O + P + S \\\\).\\n\\nFirst, substitute the given values into the expression:\\n\\n\\\\[\\nA - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\\\]\\n\\nSimplify each term:\\n\\n- \\\\( (2 + i) \\\\) remains as is.\\n- \\\\( -(-4) \\\\) becomes \\\\( +4 \\\\).\\n- \\\\( -i \\\\) remains as is.\\n- \\\\( (2 + 4i) \\\\) remains as is.\\n\\nSo, the expression becomes:\\n\\n\\\\[\\n(2 + i) + 4 - i + (2 + 4i)\\n\\\\]\\n\\nCombine the real parts and the imaginary parts separately:\\n\\n- Real parts: \\\\( 2 + 4 + 2 = 8 \\\\)\\n- Imaginary parts: \\\\( i - i + 4i = 4i \\\\)\\n\\nPutting it all together, the expression simplifies to:\\n\\n\\\\[\\n8 + 4i\\n\\\\]\\n\\nThus, the value of \\\\( A - O + P + S \\\\) is \\\\(\\\\boxed{8 + 4i}\\\\).<endofsentence>\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_ids_compressed[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_think = tokenizer.encode(\"</think>\", add_special_tokens=False)[0]\n",
    "generated_ids[-1].index(end_think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(generated_ids_compressed[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this problem here where an elephant and a lion are one mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, sounds like a relative speed problem. Let me think about how to approach this.\\n\\nFirst, let me visualize the situation. There's a lion and an elephant, one mile apart. The lion is running towards the elephant, and the elephant is running away from the lion. Since both are moving towards or away from each other, their speeds will affect the time it takes for the lion to catch up.\\n\\nI remember that when two objects are moving towards each other, their relative speed is the sum of their individual speeds. But in this case, the lion is moving towards the elephant, and the elephant is moving away from the lion, so\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next_true_tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nNow, let's add the real parts and the imaginary parts separately.\\n\\nReal parts: 2 (from A) + (-4) (from O) + 0 (from P) + 2 (from S) = 0\\n\\nImaginary parts: 1 (from A) + 0 (from O) + (-1) (from P) + 4 (from S) = 4\\n\\nSo, putting it all together, the expression simplifies to 0 + 4i, which is just 4i.\\n\\nWait, hold on, that doesn't seem right. Let me check my steps again.\\n\\nWait, when I separated the real and imaginary parts, I think I might have made a mistake in the signs.\\n\\nLet me try that again.\\n\\nStarting over:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nExpression: A - O + P + S\\n\\nSubstitute the values:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nNow, distribute the negative sign to O:\\n\\n2 + i + 4 - i + (-i) + 2 + 4i\\n\\nWait, hold on, that doesn't seem right. Let me do it step by step.\\n\\nFirst, A - O: (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P: 6 + i + (-i) = 6 + 0i = 6\\n\\nThen, add S: 6 + (2 + 4i) = 8 + 4i\\n\\nWait, that makes more sense. So, the expression simplifies to 8 + 4i.\\n\\nWait, so earlier I messed up the distribution. Let me correct that.\\n\\nSo, starting over:\\n\\nExpression: A - O + P + S\\n\\nSubstitute:\\n\\n(2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nFirst, handle A - O:\\n\\n(2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P:\\n\\n6 + i + (-i) = 6 + 0i = 6\\n\\nThen, add S:\\n\\n6 + (2 + 4i) = 8 + 4i\\n\\nSo, the final result is 8 + 4i.\\n\\nWait, so earlier I thought it was 4i, but that was incorrect. I must have made a mistake in distributing the negative sign or something.\\n\\nSo, the correct answer is 8 + 4i.\\n\\nBut let me verify once more.\\n\\nCompute A - O + P + S:\\n\\nA = 2 + i\\n\\nO = -4\\n\\nP = -i\\n\\nS = 2 + 4i\\n\\nSo, A - O + P + S = (2 + i) - (-4) + (-i) + (2 + 4i)\\n\\nCompute each operation step by step:\\n\\nFirst, (2 + i) - (-4) = 2 + i + 4 = 6 + i\\n\\nThen, add P: 6 + i + (-i) = 6 + 0i = 6\\n\\nThen, add S: 6 + (2 + 4i) = 8 + 4i\\n\\nYes, that seems correct. So, the final result is 8 + 4i.\\n\\nI think I initially made a mistake in the distribution step, but upon redoing it, I arrived at the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( A - O + P + S \\\\) is \\\\boxed{8 + 4i}.\\n</think>\\n\\nGiven the values \\\\( A = 2 + i \\\\), \\\\( O = -4 \\\\), \\\\( P = -i \\\\), and \\\\( S = 2 + 4i \\\\), we need to find the value of \\\\( A - O + P + S \\\\).\\n\\nStarting with the expression:\\n\\\\[ A - O + P + S \\\\]\\n\\nSubstitute the given values:\\n\\\\[ (2 + i) - (-4) + (-i) + (2 + 4i) \\\\]\\n\\nSimplify each operation step by step:\\n1. Compute \\\\( (2 + i) - (-4) \\\\):\\n   \\\\[ 2 + i + 4 = 6 + i \\\\]\\n2. Add \\\\( P \\\\):\\n   \\\\[ 6 + i + (-i) = 6 + 0i = 6 \\\\]\\n3. Add \\\\( S \\\\):\\n   \\\\[ 6 + (2 + 4i) = 8 + 4i \\\\]\\n\\nThus, the value of \\\\( A - O + P + S \\\\) is \\\\(\\\\boxed{8 + 4i}\\\\).<endofsentence>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_ids_compressed[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 1536])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I have this problem where an elephant and a lion are 1 mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, let me think about how to approach this.\n",
      "\n",
      "First, I know that both the elephant and the lion are moving towards or away from each other. The elephant is moving away at 19 mph, and the lion is moving towards it at 24 mph. So, their relative speed is the difference between their speeds because they're moving towards each other. Wait, no, actually, since the elephant is moving away, the lion has to cover the distance that the elephant is moving away plus the distance the elephant covers while the lion is moving towards it.\n",
      "\n",
      "Let me clarify. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So, the lion is closing the gap at a rate of 24 mph minus 19 mph, which is 5 mph. That makes sense because if two objects are moving towards each other, their relative speed is the sum of their speeds, but in this case, one is moving away and the other is moving towards, so it's the difference.\n",
      "\n",
      "So, the initial distance between them is 1 mile. The lion is closing the gap at 5 mph. To find the time it takes to catch up, I can use the formula:\n",
      "\n",
      "Time = Distance / Speed\n",
      "\n",
      "So, plugging in the numbers, the time should be 1 mile divided by 5 mph. That gives me 0.2 hours. But the question asks for the time in minutes, so I need to convert 0.2 hours to minutes. Since 1 hour is 60 minutes, 0.2 hours is 0.2 * 60 = 12 minutes.\n",
      "\n",
      "Wait, let me double-check that. If the lion is moving at 24 mph and the elephant is moving away at 19 mph, the relative speed is 24 - 19 = 5 mph. So, yes, the lion is gaining on the elephant at 5 mph. So, 1 mile divided by 5 mph is indeed 0.2 hours, which is 12 minutes. That seems right.\n",
      "\n",
      "But just to make sure I didn't make a mistake, let me think about it another way. Maybe set up an equation for their positions as functions of time and see if I get the same result.\n",
      "\n",
      "Let's denote t as the time in hours it takes for the lion to catch the elephant. In that time, the elephant will have moved a distance of 19t miles away from the starting point, and the lion will have moved 24t miles towards the elephant. Since they start 1 mile apart, the distance between them when the lion catches the elephant will be zero.\n",
      "\n",
      "So, the distance the lion covers plus the distance the elephant covers should equal the initial distance between them. Wait, no, actually, the lion is moving towards the elephant, so the distance the lion covers is 24t, and the distance the elephant covers is 19t. But since the elephant is moving away, the total distance between them when the lion catches up is 24t - 19t = 5t. This should equal the initial distance, which is 1 mile.\n",
      "\n",
      "So, 5t = 1 mile. Solving for t, we get t = 1/5 hours, which is 0.2 hours. Converting that to minutes, 0.2 * 60 = 12 minutes. Yep, same result. So, that seems consistent.\n",
      "\n",
      "Alternatively, maybe I can think about it in terms of how much distance the lion needs to cover relative to the elephant. Since the lion is moving faster, it's gaining on the elephant at 5 mph. So, the lion needs to cover the 1 mile gap at a relative speed of 5 mph. So, time = distance / speed = 1 / 5 hours, which is 12 minutes. Yep, same answer.\n",
      "\n",
      "I think that's solid. So, the lion will catch the elephant in 12 minutes.\n",
      "\n",
      "**Final Answer**\n",
      "The lion will catch the elephant in \\boxed{12} minutes.\n",
      "</think>\n",
      "\n",
      "The elephant and the lion are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. \n",
      "\n",
      "To find the time it takes for the lion to catch the elephant, we first determine their relative speed. Since the lion is moving towards the elephant and the elephant is moving away, their relative speed is the difference between their speeds:\n",
      "\n",
      "\\[\n",
      "24 \\text{ mph} - 19 \\text{ mph} = 5 \\text{ mph}\n",
      "\\]\n",
      "\n",
      "The initial distance between them is 1 mile. Using the formula for time, which is distance divided by speed, we get:\n",
      "\n",
      "\\[\n",
      "\\text{Time} = \\frac{1 \\text{ mile}}{5 \\text{ mph}} = 0.2 \\text{ hours}\n",
      "\\]\n",
      "\n",
      "Converting 0.2 hours to minutes:\n",
      "\n",
      "\\[\n",
      "0.2 \\text{ hours} \\times 60 \\text{ minutes per hour} = 12 \\text{ minutes}\n",
      "\\]\n",
      "\n",
      "Thus, the lion will catch the elephant in \\boxed{12} minutes.<endofsentence>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(torch.tensor(generated_ids[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this problem here where an elephant and a lion are one mile apart. The elephant is running directly away from the lion at 19 miles per hour, and the lion is running towards the elephant at 24 miles per hour. I need to figure out how many minutes it will take for the lion to catch the elephant. Hmm, sounds like a relative speed problem. Let me think about how to approach this.\\n\\nFirst, let me visualize the situation. There's a lion and an elephant, one mile apart. The lion is running towards the elephant, and the elephant is running away from the lion. Since both are moving towards or away from each other, their speeds will affect the time it takes for the lion to catch up.\\n\\nI remember that when two objects are moving towards each other, their relative speed is the sum of their individual speeds. But in this case, the lion is moving towards the elephant, and the elephant is moving away from the lion, so their speeds are in opposite directions. Wait, no, actually, the lion is moving towards the elephant, so their relative speed is the lion's speed minus the elephant's speed, because they are moving in the same direction. Wait, no, hold on. Let me make sure.\\n\\nThe lion is moving towards the elephant, which is moving away. So from the lion's perspective, the elephant is moving away at 19 mph. So the relative speed between the lion and the elephant is the lion's speed minus the elephant's speed because they're moving in the same direction. Wait, no, actually, if both are moving, the relative speed is the difference when moving in the same direction. Wait, maybe I'm getting confused.\\n\\nLet me recall the formula: when two objects are moving towards each other, their relative speed is the sum of their speeds. But when moving in the same direction, it's the difference. But in this case, the lion is chasing the elephant. So if the lion is moving towards the elephant, and the elephant is moving away, their speeds are in the same direction. So the relative speed is lion's speed minus elephant's speed.\\n\\nWait, hold on. Let me think in terms of distance. The initial distance is 1 mile. The lion is moving towards the elephant at 24 mph, and the elephant is moving away at 19 mph. So relative to the lion, the elephant is approaching at (24 - 19) mph, which is 5 mph. So the lion is gaining on the elephant at 5 miles per hour.\\n\\nTherefore, the time it takes for the lion to catch the elephant would be the initial distance divided by the relative speed. So 1 mile divided by 5 mph, which is 1/5 hours. To convert that into minutes, since 1 hour is 60 minutes, 1/5 hours is 12 minutes.\\n\\nWait, that seems straightforward, but let me verify. Let me think of it in terms of equations.\\n\\nLets denote the time it takes for the lion to catch the elephant as t hours.\\n\\nIn t hours, the lion will cover a distance of 24t miles.\\n\\nIn the same t hours, the elephant will cover a distance of 19t miles, but since the elephant is moving away, the distance between the lion and the elephant will decrease at a rate of (24 - 19) mph, which is 5 mph.\\n\\nSo the distance between them decreases by 5 miles every hour. Since they start 1 mile apart, the time it takes for the lion to catch the elephant is 1 / 5 hours, which is 12 minutes. So that seems consistent.\\n\\nAlternatively, I can model the positions of the lion and the elephant as functions of time and find when they meet.\\n\\nLets assume the lion starts at position 0, and the elephant starts at position 1 mile.\\n\\nLets let t be the time in hours. Then, the position of the lion after t hours is 0 + 24t = 24t miles.\\n\\nThe position of the elephant after t hours is 1 - 19t miles.\\n\\nThey meet when their positions are equal, so:\\n\\n24t = 1 - 19t\\n\\nAdding 19t to both sides:\\n\\n24t + 19t = 1\\n\\n43t = 1\\n\\nt = 1/43 hours.\\n\\nWait, hold on, this is different. Earlier, I thought it was 1/5 hours, but now I have 1/43 hours. Which one is correct?\\n\\nWait, that's a big difference. Which approach is correct?\\n\\nLet me check my equations again. Position of lion is 24t, position of elephant is 1 - 19t. So when they meet, 24t = 1 - 19t.\\n\\nSo 24t + 19t = 1\\n\\n43t = 1\\n\\nt = 1/43 hours.\\n\\nHmm, so that's approximately 1.395 minutes, which is about 83.7 seconds. But that seems too fast. Wait, but 1/43 of an hour is roughly 1.395 minutes, which is 83.7 seconds, but 1/5 is 12 minutes, which is 720 seconds. That seems like a huge difference.\\n\\nSo there's a discrepancy here. Which one is right?\\n\\nWait, maybe I made a mistake in setting up the equations. Let me think.\\n\\nWait, if the lion is moving towards the elephant, which is moving away, so the distance between them is decreasing at a rate of (24 - 19) mph, which is 5 mph. So the time should be 1 mile / 5 mph = 1/5 hours, which is 12 minutes.\\n\\nBut in the equation above, I set the positions equal, which gives me t = 1/43 hours. So which is correct?\\n\\nWait, no, the position of the lion is 24t, and the position of the elephant is 1 - 19t. So when they meet, 24t = 1 - 19t.\\n\\nWait, that would mean that the lion is moving towards the elephant, and the elephant is moving away, so the distance between them is decreasing. So the correct equation should be 24t = 1 - 19t.\\n\\nBut solving that gives t = 1/43 hours, which is approximately 1.395 minutes, which seems contradictory.\\n\\nWait, that can't be. Because if the lion is moving at 24 mph and the elephant is moving away at 19 mph, the lion is closing the distance at 5 mph.\\n\\nBut in the equation, the lion is moving at 24t, which is 24 miles per hour, but the elephant is moving away at 19 mph, so the relative speed is 24 - 19 = 5 mph.\\n\\nWait, so maybe the correct equation is that the lion is moving towards the elephant at 5 mph, so the time is 1 mile / 5 mph = 1/5 hours, which is 12 minutes.\\n\\nBut then why does the position equation give me a different answer?\\n\\nWait, perhaps I have a sign error in the position equation.\\n\\nLet me think about the coordinate system. Let's say the lion starts at position 0, and the elephant starts at position 1.\\n\\nSo the lion is moving towards the positive direction at 24 mph, so its position at time t is 24t.\\n\\nThe elephant is moving away from the lion, so its position at time t is 1 + 19t.\\n\\nWait, hold on, if the elephant is moving away from the lion, which is at position 0, then yes, the elephant's position is 1 + 19t.\\n\\nBut wait, if the lion is moving towards the elephant, which is at 1 mile, so if the lion is at position 0, and the elephant is at position 1, and the lion is moving towards the elephant, so the distance between them is decreasing.\\n\\nWait, so if the lion is moving towards the elephant, then the elephant is moving away from the lion, so the distance between them is decreasing at a rate of (24 - 19) mph, which is 5 mph.\\n\\nTherefore, the relative speed is 5 mph, so the time is 1 mile / 5 mph = 1/5 hours = 12 minutes.\\n\\nBut if I model the positions, the lion is at 24t, the elephant is at 1 + 19t. Setting them equal:\\n\\n24t = 1 + 19t\\n\\n24t - 19t = 1\\n\\n5t = 1\\n\\nt = 1/5 hours, which is 12 minutes. So that's consistent.\\n\\nWait, so earlier, I thought the position of the elephant was 1 - 19t, but that was incorrect. The correct position of the elephant is 1 + 19t because it's moving away from the lion.\\n\\nSo my initial setup was wrong. I think I confused the direction. So the correct equation is 24t = 1 + 19t, leading to t = 1/5 hours, which is 12 minutes.\\n\\nTherefore, the lion will catch the elephant in 12 minutes.\\n\\nWait, so that contradicts my initial thought that the relative speed is 5 mph. But why was I getting confused earlier?\\n\\nI think the confusion was in the direction of the elephant's movement. If the elephant is moving away from the lion, then the distance between them is increasing at 19 mph, so the relative speed at which the distance is decreasing is 24 - 19 = 5 mph.\\n\\nBut in the position equation, if the lion is moving towards the elephant, and the elephant is moving away, then the distance between them is decreasing at 5 mph. Therefore, the time to close 1 mile is 1 / 5 hours, which is 12 minutes.\\n\\nSo in summary, the correct answer is 12 minutes.\\n\\nBut let me just double-check the equations.\\n\\nLets denote t as the time in hours.\\n\\nPosition of the lion: 24t.\\n\\nPosition of the elephant: 1 + 19t.\\n\\nThey meet when 24t = 1 + 19t.\\n\\n24t - 19t = 1\\n\\n5t = 1\\n\\nt = 1/5 hours = 12 minutes.\\n\\nYes, that seems correct. So the initial mistake was in the position equation for the elephant, thinking it was moving towards, but it's actually moving away. So the correct equation is 24t = 1 + 19t.\\n\\nSo the answer is 12 minutes.\\n\\nI think that makes sense. Let me think about it another way.\\n\\nIf both were moving towards each other, their relative speed would be 24 + 19 = 43 mph, and the time would be 1/43 hours, which is about 1.395 minutes, but that's if they were moving towards each other. But in this case, they are moving away from each other in the same direction, so the relative speed is 24 - 19 = 5 mph, hence 1/5 hours.\\n\\nAlternatively, if I think about the distance between them decreasing at 5 mph, so 1 mile at 5 mph is 1/5 hours, which is 12 minutes.\\n\\nYes, that seems consistent.\\n\\nSo I think my initial setup was wrong because I thought the elephant was moving towards the lion, but it's actually moving away. So the correct answer is 12 minutes.\\n\\n**Final Answer**\\nThe lion will catch the elephant in \\\\boxed{12} minutes.\\n</think>\\n\\nThe problem involves an elephant and a lion that are initially 1 mile apart. The elephant runs directly away from the lion at 19 miles per hour, while the lion runs directly towards the elephant at 24 miles per hour. We need to determine how many minutes it will take for the lion to catch the elephant.\\n\\nFirst, we recognize that the lion is moving towards the elephant, and the elephant is moving away. Therefore, the relative speed at which the lion is approaching the elephant is the difference between their speeds: \\\\(24 - 19 = 5\\\\) miles per hour.\\n\\nThe time it takes for the lion to catch the elephant can be calculated by dividing the initial distance between them by their relative speed. The initial distance is 1 mile, and the relative speed is 5 miles per hour. Thus, the time \\\\(t\\\\) in hours is given by:\\n\\n\\\\[\\nt = \\\\frac{1 \\\\text{ mile}}{5 \\\\text{ mph}} = \\\\frac{1}{5} \\\\text{ hours}\\n\\\\]\\n\\nTo convert this time into minutes, we multiply by 60:\\n\\n\\\\[\\n\\\\frac{1}{5} \\\\text{ hours} \\\\times 60 \\\\text{ minutes per hour} = 12 \\\\text{ minutes}\\n\\\\]\\n\\nThus, the lion will catch the elephant in \\\\(\\\\boxed{12}\\\\) minutes.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer.decode(torch.tensor(generated_ids[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_true_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1536])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 224256])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeds_for_compression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_embeds torch.Size([1, 62, 1536])\n",
      "<beginofsentence><User>Here's a question: What do many people believe happens after you die?  Here are possible answers to this question: - stop moving - nothing - go to heaven - stop living - stop breathing  I believe the correct choice is \"go to heaven\", here's why:\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I'm trying to figure out the answer is correct. Let me think about it again. I think the user is trying to figure out the correct answer to the question they're asking. They provided a list of answers, and I need to heaven, but I'm not sure if I'm sure if I'm on the right track. Let me break it down step by step. The question is about what people believe happens after you die. I know that when someone dies, they believe that people often believe in something called the afterlife, which is the belief that after you die, you don't need to move or anything else. So, the correct answer is \"go to heaven\", which is the correct answer. The other options, like stopping moving, nothing, stop breathing, etc., aren't correct\n"
     ]
    }
   ],
   "source": [
    "from hidden_capacity_reasoning.utils import WINDOW_SIZE, VISION_START, VISION_END\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "\n",
    "def _crop_past_key_values(model, past_key_values, max_length):\n",
    "    \"\"\"Crops the past key values up to a certain maximum length.\"\"\"\n",
    "    new_past = []\n",
    "    if model.config.is_encoder_decoder:\n",
    "        for idx in range(len(past_key_values)):\n",
    "            new_past.append(\n",
    "                (\n",
    "                    past_key_values[idx][0][:, :, :max_length, :],\n",
    "                    past_key_values[idx][1][:, :, :max_length, :],\n",
    "                    past_key_values[idx][2],\n",
    "                    past_key_values[idx][3],\n",
    "                )\n",
    "            )\n",
    "        past_key_values = tuple(new_past)\n",
    "    # gptbigcode is special and stores kv in shape (batch_size, seq_len, dim), if it's a multi_query model\n",
    "    elif \"gptbigcode\" in model.__class__.__name__.lower() or (\n",
    "        model.config.architectures is not None\n",
    "        and \"gptbigcode\" in model.config.architectures[0].lower()\n",
    "    ):\n",
    "        if model.config.multi_query:\n",
    "            for idx in range(len(past_key_values)):\n",
    "                past_key_values[idx] = past_key_values[idx][:, :max_length, :]\n",
    "        else:\n",
    "            for idx in range(len(past_key_values)):\n",
    "                past_key_values[idx] = past_key_values[idx][:, :, :max_length, :]\n",
    "    elif isinstance(past_key_values, DynamicCache):\n",
    "        past_key_values.crop(max_length)\n",
    "    elif past_key_values is not None:\n",
    "        for idx in range(len(past_key_values)):\n",
    "            if past_key_values[idx] != ([], []):\n",
    "                new_past.append(\n",
    "                    (\n",
    "                        past_key_values[idx][0][:, :, :max_length, :],\n",
    "                        past_key_values[idx][1][:, :, :max_length, :],\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                new_past.append((past_key_values[idx][0], past_key_values[idx][1]))\n",
    "        past_key_values = tuple(new_past)\n",
    "    return past_key_values\n",
    "\n",
    "\n",
    "# model = trainer.model\n",
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        # {\"role\": \"user\", \"content\": \"how many wings has a bird?\"},\n",
    "        {\"role\": \"user\", \"content\": dataset[\"test\"].to_list()[:5][0][\"question\"]},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "with torch.no_grad(), torch.autocast(device_type=\"cuda\"):\n",
    "    start_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "        torch.tensor([[VISION_START]], device=\"cuda\")\n",
    "    )\n",
    "    end_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "        torch.tensor([[VISION_END]], device=\"cuda\")\n",
    "    )\n",
    "    generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "    generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "    temp_gen_size = 0\n",
    "    window_size = WINDOW_SIZE  # + 1\n",
    "    # new_tokens = 4\n",
    "    new_tokens = 1\n",
    "    generation_started = False\n",
    "    max_steps = (new_tokens + window_size) * 15\n",
    "    past_key_values_big = None\n",
    "    print(\"generated_embeds\", generated_embeds.shape)\n",
    "    for step in range(max_steps):\n",
    "        if temp_gen_size == window_size + new_tokens:\n",
    "            # print(\n",
    "            #     \"TOKENS FOR EMDED\",\n",
    "            #     tokenizer.decode(\n",
    "            #         generated_tokens[:, -(window_size + new_tokens) :][:, :WINDOW_SIZE]\n",
    "            #         .cpu()\n",
    "            #         .tolist()[0]\n",
    "            #     ),\n",
    "            # )\n",
    "            # tokenizer.decode(generated_tokens[:, : -window_size ].cpu().tolist()[0])\n",
    "            if hasattr(model.base_model, \"embed_pooler\"):\n",
    "                new_embeds_for_compression = (\n",
    "                    model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "                        generated_tokens[:, -(window_size + new_tokens) :][\n",
    "                            :, :WINDOW_SIZE\n",
    "                        ]\n",
    "                    )\n",
    "                ).to(torch.bfloat16)\n",
    "                compressed_part = model.base_model.embed_pooler(\n",
    "                    new_embeds_for_compression\n",
    "                )\n",
    "            else:\n",
    "                compressed_part = model.embed_pooler(new_embeds_for_compression)\n",
    "            # gen_embeds_prev = generated_tokens.shape[1]\n",
    "            if generation_started:\n",
    "                # past_key_values_big = _crop_past_key_values(\n",
    "                #     model=model,\n",
    "                #     past_key_values=past_key_values_big,\n",
    "                #     max_length=generated_embeds.shape[1] - new_tokens - 2,\n",
    "                # )\n",
    "                generated_embeds = torch.cat(\n",
    "                    [\n",
    "                        generated_embeds[:, : -(window_size + new_tokens + 1)],\n",
    "                        # generated_embeds[:, : -(window_size + new_tokens)],\n",
    "                        compressed_part,\n",
    "                        # torch.randn(1, 1, 1536, device=\"cuda\"),\n",
    "                        end_embed,\n",
    "                        generated_embeds[:, -new_tokens:],\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            else:\n",
    "                # past_key_values_big = _crop_past_key_values(\n",
    "                #     model=model,\n",
    "                #     past_key_values=past_key_values_big,\n",
    "                #     max_length=generated_embeds.shape[1] - new_tokens - 3,\n",
    "                # )\n",
    "                generated_embeds = torch.cat(\n",
    "                    [\n",
    "                        generated_embeds[:, : -(window_size + new_tokens)],\n",
    "                        start_embed,\n",
    "                        # torch.randn(1, 1, 1536, device=\"cuda\"),\n",
    "                        compressed_part,\n",
    "                        end_embed,\n",
    "                        generated_embeds[:, -new_tokens:],\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                generation_started = True\n",
    "            past_key_values_big = _crop_past_key_values(\n",
    "                model=model,\n",
    "                past_key_values=past_key_values_big,\n",
    "                max_length=generated_embeds.shape[1] - new_tokens - 2,\n",
    "            )\n",
    "            temp_gen_size = 1\n",
    "\n",
    "        outputs = model(\n",
    "            inputs_embeds=generated_embeds,\n",
    "            past_key_values=past_key_values_big,\n",
    "            # use_cache=False,\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        past_key_values_big = outputs.past_key_values\n",
    "        top_token = logits.argmax(-1)[-1][-1]\n",
    "        top_token_embed = model.get_input_embeddings()(top_token)\n",
    "        # print(top)\n",
    "        generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "\n",
    "        generated_embeds = torch.cat(\n",
    "            [generated_embeds, top_token_embed.reshape(1, 1, -1)], dim=1\n",
    "        )\n",
    "        # print(temp_gen_size, tokenizer.decode(generated_tokens[-1]))\n",
    "\n",
    "        temp_gen_size += 1\n",
    "\n",
    "print(tokenizer.decode(generated_tokens[-1]))\n",
    "\n",
    "# break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   ,       KV-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 227])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1536])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeds_for_compression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5, 6, 7, 8][:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"test\"][0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 1536])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 227])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   MATH-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe76e8b497024babbe0610aff09d788f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "2794 400 2994 2356\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1164 400 1364 1766\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "770 400 970 825\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "443 400 643 1781\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "536 400 736 1649\n",
      "===\n",
      "===\n",
      "===\n",
      "WRONG (60,-88,25,4)\n",
      "5)^2*c*(-5)^3*d*(-5)^4 = 625\n",
      "- P(6) = a + 6b + 36c + 216d = 1296\n",
      "- P(1) = a + b + c + d = 1\n",
      "\n",
      "So, P(2) = 16, P(-5) = 625, P(6) = 1296, and P(1) = 1.\n",
      "\n",
      "Wait, 16, 625, 1296, and 1. Hmm, those numbers look like squares or cubes. Let me check:\n",
      "\n",
      "- 16 is 4 squared, which is 2^4.\n",
      "- 625 is 25 squared, which is 5^4.\n",
      "- 1296 is 36 squared, which is 6^4.\n",
      "- 1 is 1^4.\n",
      "\n",
      "So, P(2) = 4^2 = 2^4, P(-5) = 25^2 = 5^4, P(6) = 36^2 = 6^4, and P(1) = 1^4.\n",
      "\n",
      "Wait, so P(x) evaluated at x=2, x=-5, x=6, x=1 gives 2^4, 5^4, 6^4, 1^4 respectively.\n",
      "\n",
      "Is there a polynomial P(x) such that P(2) = 2^4, P(-5) = (-5)^4, P(6) = 6^4, P(1) = 1^4?\n",
      "\n",
      "Wait, but 1^4 is just 1, which is the same as P(1). So, maybe P(x) is equal to x^4 at x=2, x=-5, x=6, and x=1.\n",
      "\n",
      "But P(x) is a cubic polynomial, right? Because it's a + b x + c x^2 + d x^3. So, a cubic polynomial can be uniquely determined by four points. So, if P(x) is equal to x^4 at four points, then P(x) must be equal to x^4 for all x, but that can't be because x^4 is a quartic polynomial, while P(x) is cubic. So, that's a contradiction.\n",
      "\n",
      "Wait, so maybe P(x) is equal to x^4 minus some multiple of (x - 2)(x + 5)(x - 6)(x - 1). Because if we subtract a multiple of that product, which is zero at x=2, x=-5, x=6, x=1, then P(x) would be equal to x^4 at those points.\n",
      "\n",
      "But P(x) is a cubic, so if we subtract a multiple of (x - 2)(x + 5)(x - 6)(x - 1), which is a quartic, then P(x) would be x^4 minus a quartic, which would still be a quartic. But P(x) is cubic, so that can't be.\n",
      "\n",
      "Wait, maybe I need to think differently. Let me consider that P(x) is a cubic polynomial, and it's equal to x^4 at four points. But x^4 is a quartic, so unless P(x) is equal to x^4 minus some multiple of (x - 2)(x + 5)(x - 6)(x - 1), but that would make P(x) a quartic minus a quartic, which is a cubic or lower.\n",
      "\n",
      "But P(x) is a cubic, so maybe P(x) = x^4 - k(x - 2)(x + 5)(x - 6)(x - 1). Then, P(x) would be a quartic minus a quartic, which is a cubic or lower. But we know P(x) is a cubic, so maybe k is chosen such that the quartic term cancels out.\n",
      "\n",
      "But wait, P(x) is a cubic, so the coefficient of x^4 in P(x) must be zero. So, let's write P(x) as:\n",
      "\n",
      "P(x) = a + b x + c x^2 + d x^3\n",
      "\n",
      "And we also have P(x) = x^4 - k(x - 2)(x + 5)(x - 6)(x - 1)\n",
      "\n",
      "So, let's expand the right-hand side:\n",
      "\n",
      "x^4 - k(x - 2)(x + 5)(x - 6)(x - 1)\n",
      "\n",
      "First, let's compute (x - 2)(x + 5)(x - 6)(x - 1). Let's pair them:\n",
      "\n",
      "First, compute (x - 2)(x + 5) = x^2 + 5x - 2x -10 = x^2 + 3x -10\n",
      "\n",
      "Then, compute (x - 6)(x - 1) = x^2 - x -6x +6 = x^2 -7x +6\n",
      "\n",
      "Now, multiply these two quadratics:\n",
      "\n",
      "(x^2 + 3x -10)(x^2 -7x +6)\n",
      "\n",
      "Let me compute this:\n",
      "\n",
      "First, x^2 * x^2 = x^4\n",
      "\n",
      "x^2 * (-7x) = -7x^3\n",
      "\n",
      "x^2 * 6 = 6x^2\n",
      "\n",
      "3x * x^2 = 3x^3\n",
      "\n",
      "3x * (-7x) = -21x^2\n",
      "\n",
      "3x * 6 = 18x\n",
      "\n",
      "-10 * x^2 = -10x^2\n",
      "\n",
      "-10 * (-7x) = 70x\n",
      "\n",
      "-10 * 6 = -60\n",
      "\n",
      "Now, combine like terms:\n",
      "\n",
      "x^4\n",
      "\n",
      "-7x^3 + 3x^3 = -4x^3\n",
      "\n",
      "6x^2 -21x^2 -10x^2 = (6 -21 -10)x^2 = -25x^2\n",
      "\n",
      "18x +70x = 88x\n",
      "\n",
      "-60\n",
      "\n",
      "So, (x - 2)(x + 5)(x - 6)(x - 1) = x^4 -4x^3 -25x^2 +88x -60\n",
      "\n",
      "Therefore, P(x) = x^4 - k(x^4 -4x^3 -25x^2 +88x -60)\n",
      "\n",
      "So, P(x) = x^4 -k x^4 +4k x^3 +25k x^2 -88k x +60k\n",
      "\n",
      "Combine like terms:\n",
      "\n",
      "(1 - k)x^4 +4k x^3 +25k x^2 -88k x +60k\n",
      "\n",
      "But P(x) is given as a cubic polynomial: a + b x + c x^2 + d x^3\n",
      "\n",
      "So, we have:\n",
      "\n",
      "(1 - k)x^4 +4k x^3 +25k x^2 -88k x +60k = a + b x + c x^2 + d x^3\n",
      "\n",
      "Since this must hold for all x, the coefficients of corresponding powers must be equal. Therefore:\n",
      "\n",
      "For x^4: 1 - k = 0 => k = 1\n",
      "\n",
      "For x^3: 4k = b\n",
      "\n",
      "For x^2: 25k = c\n",
      "\n",
      "For x: -88k = d\n",
      "\n",
      "Constant term: 60k = a\n",
      "\n",
      "So, since k = 1, we have:\n",
      "\n",
      "a = 60*1 = 60\n",
      "\n",
      "b = 4*1 = 4\n",
      "\n",
      "c = 25*1 = 25\n",
      "\n",
      "d = -88*1 = -88\n",
      "\n",
      "Wait, but hold on, the first equation is a + b + c + d = 1. Let's check if these values satisfy that.\n",
      "\n",
      "a + b + c + d = 60 + 4 + 25 -88 = (60 + 4) + (25 -88) = 64 -63 = 1\n",
      "\n",
      "Yes, that works. So, the ordered quadruple is (60, 4, 25, -88).\n",
      "\n",
      "But wait, let me double-check because the coefficients seem quite large. Let me verify each equation.\n",
      "\n",
      "First equation: a + b + c + d = 60 + 4 + 25 -88 = 1. Correct.\n",
      "\n",
      "Second equation: a + 2b + 4c + 8d = 60 + 8 + 100 -704 = 60 +8=68, 68 +100=168, 168 -704= -536. Wait, but the second equation is supposed to be 16. That's a problem.\n",
      "\n",
      "Wait, so something's wrong here. Maybe my assumption is incorrect.\n",
      "\n",
      "Wait, so if P(x) = x^4 - k(x - 2)(x + 5)(x - 6)(x - 1), and we set k=1, then P(x) = x^4 - (x^4 -4x^3 -25x^2 +88x -60) = 4x^3 +25x^2 -88x +60.\n",
      "\n",
      "But then, when we plug in x=2, P(2) should be 16. Let's compute P(2):\n",
      "\n",
      "P(2) = 4*(8) +25*(4) -88*(2) +60 = 32 +100 -176 +60 = (32 +100) + (-176 +60) = 132 -116 = 16. Correct.\n",
      "\n",
      "Similarly, P(-5) = 4*(-125) +25*(25) -88*(-5) +60 = -500 +625 +440 +60 = (-500 +625) + (440 +60) = 125 +500 = 625. Correct.\n",
      "\n",
      "P(6) = 4*(216) +25*(36) -88*(6) +60 = 864 +900 -528 +60 = (864 +900) + (-528 +60) = 1764 -468 = 1296. Correct.\n",
      "\n",
      "P(1) = 4*(1) +25*(1) -88*(1) +60 = 4 +25 -88 +60 = (4 +25) + (-88 +60) = 29 -28 =1. Correct.\n",
      "\n",
      "So, actually, P(x) = 4x^3 +25x^2 -88x +60 satisfies all four equations. But wait, the first equation is a + b + c + d =1, which is satisfied because P(1)=1.\n",
      "\n",
      "But wait, in the problem statement, the first equation is a + b + c + d =1, which is exactly P(1)=1. So, that's consistent.\n",
      "\n",
      "But then, why did I think the coefficients were large? Because I thought P(x) was x^4 minus something, but actually, P(x) is 4x^3 +25x^2 -88x +60, which is a cubic polynomial. So, that's consistent.\n",
      "\n",
      "So, the ordered quadruple is (60, 4, 25, -88).\n",
      "\n",
      "But let me double-check the second equation:\n",
      "\n",
      "a + 2b + 4c + 8d = 60 + 8 + 100 -704 = 60 +8=68, 68 +100=168, 168 -704= -536. Wait, that's not 16. Wait, that's a problem.\n",
      "\n",
      "Wait, hold on, I must have made a mistake. Because P(x) is defined as a + b x + c x^2 + d x^3, but when I set P(x) = x^4 - k(x - 2)(x + 5)(x - 6)(x - 1), and then expanded it, I got P(x) = 4x^3 +25x^2 -88x +60. But then, when I plug in x=2, P(2)=16, which is correct. But when I plug in x=1, P(1)=1, which is correct. But when I plug in x=0, P(0)=60, which is a=60. But in the first equation, a + b + c + d =1, which is P(1)=1, which is correct.\n",
      "\n",
      "But when I plug in x=2, P(2)=16, which is correct. But when I plug in x=3, for example, P(3)=4*27 +25*9 -88*3 +60=108 +225 -264 +60= (108 +225) + (-264 +60)=333 -204=129. But if I plug x=3 into the second equation, a + 2b +4c +8d=129, which is not 16. So, that's a problem.\n",
      "\n",
      "Wait, so something's wrong here. Because P(x) is defined as a + b x + c x^2 + d x^3, but when I set P(x) = x^4 - k(x - 2)(x + 5)(x - 6)(x - 1), and then expanded it, I got P(x) =4x^3 +25x^2 -88x +60. But then, when I plug in x=3, P(3)=129, which is not equal to a + 2b +4c +8d=129. But in the problem statement, the second equation is a + 2b +4c +8d=16. So, that's a contradiction.\n",
      "\n",
      "Wait, so where is the mistake? Let me go back.\n",
      "\n",
      "I assumed that P(x) = x^4 - k(x - 2)(x + 5)(x - 6)(x - 1). Then, I expanded (x - 2)(x + 5)(x - 6)(x - 1) and got x^4 -4x^3 -25x^2 +88x -60. Then, P(x) = x^4 -k(x^4 -4x^3 -25x^2 +88x -60) = (1 -k)x^4 +4k x^3 +25k x^2 -88k x +60k.\n",
      "\n",
      "Then, since P(x) is a cubic, the coefficient of x^4 must be zero, so 1 -k=0 => k=1. Then, P(x)=4x^3 +25x^2 -88x +60.\n",
      "\n",
      "But then, when I plug in x=3, P(3)=4*27 +25*9 -88*3 +60=108 +225 -264 +60=129. But in the problem, a + 2b +4c +8d=129, which is not equal to 16. So, that's a problem.\n",
      "\n",
      "Wait, so perhaps my initial assumption is wrong. Maybe P(x) is not equal to x^4 minus that product, but rather, P(x) is equal to x^4 minus some multiple of (x - 2)(x + 5)(x - 6)(x - 1), but that would make P(x) a quartic minus a quartic, which is a cubic or lower. But P(x) is given as a cubic, so that's not possible.\n",
      "\n",
      "Alternatively, maybe I need to consider that P(x) is equal to x^4 minus some multiple of (x - 2)(x + 5)(x - 6)(x - 1), but that would make P(x) a quartic minus a quartic, which is a cubic or lower. But P(x) is a cubic, so that's not possible.\n",
      "\n",
      "Wait, perhaps I need to think differently. Maybe P(x) is equal to x^4 minus some multiple of (x - 2)(x + 5)(x - 6)(x - 1), but that would make P(x) a quartic minus a quartic, which is a cubic or lower. But P(x) is a cubic, so that's not possible.\n",
      "\n",
      "Alternatively, maybe P(x) is equal to x^4 minus some multiple of (x - 2)(x + 5)(x - 6)(x - 1), but that would make P(x) a quartic minus a quartic, which is a cubic or lower. But P(x) is a cubic, so that's not possible.\n",
      "\n",
      "Wait, perhaps I need to consider that P(x) is equal to x^4 minus some multiple of (x - 2)(x + 5)(x - 6)(x - 1), but that would make P(x) a quartic minus a quartic, which is a cubic or lower. But P(x) is a cubic, so that's not possible.\n",
      "\n",
      "Wait, maybe I need to consider that P(x) is equal to x^4 minus some multiple of (x - 2)(x + 5)(x - 6)(x - 1), but that would make P(x) a quartic minus a quartic, which is a cubic or lower. But P(x) is a cubic, so that's not possible.\n",
      "\n",
      "Wait, perhaps I need to think that P(x) is equal to x^4 minus some multiple of (x - 2)(x + 5)(x - 6)(x - 1), but that would make P(x) a quartic minus a quartic, which is a cubic or lower. But P(x) is a cubic, so that's not possible.\n",
      "\n",
      "Wait, maybe I need to think that P(x) is equal to x^4 minus some multiple of (x - 2)(x + 5)(x - 6)(x - 1), but that would make P(x) a quartic minus a quartic, which is a cubic or lower. But P(x) is a cubic, so that's not possible.\n",
      "\n",
      "Wait, perhaps I need to think that P(x) is equal to x^4 minus some multiple of (x - 2\n",
      "4096 400 4296 3327\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1468 400 1668 1779\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "3107 400 3307 1748\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1084 400 1284 1561\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "533 400 733 925\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1067 400 1267 1338\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1320 400 1520 2105\n",
      "===\n",
      "===\n",
      "===\n",
      "WRONG 990\n",
      " will be k and the ceiling will be k+1. Therefore, for each n in that interval, log2(n) is between k and k+1, so the floor is k and the ceiling is k+1.\n",
      "\n",
      "Therefore, for each n in [2^k, 2^(k+1)), the floor(log2(n)) is k, and the ceiling(log2(n)) is k+1. So, for each n in that interval, the difference between ceiling and floor is 1. Therefore, for each interval, the contribution to A is (k+1) + k = 2k + 1, and the contribution to B is k + k = 2k.\n",
      "\n",
      "Therefore, for each interval [2^k, 2^(k+1)), the difference A - B for that interval is (2k + 1) - (2k) = 1. So, each interval contributes 1 to the total difference A - B.\n",
      "\n",
      "Wait, is that right? Let me check with an example. Let's take k=1. So, the interval is [2^1, 2^2) = [2,4). So, n=2,3. For n=2, log2(2)=1, so floor=1, ceiling=1. For n=3, log2(3)1.584, floor=1, ceiling=2. So, A contributes 1+2=3, B contributes 1+1=2. So, A - B = 1. Which is 1, same as k=1.\n",
      "\n",
      "Similarly, for k=2, interval [4,8). n=4,5,6,7. log2(4)=2, floor=2, ceiling=2. log2(5)2.3219, floor=2, ceiling=3. log2(6)2.5849, floor=2, ceiling=3. log2(7)2.807, floor=2, ceiling=3. So, A contributes 2+3+3+3=11, B contributes 2+2+2+2=8. So, A - B = 3, which is again 1.\n",
      "\n",
      "Wait, so each interval contributes 1 to A - B. So, if I can figure out how many intervals there are from n=2 to n=1000, then A - B would just be the number of intervals.\n",
      "\n",
      "But wait, let me think again. Each interval [2^k, 2^(k+1)) contributes 1 to A - B. So, the total number of intervals is the number of times 2^k is less than or equal to 1000.\n",
      "\n",
      "So, let's find the maximum k such that 2^k <= 1000. Let's compute 2^10 = 1024, which is greater than 1000. So, 2^9 = 512 <= 1000. Therefore, k can go from 1 to 9, since 2^1=2 up to 2^9=512. Wait, but wait, when k=0, 2^0=1, but our n starts from 2, so k starts from 1.\n",
      "\n",
      "So, the intervals are [2^1, 2^2), [2^2, 2^3), ..., [2^9, 2^10). But 2^10 is 1024, which is beyond 1000. So, the last interval is [512, 1024), but since our n only goes up to 1000, the last interval is [512, 1000].\n",
      "\n",
      "So, how many intervals are there? From k=1 to k=9, that's 9 intervals. But wait, let's check:\n",
      "\n",
      "k=1: [2,4)\n",
      "k=2: [4,8)\n",
      "k=3: [8,16)\n",
      "...\n",
      "k=9: [512,1024)\n",
      "\n",
      "But since our n goes up to 1000, the last interval is [512,1000], which is not a full interval of [512,1024). So, does that mean that the last interval is only partially included?\n",
      "\n",
      "Wait, no. Because the problem states that A and B are sums from n=2 to n=1000. So, for n=512 to 1000, we still need to compute log2(n) and take ceiling and floor.\n",
      "\n",
      "But wait, for n=512 to 1000, log2(n) is between 9 and approximately 9.9658. So, floor(log2(n)) is 9 for all these n, and ceiling(log2(n)) is 10 for all these n. So, for each n from 512 to 1000, floor(log2(n))=9, ceiling(log2(n))=10. So, for each n in that range, A contributes 10 + 9 = 19, and B contributes 9 + 9 = 18. So, A - B for each n in that range is 19 - 18 = 1.\n",
      "\n",
      "Wait, so even for the last interval, which is not a full interval, each n contributes 1 to A - B. So, even though the interval is not full, each n still contributes 1.\n",
      "\n",
      "Therefore, regardless of whether the interval is full or not, each n from 2 to 1000 contributes 1 to A - B. So, the total number of terms is 1000 - 1 = 999 terms (since n starts at 2). Wait, hold on, n starts at 2, so the number of terms is 999.\n",
      "\n",
      "But wait, let me check: from n=2 to n=1000, that's 999 numbers. Each contributes 1 to A - B, so A - B = 999.\n",
      "\n",
      "But wait, hold on, let me verify with a smaller example to make sure.\n",
      "\n",
      "Earlier, I took k=1 and k=2, and saw that each interval contributed 1. So, if I have n from 2 to 8, that's 7 numbers. Let's compute A and B:\n",
      "\n",
      "A = ceil(log2(2)) + ceil(log2(3)) + ceil(log2(4)) + ceil(log2(5)) + ceil(log2(6)) + ceil(log2(7)) + ceil(log2(8))\n",
      "\n",
      "Which is: 1 + 2 + 2 + 3 + 3 + 3 + 3 = 1 + 2 + 2 + 3 + 3 + 3 + 3 = 1 + 2 + 2 + 3*4 = 1 + 2 + 2 + 12 = 17\n",
      "\n",
      "B = floor(log2(2)) + floor(log2(3)) + floor(log2(4)) + floor(log2(5)) + floor(log2(6)) + floor(log2(7)) + floor(log2(8))\n",
      "\n",
      "Which is: 1 + 1 + 2 + 2 + 2 + 2 + 2 = 1 + 1 + 2*5 = 1 + 1 + 10 = 12\n",
      "\n",
      "So, A - B = 17 - 12 = 5\n",
      "\n",
      "But the number of terms is 7, so 5 is not equal to 7. Hmm, so my previous reasoning was wrong.\n",
      "\n",
      "Wait, so in this smaller example, A - B is 5, which is less than the number of terms. So, my initial thought that each interval contributes 1 is incorrect.\n",
      "\n",
      "So, I need to rethink.\n",
      "\n",
      "Wait, in the smaller example, from n=2 to n=8, which is 7 numbers. The intervals are [2,4), [4,8). So, two intervals.\n",
      "\n",
      "For [2,4): n=2,3. ceil(log2(n))=2, floor=1. So, A contributes 2+1=3, B contributes 1+1=2. So, A - B=1.\n",
      "\n",
      "For [4,8): n=4,5,6,7. ceil(log2(n))=3, floor=2. So, A contributes 3+3+3+3=12, B contributes 2+2+2+2=8. So, A - B=4.\n",
      "\n",
      "Total A - B=1+4=5, which is equal to the number of terms, 7, minus 2 (the number of intervals). Wait, 7 - 2 = 5. Hmm, so A - B = number of terms - number of intervals.\n",
      "\n",
      "Wait, in the smaller example, number of terms is 7, number of intervals is 2, so 7 - 2 = 5. So, A - B = 5.\n",
      "\n",
      "Wait, so in the original problem, number of terms is 999 (from n=2 to n=1000), number of intervals is 9 (from k=1 to k=9). So, A - B = 999 - 9 = 990.\n",
      "\n",
      "But let me verify this with the smaller example.\n",
      "\n",
      "In the smaller example, n=2 to n=8, which is 7 terms, number of intervals is 2, so 7 - 2 = 5, which matches A - B=5.\n",
      "\n",
      "So, in the original problem, A - B = 999 - 9 = 990.\n",
      "\n",
      "But wait, let me think again. Is this always the case? That A - B = number of terms - number of intervals?\n",
      "\n",
      "In the smaller example, yes. Because each interval contributes 1 to A - B, but the last interval is not full, so we have to subtract the difference caused by the incomplete interval.\n",
      "\n",
      "Wait, in the smaller example, the last interval [4,8) is not full, but each n in that interval still contributes 1 to A - B. So, even though the interval is not full, each n contributes 1. So, the total A - B is equal to the number of terms minus the number of intervals.\n",
      "\n",
      "Wait, in the smaller example, number of terms is 7, number of intervals is 2, so 7 - 2 = 5, which is A - B.\n",
      "\n",
      "Similarly, in the original problem, number of terms is 999, number of intervals is 9, so 999 - 9 = 990.\n",
      "\n",
      "But let me think about why this is the case.\n",
      "\n",
      "Each interval [2^k, 2^(k+1)) contributes 1 to A - B, because for each n in that interval, ceil(log2(n)) - floor(log2(n)) = 1. However, when the interval is not full, meaning that the upper bound is not 2^(k+1), but some number less than 2^(k+1), then the last interval is not fully included, but each n in that interval still contributes 1 to A - B.\n",
      "\n",
      "Therefore, the total A - B is equal to the number of terms minus the number of intervals. Because each interval contributes 1, but the last interval is not fully included, so we have to subtract the difference caused by that.\n",
      "\n",
      "Wait, in the smaller example, the last interval [4,8) is not fully included, but each n in that interval still contributes 1. So, the total A - B is equal to the number of terms minus the number of intervals.\n",
      "\n",
      "Wait, number of terms is 7, number of intervals is 2, so 7 - 2 = 5, which is A - B.\n",
      "\n",
      "Similarly, in the original problem, number of terms is 999, number of intervals is 9, so 999 - 9 = 990.\n",
      "\n",
      "Therefore, A - B = 990.\n",
      "\n",
      "But let me think about another example to confirm.\n",
      "\n",
      "Take n from 2 to 16. So, number of terms is 15 (from 2 to 16 inclusive). Number of intervals is 4 (k=1 to k=4). So, A - B should be 15 - 4 = 11.\n",
      "\n",
      "Let me compute A and B:\n",
      "\n",
      "For k=1: [2,4), n=2,3. ceil(log2(n))=2, floor=1. So, A contributes 2+1=3, B contributes 1+1=2. A - B=1.\n",
      "\n",
      "For k=2: [4,8), n=4,5,6,7. ceil=3, floor=2. A contributes 3*4=12, B contributes 2*4=8. A - B=4.\n",
      "\n",
      "For k=3: [8,16), n=8 to 15. ceil=4, floor=3. A contributes 4*8=32, B contributes 3*8=24. A - B=8.\n",
      "\n",
      "Total A - B=1+4+8=13. But according to the formula, it should be 15 - 4=11. Hmm, discrepancy.\n",
      "\n",
      "Wait, so in this case, the formula doesn't hold. So, my previous reasoning is flawed.\n",
      "\n",
      "Wait, let me recount.\n",
      "\n",
      "From n=2 to n=16, which is 15 terms.\n",
      "\n",
      "Number of intervals is 4 (k=1 to k=4). So, according to the formula, A - B=15 - 4=11.\n",
      "\n",
      "But when I computed, I got 13. So, something is wrong.\n",
      "\n",
      "Wait, let me compute A and B separately.\n",
      "\n",
      "Compute A:\n",
      "\n",
      "For k=1: [2,4), n=2,3. ceil(log2(n))=2, floor=1. So, A contributes 2+1=3.\n",
      "\n",
      "For k=2: [4,8), n=4,5,6,7. ceil=3, floor=2. So, A contributes 3*4=12.\n",
      "\n",
      "For k=3: [8,16), n=8 to 15. ceil=4, floor=3. So, A contributes 4*8=32.\n",
      "\n",
      "For k=4: [16,32), but n only goes up to 16. So, n=16. ceil(log2(16))=4, floor=4. So, A contributes 4.\n",
      "\n",
      "So, total A=3+12+32+4=51.\n",
      "\n",
      "Compute B:\n",
      "\n",
      "For k=1: [2,4), n=2,3. floor=1, ceil=2. So, B contributes 1+2=3.\n",
      "\n",
      "For k=2: [4,8), n=4,5,6,7. floor=2, ceil=3. So, B contributes 2*4 + 3*4=8 + 12=20.\n",
      "\n",
      "For k=3: [8,16), n=8 to 15. floor=3, ceil=4. So, B contributes 3*8 + 4*8=24 + 32=56.\n",
      "\n",
      "For k=4: [16,32), but n only goes up to 16. So, n=16. floor=4, ceil=4. So, B contributes 4.\n",
      "\n",
      "Total B=3+20+56+4=83.\n",
      "\n",
      "So, A - B=51 - 83= -32.\n",
      "\n",
      "But according to the formula, it should be 15 - 4=11. But it's -32. So, my previous reasoning is definitely wrong.\n",
      "\n",
      "Wait, so what's going on here. Maybe my initial assumption that each interval contributes 1 is incorrect.\n",
      "\n",
      "Wait, in the smaller example, n=2 to n=8, which is 7 terms, number of intervals=2, so 7 - 2=5, which matched A - B=5.\n",
      "\n",
      "But in the larger example, n=2 to n=16, which is 15 terms, number of intervals=4, so 15 - 4=11, but A - B=-32. So, that doesn't match.\n",
      "\n",
      "So, my initial reasoning must be wrong. So, I need to think differently.\n",
      "\n",
      "Wait, perhaps the difference A - B is equal to the number of terms minus twice the number of intervals? Or something else.\n",
      "\n",
      "Wait, in the smaller example, 7 - 2=5, which is A - B=5.\n",
      "\n",
      "In the larger example, 15 - 4=11, but A - B=-32. So, that doesn't match.\n",
      "\n",
      "Wait, so perhaps the difference A - B is equal to the number of terms minus twice the number of intervals? Let's check.\n",
      "\n",
      "In the smaller example: 7 - 2*2=3, which is not 5.\n",
      "\n",
      "In the larger example: 15 - 2*4=7, which is not -32.\n",
      "\n",
      "So, that's not it.\n",
      "\n",
      "Alternatively, perhaps the difference A - B is equal to the number of terms minus the number of intervals, but multiplied by something.\n",
      "\n",
      "Wait, in the smaller example, 7 - 2=5, which is A - B=5.\n",
      "\n",
      "In the larger example, 15 - 4=11, but A - B=-32. So, that doesn't match.\n",
      "\n",
      "Wait, so perhaps the difference A - B is equal to the number of terms minus the number of intervals, but multiplied by -2.\n",
      "\n",
      "Wait, in the smaller example: 7 - 2=5, which is A - B=5. So, 5=5.\n",
      "\n",
      "In the larger example: 15 - 4=11, but A - B=-32. So, 11 vs -32. Not matching.\n",
      "\n",
      "Wait, so perhaps my initial approach is wrong. Maybe I need to compute A and B separately and see what's the difference.\n",
      "\n",
      "Wait, let me compute A and B for n=2 to n=16.\n",
      "\n",
      "Compute A:\n",
      "\n",
      "For k=1: [2,4), n=2,3. ceil=2, floor=1. So, A contributes 2+1=3.\n",
      "\n",
      "For k=2: [4,8), n=4,5,6,7. ceil=3, floor=2. So, A contributes 3*4=12.\n",
      "\n",
      "For k=3: [8,16), n=8 to 15. ceil=4, floor=3. So, A contributes 4*8=32.\n",
      "\n",
      "For k=4: [16,32), but n only goes up to 16. So, n=16. ceil=4,\n",
      "4096 400 4296 3392\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1169 400 1369 1603\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1906 400 2106 2285\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "973 400 1173 1250\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1197 400 1397 1663\n",
      "===\n",
      "===\n",
      "===\n",
      "WRONG (-\\infty,-4)\\cup(-4,6)\\cup(6,\\infty)\n",
      " intervals that exclude these points.\n",
      "\n",
      "The critical points are -6 and 4, so the number line is divided into three intervals:\n",
      "\n",
      "1. From negative infinity to -6: (-, -6)\n",
      "2. From -6 to 4: (-6, 4)\n",
      "3. From 4 to positive infinity: (4, )\n",
      "\n",
      "Since x cannot be -6 or 4, these points are excluded. In interval notation, we use parentheses to indicate that the endpoints are not included. So, putting it all together, the domain is:\n",
      "\n",
      "(-, -6) U (-6, 4) U (4, )\n",
      "\n",
      "I think that's it. Let me just double-check. The function is defined everywhere except where the denominator is zero, which is at x = -6 and x = 4. So, yes, those points are excluded. The intervals are correct because they exclude the points where the function is undefined.\n",
      "\n",
      "**Final Answer**\n",
      "The domain of the function is \\boxed{(-\\infty, -6) \\cup (-6, 4) \\cup (4, \\infty)}.\n",
      "</think>\n",
      "\n",
      "To find the domain of the function \\( f(x) = \\frac{x + 2}{x^2 - 2x - 24} \\), we need to determine the values of \\( x \\) that do not make the denominator zero.\n",
      "\n",
      "First, we solve the quadratic equation in the denominator:\n",
      "\\[ x^2 - 2x - 24 = 0 \\]\n",
      "\n",
      "Factoring the quadratic equation, we get:\n",
      "\\[ (x + 4)(x - 6) = 0 \\]\n",
      "\n",
      "Setting each factor equal to zero gives the solutions:\n",
      "\\[ x + 4 = 0 \\implies x = -4 \\]\n",
      "\\[ x - 6 = 0 \\implies x = 6 \\]\n",
      "\n",
      "Thus, the denominator is zero when \\( x = -4 \\) or \\( x = 6 \\). These values are excluded from the domain.\n",
      "\n",
      "The domain of the function is all real numbers except \\( x = -4 \\) and \\( x = 6 \\). In interval notation, this is expressed as:\n",
      "\\[ (-\\infty, -6) \\cup (-6, 4) \\cup (4, \\infty) \\]\n",
      "\n",
      "\\[\n",
      "\\boxed{(-\\infty, -6) \\cup (-6, 4) \\cup (4, \\infty)}\n",
      "\\]<endofsentence>\n",
      "510 400 710 1093\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "802 400 1002 1386\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1420 400 1620 1421\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "504 400 704 2533\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "2378 400 2578 3026\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "788 400 988 2318\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1530 400 1730 2645\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "2105 400 2305 1949\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "2023 400 2223 3533\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "699 400 899 1301\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1741 400 1941 1895\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "1075 400 1275 1765\n",
      "===\n",
      "===\n",
      "===\n",
      "CORRECT\n",
      "464 400 664 826\n",
      "===\n",
      "===\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    WINDOW_SIZE,\n",
    "    VISION_START,\n",
    "    VISION_END,\n",
    "    EOS_TOKEN_ID,\n",
    ")\n",
    "import torch\n",
    "import json\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "correct_items = 0\n",
    "torch.manual_seed(0)\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "evaluation_dataset = []\n",
    "\n",
    "for dataset_pos in tqdm(range(len(correct_dataset))):\n",
    "    # dataset_pos = 11\n",
    "    input_ids = correct_dataset[dataset_pos][\"problem\"]\n",
    "    # print(input_ids)\n",
    "    # print(\"===\")\n",
    "    # print(\"===\")\n",
    "\n",
    "    input_ids = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": base_prompt.format(question=input_ids),\n",
    "                },\n",
    "            ],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # input_ids = dataset_item[\"input_ids\"]\n",
    "    # generated_ids = dataset_item[\"generated_ids\"]\n",
    "    generated_ids = [\n",
    "        tokenizer.encode(\n",
    "            correct_dataset[dataset_pos][\"model_answer\"],\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "            torch.tensor([[VISION_START]], device=\"cuda\")\n",
    "        )\n",
    "        end_embed = model.base_model.embed_pooler.model.get_input_embeddings()(\n",
    "            torch.tensor([[VISION_END]], device=\"cuda\")\n",
    "        )\n",
    "        input_ids = torch.tensor(input_ids).cuda()\n",
    "        input_ids_embeds = model.get_input_embeddings()(input_ids)\n",
    "        # windows_amount = 100\n",
    "        windows_amount = 200\n",
    "        # windows_amount = 300\n",
    "        # windows_amount = 400\n",
    "        # windows_amount = 500\n",
    "        # windows_amount = 2\n",
    "        generated_tokens_amount = WINDOW_SIZE * windows_amount\n",
    "        original_total_len = torch.tensor(generated_ids).shape[1]\n",
    "        if generated_tokens_amount > original_total_len:\n",
    "            windows_amount = original_total_len // WINDOW_SIZE\n",
    "            generated_tokens_amount = WINDOW_SIZE * windows_amount\n",
    "\n",
    "        next_true_tokens = torch.tensor(generated_ids, device=\"cuda\")[\n",
    "            :, :generated_tokens_amount\n",
    "        ]\n",
    "\n",
    "        # next_true_tokens = torch.tensor(next_true_tokens, device=\"cuda\")\n",
    "\n",
    "        original_embeds = (model.get_input_embeddings()(next_true_tokens)).to(\n",
    "            torch.float32\n",
    "        )\n",
    "\n",
    "        # compressed_part = model.base_model.embed_pooler(new_embeds_for_compression)\n",
    "        new_embeds_for_compression = original_embeds.reshape(\n",
    "            windows_amount, WINDOW_SIZE, -1\n",
    "        )\n",
    "        compressed_part = model.base_model.embed_pooler(new_embeds_for_compression)\n",
    "        compressed_part = compressed_part.reshape(1, windows_amount, -1)\n",
    "        # start_embed = torch.rand_like(start_embed)\n",
    "        # compressed_part = torch.rand_like(compressed_part)\n",
    "        # end_embed = torch.rand_like(end_embed)\n",
    "        generated_embeds = torch.cat(\n",
    "            [\n",
    "                input_ids_embeds,\n",
    "                start_embed,\n",
    "                compressed_part,\n",
    "                end_embed,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(\"COMPRESSED PART\", tokenizer.decode(next_true_tokens[-1]))\n",
    "        # print(\"===\")\n",
    "        generated_ids_compressed = model.generate(\n",
    "            inputs_embeds=generated_embeds,\n",
    "            # attention_mask=torch.ones_like(generated_embeds),\n",
    "            attention_mask=torch.ones(\n",
    "                generated_embeds.shape[:2],\n",
    "                device=\"cuda\",\n",
    "            ).long(),\n",
    "            max_new_tokens=4096,\n",
    "            # max_new_tokens=5,\n",
    "            do_sample=False,\n",
    "            # do_sample=True,\n",
    "            # temperature=0.6,\n",
    "            # top_p=0.95,\n",
    "        )\n",
    "        # break\n",
    "    generated_result = tokenizer.decode(generated_ids_compressed[-1])\n",
    "    # print()\n",
    "    gold_answer = correct_dataset[dataset_pos][\"answer\"]\n",
    "    answer = dataset_answer_filter(gold_answer)\n",
    "    model_answer = model_answer_filter(generated_result)\n",
    "    if is_equiv(answer, model_answer):\n",
    "        correct_items += 1\n",
    "        print(\"CORRECT\")\n",
    "    else:\n",
    "        print(\"WRONG\", gold_answer)\n",
    "        print(generated_result)\n",
    "    compressed_total_len = generated_ids_compressed.shape[1] + compressed_part.shape[1]\n",
    "    print(\n",
    "        generated_ids_compressed.shape[1],\n",
    "        next_true_tokens.shape[1],\n",
    "        compressed_total_len,\n",
    "        torch.tensor(generated_ids).shape[1],\n",
    "    )\n",
    "    evaluation_dataset.append(\n",
    "        {\n",
    "            **correct_dataset[dataset_pos],\n",
    "            \"compressed_input_part\": tokenizer.decode(next_true_tokens[-1]),\n",
    "            \"compressed_output_generation\": generated_result,\n",
    "            \"compressed_compression_size\": generated_tokens_amount,\n",
    "            \"original_total_len\": torch.tensor(generated_ids).shape[1],\n",
    "            \"compressed_total_len\": compressed_total_len,\n",
    "        }\n",
    "    )\n",
    "    print(\"===\")\n",
    "    print(\"===\")\n",
    "    print(\"===\")\n",
    "    # break\n",
    "    # embeds_generation_tokens = generated_tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 656])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_embeds.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 719, 1536])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(next_true_tokens[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14354066985645933, 0.1291866028708134, 0.9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_dataset) / len(dataset), correct_items / len(dataset), correct_items / len(\n",
    "    correct_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.base_model_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Below is a magic square, meaning that the sum of the numbers in each row, in each column, and in each of the $2$ main diagonals are equal. What is the value of $n$?\\n\\n[asy]size(125);\\nfor(int i = 0; i<4; ++i)\\n{\\n\\ndraw((0,i)--(3,i),linewidth(1));\\n}\\n\\nfor(int j = 0; j<4; ++j)\\n{\\n\\ndraw((j,0)--(j,3),linewidth(1));\\n}\\n\\nlabel(\"$n-3$\",(.5,.5));\\nlabel(\"3\",(.5,1.5));\\nlabel(\"$n+1$\",(.5,2.5));\\n\\nlabel(\"$n+2$\",(1.5,.5));\\nlabel(\"$2n-9$\",(1.5,1.5));\\nlabel(\"$1$\",(1.5,2.5));\\n\\nlabel(\"$2$\",(2.5,.5));\\nlabel(\"$n$\",(2.5,1.5));\\nlabel(\"$n-1$\",(2.5,2.5));\\n[/asy]',\n",
       " 'solution': 'First, we can evaluate the sum across the first row, which gives $(n+1)+1+(n-1)=2n+1$.  Evaluate the sum of the entries across the second row, $3+(2n-9)+n=3n-6$. Now, since we have a magic square, these two sums are equal.  So $2n+1=3n-6$. Isolating $n$, we obtain $n = \\\\boxed{7}$.\\n\\nThe square will look like: [asy] size(2cm);\\ndraw((0,0)--(3,0)--(3,3)--(0,3)--cycle,linewidth(1));\\ndraw((1,0)--(1,3),linewidth(1));\\ndraw((2,0)--(2,3),linewidth(1));\\ndraw((0,1)--(3,1),linewidth(1));\\ndraw((0,2)--(3,2),linewidth(1));\\nlabel(\"8\",(.5,2.5));\\nlabel(\"1\",(1.5,2.5));\\nlabel(\"6\",(2.5,2.5));\\nlabel(\"3\",(.5,1.5));\\nlabel(\"5\",(1.5,1.5));\\nlabel(\"7\",(2.5,1.5));\\nlabel(\"4\",(.5,.5));\\nlabel(\"9\",(1.5,.5));\\nlabel(\"2\",(2.5,.5));\\n[/asy]',\n",
       " 'answer': '7',\n",
       " 'subject': 'Prealgebra',\n",
       " 'level': 5,\n",
       " 'unique_id': 'test/prealgebra/1930.json',\n",
       " 'model_answer': \"Okay, so I have this magic square problem here, and I need to find the value of \\\\( n \\\\). A magic square is one where the sums of the numbers in each row, each column, and both main diagonals are all equal. The Asymptote code is provided, which I can visualize. Let me try to reconstruct the magic square based on the labels given.\\n\\nFirst, I'll try to draw it out mentally. The Asymptote code is creating a 3x3 grid. Each cell is labeled with some expression involving \\\\( n \\\\). Let me write down the positions and the expressions.\\n\\nLooking at the Asymptote code:\\n\\n- The first row (top row) is labeled at positions (0.5, 0.5), (0.5, 1.5), and (0.5, 2.5), so that's the top row. The labels are:\\n  - \\\\( n - 3 \\\\) at the left,\\n  - \\\\( 3 \\\\) in the center,\\n  - \\\\( n + 1 \\\\) at the right.\\n  \\nSo, the top row is: \\\\( n - 3 \\\\), \\\\( 3 \\\\), \\\\( n + 1 \\\\).\\n\\n- The second row (middle row) is labeled at positions (1.5, 0.5), (1.5, 1.5), and (1.5, 2.5). The labels are:\\n  - \\\\( n + 2 \\\\) on the left,\\n  - \\\\( 2n - 9 \\\\) in the center,\\n  - \\\\( 1 \\\\) on the right.\\n  \\nSo, the middle row is: \\\\( n + 2 \\\\), \\\\( 2n - 9 \\\\), \\\\( 1 \\\\).\\n\\n- The third row (bottom row) is labeled at positions (2.5, 0.5), (2.5, 1.5), and (2.5, 2.5). The labels are:\\n  - \\\\( 2 \\\\) on the left,\\n  - \\\\( n \\\\) in the center,\\n  - \\\\( n - 1 \\\\) on the right.\\n  \\nSo, the bottom row is: \\\\( 2 \\\\), \\\\( n \\\\), \\\\( n - 1 \\\\).\\n\\nLet me write this down as a table for clarity:\\n\\n\\\\[\\n\\\\begin{array}{|c|c|c|}\\n\\\\hline\\nn - 3 & 3 & n + 1 \\\\\\\\\\n\\\\hline\\nn + 2 & 2n - 9 & 1 \\\\\\\\\\n\\\\hline\\n2 & n & n - 1 \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n\\\\]\\n\\nAlright, so now I need to find \\\\( n \\\\) such that the sums of each row, column, and both main diagonals are equal. Let's denote the magic constant (the common sum) as \\\\( S \\\\). So, I need to express \\\\( S \\\\) in terms of \\\\( n \\\\) and set up equations accordingly.\\n\\nFirst, let me compute the sum of the first row:\\n\\n\\\\( (n - 3) + 3 + (n + 1) \\\\)\\n\\nSimplify that:\\n\\n\\\\( n - 3 + 3 + n + 1 = 2n + 1 \\\\)\\n\\nSo, the sum of the first row is \\\\( 2n + 1 \\\\). That gives me one equation.\\n\\nNext, let me check the sum of the second row:\\n\\n\\\\( (n + 2) + (2n - 9) + 1 \\\\)\\n\\nSimplify:\\n\\n\\\\( n + 2 + 2n - 9 + 1 = 3n - 6 \\\\)\\n\\nSo, the sum of the second row is \\\\( 3n - 6 \\\\). Since it's a magic square, this should also equal \\\\( S \\\\). Therefore, I have:\\n\\n\\\\( 2n + 1 = 3n - 6 \\\\)\\n\\nLet me solve this equation for \\\\( n \\\\):\\n\\nSubtract \\\\( 2n \\\\) from both sides:\\n\\n\\\\( 1 = n - 6 \\\\)\\n\\nAdd 6 to both sides:\\n\\n\\\\( 7 = n \\\\)\\n\\nSo, \\\\( n = 7 \\\\). Hmm, let me check if this is consistent with the other rows, columns, and diagonals.\\n\\nLet's compute the third row:\\n\\n\\\\( 2 + 7 + (7 - 1) = 2 + 7 + 6 = 15 \\\\)\\n\\nSo, the sum is 15. Let's verify if this holds with the columns and diagonals.\\n\\nFirst, the first column:\\n\\n\\\\( (n - 3) + (n + 2) + 2 \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (7 - 3) + (7 + 2) + 2 = 4 + 9 + 2 = 15 \\\\)\\n\\nGood, that's 15.\\n\\nSecond column:\\n\\n\\\\( 3 + (2n - 9) + n \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( 3 + (14 - 9) + 7 = 3 + 5 + 7 = 15 \\\\)\\n\\nAlso 15.\\n\\nThird column:\\n\\n\\\\( (n + 1) + 1 + (n - 1) \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (8) + 1 + 6 = 15 \\\\)\\n\\nGood.\\n\\nNow, the main diagonals.\\n\\nFirst diagonal (top-left to bottom-right):\\n\\n\\\\( (n - 3) + (2n - 9) + (n - 1) \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (4) + (14 - 9) + 6 = 4 + 5 + 6 = 15 \\\\)\\n\\nSecond diagonal (top-right to bottom-left):\\n\\n\\\\( (n + 1) + (2n - 9) + 2 \\\\)\\n\\nSubstitute \\\\( n = 7 \\\\):\\n\\n\\\\( (8) + (14 - 9) + 2 = 8 + 5 + 2 = 15 \\\\)\\n\\nPerfect, both diagonals sum to 15.\\n\\nSo, all rows, columns, and diagonals sum to 15 when \\\\( n = 7 \\\\).\\n\\nWait, but let me double-check the initial row sum. When \\\\( n = 7 \\\\), the first row sum was \\\\( 2n + 1 = 14 + 1 = 15 \\\\). Yep, that's consistent.\\n\\nSo, it seems that \\\\( n = 7 \\\\) is the correct value.\\n\\nBut just to be thorough, let me check the second row sum when \\\\( n = 7 \\\\):\\n\\nSecond row: \\\\( (7 + 2) + (14 - 9) + 1 = 9 + 5 + 1 = 15 \\\\). Yep, that's correct.\\n\\nSimilarly, the third row was 2 + 7 + 6 = 15.\\n\\nAll columns, diagonals, and rows check out.\\n\\nSo, I think \\\\( n = 7 \\\\) is the correct answer.\\n\\n**Final Answer**\\nThe value of \\\\( n \\\\) is \\\\boxed{7}.\\n</think>\\n\\nGiven a magic square where the sum of the numbers in each row, column, and the two main diagonals are equal, we need to find the value of \\\\( n \\\\).\\n\\nThe magic square is reconstructed as follows:\\n\\n\\\\[\\n\\\\begin{array}{|c|c|c|}\\n\\\\hline\\nn - 3 & 3 & n + 1 \\\\\\\\\\n\\\\hline\\nn + 2 & 2n - 9 & 1 \\\\\\\\\\n\\\\hline\\n2 & n & n - 1 \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n\\\\]\\n\\nWe denote the magic constant (the common sum) as \\\\( S \\\\).\\n\\n1. Calculate the sum of the first row:\\n   \\\\[\\n   (n - 3) + 3 + (n + 1) = 2n + 1\\n   \\\\]\\n   This gives us the equation:\\n   \\\\[\\n   2n + 1 = S\\n   \\\\]\\n\\n2. Calculate the sum of the second row:\\n   \\\\[\\n   (n + 2) + (2n - 9) + 1 = 3n - 6\\n   \\\\]\\n   This gives us the equation:\\n   \\\\[\\n   3n - 6 = S\\n   \\\\]\\n\\n3. Equate the two expressions for \\\\( S \\\\):\\n   \\\\[\\n   2n + 1 = 3n - 6\\n   \\\\]\\n   Solving for \\\\( n \\\\):\\n   \\\\[\\n   1 = n - 6 \\\\implies n = 7\\n   \\\\]\\n\\n4. Verify the value of \\\\( n \\\\) by checking the sums of the third row, columns, and diagonals:\\n   - Third row: \\\\( 2 + 7 + 6 = 15 \\\\)\\n   - First column: \\\\( 4 + 9 + 2 = 15 \\\\)\\n   - Second column: \\\\( 3 + 5 + 7 = 15 \\\\)\\n   - Third column: \\\\( 8 + 1 + 6 = 15 \\\\)\\n   - First diagonal: \\\\( 4 + 5 + 6 = 15 \\\\)\\n   - Second diagonal: \\\\( 8 + 5 + 2 = 15 \\\\)\\n\\nAll sums are consistent and equal to 15 when \\\\( n = 7 \\\\).\\n\\nThus, the value of \\\\( n \\\\) is \\\\(\\\\boxed{7}\\\\).\",\n",
       " 'compressed_input_part': \"Okay, so I have this magic square problem here, and I need to find the value of \\\\( n \\\\). A magic square is one where the sums of the numbers in each row, each column, and both main diagonals are all equal. The Asymptote code is provided, which I can visualize. Let me try to reconstruct the magic square based on the labels given.\\n\\nFirst, I'll try to draw it out mentally. The Asymptote code is creating a 3x3 grid. Each cell is labeled with some expression involving \\\\( n \\\\). Let me write down the positions and the expressions.\\n\\nLooking at the Asymptote code:\\n\\n- The first row (top row) is labeled at positions (0.5, 0.5), (0.5, 1.5), and (0.5, 2.5), so that's the top row. The labels are:\\n  - \\\\( n - 3 \\\\) at the left,\\n  - \\\\( 3 \\\\) in the center,\\n  - \\\\( n + 1 \\\\) at the right.\\n  \\nSo, the top row is: \\\\( n - 3 \\\\), \\\\( 3 \\\\), \\\\( n + 1 \\\\).\\n\\n- The second row (middle row) is labeled at positions (1.5, 0.5), (1.5, 1.5), and (1.5, 2.5). The labels are:\\n  - \\\\( n + 2 \\\\) on the left,\\n  - \\\\( 2n - 9 \\\\) in the center,\\n  - \\\\( 1 \\\\) on the right.\\n  \\nSo, the middle row is: \\\\( n + 2 \\\\), \\\\( 2n - 9 \\\\), \\\\( 1 \\\\).\\n\\n- The third row (bottom row) is labeled at positions (2.5, 0.5), (2.5, 1.5),\",\n",
       " 'compressed_output_generation': \" (2.5, 2.5). The labels are:\\n  - \\\\( 2 \\\\) on the left,\\n  - \\\\( n \\\\) in the center,\\n  - \\\\( n - 1 \\\\) on the right.\\n\\nSo, the bottom row is: \\\\( 2 \\\\), \\\\( n \\\\), \\\\( n - 1 \\\\).\\n\\nNow, let me summarize all the labels:\\n\\n- Row 1 (Top): \\\\( n - 3 \\\\), \\\\( 3 \\\\), \\\\( n + 1 \\\\)\\n- Row 2 (Middle): \\\\( n + 2 \\\\), \\\\( 2n - 9 \\\\), \\\\( 1 \\\\)\\n- Row 3 (Bottom): \\\\( 2 \\\\), \\\\( n \\\\), \\\\( n - 1 \\\\)\\n\\nI can calculate the sums of the rows and set them equal to each other since it's a magic square.\\n\\nLet me compute the sum of Row 1: \\\\( (n - 3) + 3 + (n + 1) \\\\). Simplify that: \\\\( n - 3 + 3 + n + 1 = 2n + 1 \\\\).\\n\\nSum of Row 2: \\\\( (n + 2) + (2n - 9) + 1 \\\\). Simplify: \\\\( n + 2 + 2n - 9 + 1 = 3n - 6 \\\\).\\n\\nSum of Row 3: \\\\( 2 + n + (n - 1) \\\\). Simplify: \\\\( 2 + n + n - 1 = 2n + 1 \\\\).\\n\\nSo, Row 1 and Row 3 both sum to \\\\( 2n + 1 \\\\). Row 2 sums to \\\\( 3n - 6 \\\\). Since it's a magic square, all rows must sum to the same value. Therefore, I can set Row 1 equal to Row 2:\\n\\n\\\\( 2n + 1 = 3n - 6 \\\\)\\n\\nSolving for \\\\( n \\\\):\\n\\nSubtract \\\\( 2n \\\\) from both sides: \\\\( 1 = n - 6 \\\\)\\n\\nAdd 6 to both sides: \\\\( n = 7 \\\\)\\n\\nWait, is that it? Let me check if this value of \\\\( n \\\\) works for the columns as well.\\n\\nLet me compute the sums of the columns to verify.\\n\\nFirst, label the magic square based on \\\\( n = 7 \\\\):\\n\\n- Row 1: \\\\( 7 - 3 = 4 \\\\), \\\\( 3 \\\\), \\\\( 7 + 1 = 8 \\\\)\\n- Row 2: \\\\( 7 + 2 = 9 \\\\), \\\\( 2*7 - 9 = 14 - 9 = 5 \\\\), \\\\( 1 \\\\)\\n- Row 3: \\\\( 2 \\\\), \\\\( 7 \\\\), \\\\( 7 - 1 = 6 \\\\)\\n\\nSo, the matrix is:\\n\\n4 | 3 | 8\\n\\n9 | 5 | 1\\n\\n2 | 7 | 6\\n\\nNow, let's check the columns:\\n\\n- Column 1: 4 + 9 + 2 = 15\\n- Column 2: 3 + 5 + 7 = 15\\n- Column 3: 8 + 1 + 6 = 15\\n\\nGood, all columns sum to 15.\\n\\nNow, let's check the diagonals.\\n\\nMain diagonal (top-left to bottom-right): 4, 5, 6. Sum: 4 + 5 + 6 = 15.\\n\\nOther diagonal (top-right to bottom-left): 8, 5, 2. Sum: 8 + 5 + 2 = 15.\\n\\nPerfect, all rows, columns, and diagonals sum to 15 when \\\\( n = 7 \\\\).\\n\\nTherefore, the value of \\\\( n \\\\) is 7.\\n\\n**Final Answer**\\nThe value of \\\\( n \\\\) is \\\\boxed{7}.\\n</think>\\n\\nTo find the value of \\\\( n \\\\) in the given magic square, we start by noting the labels of each cell based on the Asymptote code. The matrix is reconstructed as follows:\\n\\n- Row 1 (Top): \\\\( n - 3 \\\\), \\\\( 3 \\\\), \\\\( n + 1 \\\\)\\n- Row 2 (Middle): \\\\( n + 2 \\\\), \\\\( 2n - 9 \\\\), \\\\( 1 \\\\)\\n- Row 3 (Bottom): \\\\( 2 \\\\), \\\\( n \\\\), \\\\( n - 1 \\\\)\\n\\nWe calculate the sums of the rows:\\n\\n- Sum of Row 1: \\\\( (n - 3) + 3 + (n + 1) = 2n + 1 \\\\)\\n- Sum of Row 2: \\\\( (n + 2) + (2n - 9) + 1 = 3n - 6 \\\\)\\n- Sum of Row 3: \\\\( 2 + n + (n - 1) = 2n + 1 \\\\)\\n\\nSince it is a magic square, all rows must sum to the same value. Setting the sum of Row 1 equal to Row 2:\\n\\n\\\\[ 2n + 1 = 3n - 6 \\\\]\\n\\nSolving for \\\\( n \\\\):\\n\\n\\\\[ 1 = n - 6 \\\\]\\n\\\\[ n = 7 \\\\]\\n\\nTo verify, we check the columns and diagonals with \\\\( n = 7 \\\\):\\n\\n- The matrix becomes:\\n  \\\\[\\n  \\\\begin{array}{ccc}\\n  4 & 3 & 8 \\\\\\\\\\n  9 & 5 & 1 \\\\\\\\\\n  2 & 7 & 6 \\\\\\\\\\n  \\\\end{array}\\n  \\\\]\\n\\n- Column sums: 4 + 9 + 2 = 15, 3 + 5 + 7 = 15, 8 + 1 + 6 = 15\\n- Diagonals: 4 + 5 + 6 = 15, 8 + 5 + 2 = 15\\n\\nAll sums are equal to 15, confirming the magic square. Therefore, the value of \\\\( n \\\\) is \\\\(\\\\boxed{7}\\\\).<endofsentence>\",\n",
       " 'compressed_compression_size': 400,\n",
       " 'original_total_len': 2019,\n",
       " 'compressed_total_len': 1502}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "base_path = \"hidden_capacity_reasoning/evaluation/math_500/evals/compression_tests\"\n",
    "save_id = f\"test_correct_dataset_{len(correct_dataset)}_compressed_window_amount=400,window=2__2025_04_22_01_43_43_347583__10000\"\n",
    "with open(f\"{base_path}/{save_id}.json\", \"w\") as f:\n",
    "    json.dump(evaluation_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\n",
    "    f\"hidden_capacity_reasoning/evaluation/math_500/evals/compression_tests/correct_dataset_155_compressed_window_amount=400,window=2__2025_04_22_01_43_43_347583__90000.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    evaluation_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57044, 49762, 0.8723441553888227)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_total_len = 0\n",
    "compressed_total_len = 0\n",
    "for item in evaluation_dataset:\n",
    "    original_total_len += item[\"original_total_len\"]\n",
    "    compressed_total_len += item[\"compressed_total_len\"]\n",
    "original_total_len, compressed_total_len, compressed_total_len / original_total_len"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
