{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "Some weights of Qwen2ForCausalLMCompressionV1 were not initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B and are newly initialized: ['embed_pooler.model.layers.0.input_layernorm.weight', 'embed_pooler.model.layers.0.mlp.down_proj.weight', 'embed_pooler.model.layers.0.mlp.gate_proj.weight', 'embed_pooler.model.layers.0.mlp.up_proj.weight', 'embed_pooler.model.layers.0.post_attention_layernorm.weight', 'embed_pooler.model.layers.0.self_attn.k_proj.bias', 'embed_pooler.model.layers.0.self_attn.k_proj.weight', 'embed_pooler.model.layers.0.self_attn.o_proj.weight', 'embed_pooler.model.layers.0.self_attn.q_proj.bias', 'embed_pooler.model.layers.0.self_attn.q_proj.weight', 'embed_pooler.model.layers.0.self_attn.v_proj.bias', 'embed_pooler.model.layers.0.self_attn.v_proj.weight', 'embed_pooler.model.layers.1.input_layernorm.weight', 'embed_pooler.model.layers.1.mlp.down_proj.weight', 'embed_pooler.model.layers.1.mlp.gate_proj.weight', 'embed_pooler.model.layers.1.mlp.up_proj.weight', 'embed_pooler.model.layers.1.post_attention_layernorm.weight', 'embed_pooler.model.layers.1.self_attn.k_proj.bias', 'embed_pooler.model.layers.1.self_attn.k_proj.weight', 'embed_pooler.model.layers.1.self_attn.o_proj.weight', 'embed_pooler.model.layers.1.self_attn.q_proj.bias', 'embed_pooler.model.layers.1.self_attn.q_proj.weight', 'embed_pooler.model.layers.1.self_attn.v_proj.bias', 'embed_pooler.model.layers.1.self_attn.v_proj.weight', 'embed_pooler.model.layers.10.input_layernorm.weight', 'embed_pooler.model.layers.10.mlp.down_proj.weight', 'embed_pooler.model.layers.10.mlp.gate_proj.weight', 'embed_pooler.model.layers.10.mlp.up_proj.weight', 'embed_pooler.model.layers.10.post_attention_layernorm.weight', 'embed_pooler.model.layers.10.self_attn.k_proj.bias', 'embed_pooler.model.layers.10.self_attn.k_proj.weight', 'embed_pooler.model.layers.10.self_attn.o_proj.weight', 'embed_pooler.model.layers.10.self_attn.q_proj.bias', 'embed_pooler.model.layers.10.self_attn.q_proj.weight', 'embed_pooler.model.layers.10.self_attn.v_proj.bias', 'embed_pooler.model.layers.10.self_attn.v_proj.weight', 'embed_pooler.model.layers.11.input_layernorm.weight', 'embed_pooler.model.layers.11.mlp.down_proj.weight', 'embed_pooler.model.layers.11.mlp.gate_proj.weight', 'embed_pooler.model.layers.11.mlp.up_proj.weight', 'embed_pooler.model.layers.11.post_attention_layernorm.weight', 'embed_pooler.model.layers.11.self_attn.k_proj.bias', 'embed_pooler.model.layers.11.self_attn.k_proj.weight', 'embed_pooler.model.layers.11.self_attn.o_proj.weight', 'embed_pooler.model.layers.11.self_attn.q_proj.bias', 'embed_pooler.model.layers.11.self_attn.q_proj.weight', 'embed_pooler.model.layers.11.self_attn.v_proj.bias', 'embed_pooler.model.layers.11.self_attn.v_proj.weight', 'embed_pooler.model.layers.12.input_layernorm.weight', 'embed_pooler.model.layers.12.mlp.down_proj.weight', 'embed_pooler.model.layers.12.mlp.gate_proj.weight', 'embed_pooler.model.layers.12.mlp.up_proj.weight', 'embed_pooler.model.layers.12.post_attention_layernorm.weight', 'embed_pooler.model.layers.12.self_attn.k_proj.bias', 'embed_pooler.model.layers.12.self_attn.k_proj.weight', 'embed_pooler.model.layers.12.self_attn.o_proj.weight', 'embed_pooler.model.layers.12.self_attn.q_proj.bias', 'embed_pooler.model.layers.12.self_attn.q_proj.weight', 'embed_pooler.model.layers.12.self_attn.v_proj.bias', 'embed_pooler.model.layers.12.self_attn.v_proj.weight', 'embed_pooler.model.layers.13.input_layernorm.weight', 'embed_pooler.model.layers.13.mlp.down_proj.weight', 'embed_pooler.model.layers.13.mlp.gate_proj.weight', 'embed_pooler.model.layers.13.mlp.up_proj.weight', 'embed_pooler.model.layers.13.post_attention_layernorm.weight', 'embed_pooler.model.layers.13.self_attn.k_proj.bias', 'embed_pooler.model.layers.13.self_attn.k_proj.weight', 'embed_pooler.model.layers.13.self_attn.o_proj.weight', 'embed_pooler.model.layers.13.self_attn.q_proj.bias', 'embed_pooler.model.layers.13.self_attn.q_proj.weight', 'embed_pooler.model.layers.13.self_attn.v_proj.bias', 'embed_pooler.model.layers.13.self_attn.v_proj.weight', 'embed_pooler.model.layers.14.input_layernorm.weight', 'embed_pooler.model.layers.14.mlp.down_proj.weight', 'embed_pooler.model.layers.14.mlp.gate_proj.weight', 'embed_pooler.model.layers.14.mlp.up_proj.weight', 'embed_pooler.model.layers.14.post_attention_layernorm.weight', 'embed_pooler.model.layers.14.self_attn.k_proj.bias', 'embed_pooler.model.layers.14.self_attn.k_proj.weight', 'embed_pooler.model.layers.14.self_attn.o_proj.weight', 'embed_pooler.model.layers.14.self_attn.q_proj.bias', 'embed_pooler.model.layers.14.self_attn.q_proj.weight', 'embed_pooler.model.layers.14.self_attn.v_proj.bias', 'embed_pooler.model.layers.14.self_attn.v_proj.weight', 'embed_pooler.model.layers.15.input_layernorm.weight', 'embed_pooler.model.layers.15.mlp.down_proj.weight', 'embed_pooler.model.layers.15.mlp.gate_proj.weight', 'embed_pooler.model.layers.15.mlp.up_proj.weight', 'embed_pooler.model.layers.15.post_attention_layernorm.weight', 'embed_pooler.model.layers.15.self_attn.k_proj.bias', 'embed_pooler.model.layers.15.self_attn.k_proj.weight', 'embed_pooler.model.layers.15.self_attn.o_proj.weight', 'embed_pooler.model.layers.15.self_attn.q_proj.bias', 'embed_pooler.model.layers.15.self_attn.q_proj.weight', 'embed_pooler.model.layers.15.self_attn.v_proj.bias', 'embed_pooler.model.layers.15.self_attn.v_proj.weight', 'embed_pooler.model.layers.16.input_layernorm.weight', 'embed_pooler.model.layers.16.mlp.down_proj.weight', 'embed_pooler.model.layers.16.mlp.gate_proj.weight', 'embed_pooler.model.layers.16.mlp.up_proj.weight', 'embed_pooler.model.layers.16.post_attention_layernorm.weight', 'embed_pooler.model.layers.16.self_attn.k_proj.bias', 'embed_pooler.model.layers.16.self_attn.k_proj.weight', 'embed_pooler.model.layers.16.self_attn.o_proj.weight', 'embed_pooler.model.layers.16.self_attn.q_proj.bias', 'embed_pooler.model.layers.16.self_attn.q_proj.weight', 'embed_pooler.model.layers.16.self_attn.v_proj.bias', 'embed_pooler.model.layers.16.self_attn.v_proj.weight', 'embed_pooler.model.layers.17.input_layernorm.weight', 'embed_pooler.model.layers.17.mlp.down_proj.weight', 'embed_pooler.model.layers.17.mlp.gate_proj.weight', 'embed_pooler.model.layers.17.mlp.up_proj.weight', 'embed_pooler.model.layers.17.post_attention_layernorm.weight', 'embed_pooler.model.layers.17.self_attn.k_proj.bias', 'embed_pooler.model.layers.17.self_attn.k_proj.weight', 'embed_pooler.model.layers.17.self_attn.o_proj.weight', 'embed_pooler.model.layers.17.self_attn.q_proj.bias', 'embed_pooler.model.layers.17.self_attn.q_proj.weight', 'embed_pooler.model.layers.17.self_attn.v_proj.bias', 'embed_pooler.model.layers.17.self_attn.v_proj.weight', 'embed_pooler.model.layers.18.input_layernorm.weight', 'embed_pooler.model.layers.18.mlp.down_proj.weight', 'embed_pooler.model.layers.18.mlp.gate_proj.weight', 'embed_pooler.model.layers.18.mlp.up_proj.weight', 'embed_pooler.model.layers.18.post_attention_layernorm.weight', 'embed_pooler.model.layers.18.self_attn.k_proj.bias', 'embed_pooler.model.layers.18.self_attn.k_proj.weight', 'embed_pooler.model.layers.18.self_attn.o_proj.weight', 'embed_pooler.model.layers.18.self_attn.q_proj.bias', 'embed_pooler.model.layers.18.self_attn.q_proj.weight', 'embed_pooler.model.layers.18.self_attn.v_proj.bias', 'embed_pooler.model.layers.18.self_attn.v_proj.weight', 'embed_pooler.model.layers.19.input_layernorm.weight', 'embed_pooler.model.layers.19.mlp.down_proj.weight', 'embed_pooler.model.layers.19.mlp.gate_proj.weight', 'embed_pooler.model.layers.19.mlp.up_proj.weight', 'embed_pooler.model.layers.19.post_attention_layernorm.weight', 'embed_pooler.model.layers.19.self_attn.k_proj.bias', 'embed_pooler.model.layers.19.self_attn.k_proj.weight', 'embed_pooler.model.layers.19.self_attn.o_proj.weight', 'embed_pooler.model.layers.19.self_attn.q_proj.bias', 'embed_pooler.model.layers.19.self_attn.q_proj.weight', 'embed_pooler.model.layers.19.self_attn.v_proj.bias', 'embed_pooler.model.layers.19.self_attn.v_proj.weight', 'embed_pooler.model.layers.2.input_layernorm.weight', 'embed_pooler.model.layers.2.mlp.down_proj.weight', 'embed_pooler.model.layers.2.mlp.gate_proj.weight', 'embed_pooler.model.layers.2.mlp.up_proj.weight', 'embed_pooler.model.layers.2.post_attention_layernorm.weight', 'embed_pooler.model.layers.2.self_attn.k_proj.bias', 'embed_pooler.model.layers.2.self_attn.k_proj.weight', 'embed_pooler.model.layers.2.self_attn.o_proj.weight', 'embed_pooler.model.layers.2.self_attn.q_proj.bias', 'embed_pooler.model.layers.2.self_attn.q_proj.weight', 'embed_pooler.model.layers.2.self_attn.v_proj.bias', 'embed_pooler.model.layers.2.self_attn.v_proj.weight', 'embed_pooler.model.layers.20.input_layernorm.weight', 'embed_pooler.model.layers.20.mlp.down_proj.weight', 'embed_pooler.model.layers.20.mlp.gate_proj.weight', 'embed_pooler.model.layers.20.mlp.up_proj.weight', 'embed_pooler.model.layers.20.post_attention_layernorm.weight', 'embed_pooler.model.layers.20.self_attn.k_proj.bias', 'embed_pooler.model.layers.20.self_attn.k_proj.weight', 'embed_pooler.model.layers.20.self_attn.o_proj.weight', 'embed_pooler.model.layers.20.self_attn.q_proj.bias', 'embed_pooler.model.layers.20.self_attn.q_proj.weight', 'embed_pooler.model.layers.20.self_attn.v_proj.bias', 'embed_pooler.model.layers.20.self_attn.v_proj.weight', 'embed_pooler.model.layers.21.input_layernorm.weight', 'embed_pooler.model.layers.21.mlp.down_proj.weight', 'embed_pooler.model.layers.21.mlp.gate_proj.weight', 'embed_pooler.model.layers.21.mlp.up_proj.weight', 'embed_pooler.model.layers.21.post_attention_layernorm.weight', 'embed_pooler.model.layers.21.self_attn.k_proj.bias', 'embed_pooler.model.layers.21.self_attn.k_proj.weight', 'embed_pooler.model.layers.21.self_attn.o_proj.weight', 'embed_pooler.model.layers.21.self_attn.q_proj.bias', 'embed_pooler.model.layers.21.self_attn.q_proj.weight', 'embed_pooler.model.layers.21.self_attn.v_proj.bias', 'embed_pooler.model.layers.21.self_attn.v_proj.weight', 'embed_pooler.model.layers.22.input_layernorm.weight', 'embed_pooler.model.layers.22.mlp.down_proj.weight', 'embed_pooler.model.layers.22.mlp.gate_proj.weight', 'embed_pooler.model.layers.22.mlp.up_proj.weight', 'embed_pooler.model.layers.22.post_attention_layernorm.weight', 'embed_pooler.model.layers.22.self_attn.k_proj.bias', 'embed_pooler.model.layers.22.self_attn.k_proj.weight', 'embed_pooler.model.layers.22.self_attn.o_proj.weight', 'embed_pooler.model.layers.22.self_attn.q_proj.bias', 'embed_pooler.model.layers.22.self_attn.q_proj.weight', 'embed_pooler.model.layers.22.self_attn.v_proj.bias', 'embed_pooler.model.layers.22.self_attn.v_proj.weight', 'embed_pooler.model.layers.23.input_layernorm.weight', 'embed_pooler.model.layers.23.mlp.down_proj.weight', 'embed_pooler.model.layers.23.mlp.gate_proj.weight', 'embed_pooler.model.layers.23.mlp.up_proj.weight', 'embed_pooler.model.layers.23.post_attention_layernorm.weight', 'embed_pooler.model.layers.23.self_attn.k_proj.bias', 'embed_pooler.model.layers.23.self_attn.k_proj.weight', 'embed_pooler.model.layers.23.self_attn.o_proj.weight', 'embed_pooler.model.layers.23.self_attn.q_proj.bias', 'embed_pooler.model.layers.23.self_attn.q_proj.weight', 'embed_pooler.model.layers.23.self_attn.v_proj.bias', 'embed_pooler.model.layers.23.self_attn.v_proj.weight', 'embed_pooler.model.layers.24.input_layernorm.weight', 'embed_pooler.model.layers.24.mlp.down_proj.weight', 'embed_pooler.model.layers.24.mlp.gate_proj.weight', 'embed_pooler.model.layers.24.mlp.up_proj.weight', 'embed_pooler.model.layers.24.post_attention_layernorm.weight', 'embed_pooler.model.layers.24.self_attn.k_proj.bias', 'embed_pooler.model.layers.24.self_attn.k_proj.weight', 'embed_pooler.model.layers.24.self_attn.o_proj.weight', 'embed_pooler.model.layers.24.self_attn.q_proj.bias', 'embed_pooler.model.layers.24.self_attn.q_proj.weight', 'embed_pooler.model.layers.24.self_attn.v_proj.bias', 'embed_pooler.model.layers.24.self_attn.v_proj.weight', 'embed_pooler.model.layers.25.input_layernorm.weight', 'embed_pooler.model.layers.25.mlp.down_proj.weight', 'embed_pooler.model.layers.25.mlp.gate_proj.weight', 'embed_pooler.model.layers.25.mlp.up_proj.weight', 'embed_pooler.model.layers.25.post_attention_layernorm.weight', 'embed_pooler.model.layers.25.self_attn.k_proj.bias', 'embed_pooler.model.layers.25.self_attn.k_proj.weight', 'embed_pooler.model.layers.25.self_attn.o_proj.weight', 'embed_pooler.model.layers.25.self_attn.q_proj.bias', 'embed_pooler.model.layers.25.self_attn.q_proj.weight', 'embed_pooler.model.layers.25.self_attn.v_proj.bias', 'embed_pooler.model.layers.25.self_attn.v_proj.weight', 'embed_pooler.model.layers.26.input_layernorm.weight', 'embed_pooler.model.layers.26.mlp.down_proj.weight', 'embed_pooler.model.layers.26.mlp.gate_proj.weight', 'embed_pooler.model.layers.26.mlp.up_proj.weight', 'embed_pooler.model.layers.26.post_attention_layernorm.weight', 'embed_pooler.model.layers.26.self_attn.k_proj.bias', 'embed_pooler.model.layers.26.self_attn.k_proj.weight', 'embed_pooler.model.layers.26.self_attn.o_proj.weight', 'embed_pooler.model.layers.26.self_attn.q_proj.bias', 'embed_pooler.model.layers.26.self_attn.q_proj.weight', 'embed_pooler.model.layers.26.self_attn.v_proj.bias', 'embed_pooler.model.layers.26.self_attn.v_proj.weight', 'embed_pooler.model.layers.27.input_layernorm.weight', 'embed_pooler.model.layers.27.mlp.down_proj.weight', 'embed_pooler.model.layers.27.mlp.gate_proj.weight', 'embed_pooler.model.layers.27.mlp.up_proj.weight', 'embed_pooler.model.layers.27.post_attention_layernorm.weight', 'embed_pooler.model.layers.27.self_attn.k_proj.bias', 'embed_pooler.model.layers.27.self_attn.k_proj.weight', 'embed_pooler.model.layers.27.self_attn.o_proj.weight', 'embed_pooler.model.layers.27.self_attn.q_proj.bias', 'embed_pooler.model.layers.27.self_attn.q_proj.weight', 'embed_pooler.model.layers.27.self_attn.v_proj.bias', 'embed_pooler.model.layers.27.self_attn.v_proj.weight', 'embed_pooler.model.layers.3.input_layernorm.weight', 'embed_pooler.model.layers.3.mlp.down_proj.weight', 'embed_pooler.model.layers.3.mlp.gate_proj.weight', 'embed_pooler.model.layers.3.mlp.up_proj.weight', 'embed_pooler.model.layers.3.post_attention_layernorm.weight', 'embed_pooler.model.layers.3.self_attn.k_proj.bias', 'embed_pooler.model.layers.3.self_attn.k_proj.weight', 'embed_pooler.model.layers.3.self_attn.o_proj.weight', 'embed_pooler.model.layers.3.self_attn.q_proj.bias', 'embed_pooler.model.layers.3.self_attn.q_proj.weight', 'embed_pooler.model.layers.3.self_attn.v_proj.bias', 'embed_pooler.model.layers.3.self_attn.v_proj.weight', 'embed_pooler.model.layers.4.input_layernorm.weight', 'embed_pooler.model.layers.4.mlp.down_proj.weight', 'embed_pooler.model.layers.4.mlp.gate_proj.weight', 'embed_pooler.model.layers.4.mlp.up_proj.weight', 'embed_pooler.model.layers.4.post_attention_layernorm.weight', 'embed_pooler.model.layers.4.self_attn.k_proj.bias', 'embed_pooler.model.layers.4.self_attn.k_proj.weight', 'embed_pooler.model.layers.4.self_attn.o_proj.weight', 'embed_pooler.model.layers.4.self_attn.q_proj.bias', 'embed_pooler.model.layers.4.self_attn.q_proj.weight', 'embed_pooler.model.layers.4.self_attn.v_proj.bias', 'embed_pooler.model.layers.4.self_attn.v_proj.weight', 'embed_pooler.model.layers.5.input_layernorm.weight', 'embed_pooler.model.layers.5.mlp.down_proj.weight', 'embed_pooler.model.layers.5.mlp.gate_proj.weight', 'embed_pooler.model.layers.5.mlp.up_proj.weight', 'embed_pooler.model.layers.5.post_attention_layernorm.weight', 'embed_pooler.model.layers.5.self_attn.k_proj.bias', 'embed_pooler.model.layers.5.self_attn.k_proj.weight', 'embed_pooler.model.layers.5.self_attn.o_proj.weight', 'embed_pooler.model.layers.5.self_attn.q_proj.bias', 'embed_pooler.model.layers.5.self_attn.q_proj.weight', 'embed_pooler.model.layers.5.self_attn.v_proj.bias', 'embed_pooler.model.layers.5.self_attn.v_proj.weight', 'embed_pooler.model.layers.6.input_layernorm.weight', 'embed_pooler.model.layers.6.mlp.down_proj.weight', 'embed_pooler.model.layers.6.mlp.gate_proj.weight', 'embed_pooler.model.layers.6.mlp.up_proj.weight', 'embed_pooler.model.layers.6.post_attention_layernorm.weight', 'embed_pooler.model.layers.6.self_attn.k_proj.bias', 'embed_pooler.model.layers.6.self_attn.k_proj.weight', 'embed_pooler.model.layers.6.self_attn.o_proj.weight', 'embed_pooler.model.layers.6.self_attn.q_proj.bias', 'embed_pooler.model.layers.6.self_attn.q_proj.weight', 'embed_pooler.model.layers.6.self_attn.v_proj.bias', 'embed_pooler.model.layers.6.self_attn.v_proj.weight', 'embed_pooler.model.layers.7.input_layernorm.weight', 'embed_pooler.model.layers.7.mlp.down_proj.weight', 'embed_pooler.model.layers.7.mlp.gate_proj.weight', 'embed_pooler.model.layers.7.mlp.up_proj.weight', 'embed_pooler.model.layers.7.post_attention_layernorm.weight', 'embed_pooler.model.layers.7.self_attn.k_proj.bias', 'embed_pooler.model.layers.7.self_attn.k_proj.weight', 'embed_pooler.model.layers.7.self_attn.o_proj.weight', 'embed_pooler.model.layers.7.self_attn.q_proj.bias', 'embed_pooler.model.layers.7.self_attn.q_proj.weight', 'embed_pooler.model.layers.7.self_attn.v_proj.bias', 'embed_pooler.model.layers.7.self_attn.v_proj.weight', 'embed_pooler.model.layers.8.input_layernorm.weight', 'embed_pooler.model.layers.8.mlp.down_proj.weight', 'embed_pooler.model.layers.8.mlp.gate_proj.weight', 'embed_pooler.model.layers.8.mlp.up_proj.weight', 'embed_pooler.model.layers.8.post_attention_layernorm.weight', 'embed_pooler.model.layers.8.self_attn.k_proj.bias', 'embed_pooler.model.layers.8.self_attn.k_proj.weight', 'embed_pooler.model.layers.8.self_attn.o_proj.weight', 'embed_pooler.model.layers.8.self_attn.q_proj.bias', 'embed_pooler.model.layers.8.self_attn.q_proj.weight', 'embed_pooler.model.layers.8.self_attn.v_proj.bias', 'embed_pooler.model.layers.8.self_attn.v_proj.weight', 'embed_pooler.model.layers.9.input_layernorm.weight', 'embed_pooler.model.layers.9.mlp.down_proj.weight', 'embed_pooler.model.layers.9.mlp.gate_proj.weight', 'embed_pooler.model.layers.9.mlp.up_proj.weight', 'embed_pooler.model.layers.9.post_attention_layernorm.weight', 'embed_pooler.model.layers.9.self_attn.k_proj.bias', 'embed_pooler.model.layers.9.self_attn.k_proj.weight', 'embed_pooler.model.layers.9.self_attn.o_proj.weight', 'embed_pooler.model.layers.9.self_attn.q_proj.bias', 'embed_pooler.model.layers.9.self_attn.q_proj.weight', 'embed_pooler.model.layers.9.self_attn.v_proj.bias', 'embed_pooler.model.layers.9.self_attn.v_proj.weight', 'embed_pooler.model.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<beginofsentence>You are a helpful assistant.<User>how many wings has a bird?<Assistant><think>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Okay'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"hidden_capacity_reasoning\"\n",
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from trl import (\n",
    "    ModelConfig,\n",
    "    ScriptArguments,\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    "    TrlParser,\n",
    "    get_kbit_device_map,\n",
    ")\n",
    "\n",
    "\n",
    "class Qwen2ModelEmbedPoolerV1(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.model.embed_tokens = None\n",
    "        self.lm_head = None\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds):\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )[0]\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.sum(1) / torch.tensor(\n",
    "            input_embeds.shape[1],\n",
    "            device=input_embeds.device,\n",
    "            dtype=input_embeds.dtype,\n",
    "        )\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.unsqueeze(1)\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "class Qwen2ForCausalLMCompressionV1(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = torch.nn.Linear(\n",
    "            config.hidden_size, config.vocab_size, bias=False\n",
    "        )\n",
    "        # print(config._name_or_path)\n",
    "        self.embed_pooler = Qwen2ModelEmbedPoolerV1.from_pretrained(\n",
    "            config._name_or_path,\n",
    "            config=config,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "        )\n",
    "\n",
    "        self.post_init()\n",
    "        # Initialize weights and apply final processing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        cache_position=None,\n",
    "        logits_to_keep=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if \"replaced_original_tokens\" in kwargs:\n",
    "            pass\n",
    "        return super().forward(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            position_ids,\n",
    "            past_key_values,\n",
    "            inputs_embeds,\n",
    "            labels,\n",
    "            use_cache,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "            return_dict,\n",
    "            cache_position,\n",
    "            logits_to_keep,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "# torch.set_grad_enabled(False)\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# model_name = \"./test_model/\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = Qwen2ForCausalLMCompressionV1.from_pretrained(\n",
    "    model_name,\n",
    "    # torch_dtype=torch.float16,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=get_kbit_device_map(),\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# model = model.eval().cuda()\n",
    "# model.model = model.embed_pooler.model\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.model.requires_grad_(False)\n",
    "\n",
    "prompt = \"how many wings has a bird?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=1)\n",
    "    # generated_ids = model.generate(\n",
    "    #     model_inputs.input_ids,\n",
    "    #     max_new_tokens=1000,\n",
    "    #     do_sample=False,\n",
    "    # )\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ModelEmbedPoolerV1(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): None\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): None\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed_pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "temp_model = Qwen2ModelEmbedPoolerV1.from_pretrained(\n",
    "    model_name,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "print(model.embed_pooler.load_state_dict(temp_model.state_dict()))\n",
    "temp_model = temp_model.cpu()\n",
    "del temp_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         (    None  )\n",
    "#          newly initialized\n",
    "# model.model.embed_tokens.weight == model.embed_pooler.model.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 900/900 [00:01<00:00, 631.96it/s]\n",
      "100%|| 900/900 [00:04<00:00, 186.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['replaced_original_tokens', 'compressed_input_ids', 'original_tokens'],\n",
       "    num_rows: 124217\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from hidden_capacity_reasoning.utils import (\n",
    "    generate_train_examples,\n",
    "    pad_train_examples,\n",
    "    tokenize_single_turn,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=5, seed=42)\n",
    "dataset\n",
    "\n",
    "# test\n",
    "tokenize_single_turn(\n",
    "    question=dataset[\"train\"][0][\"question\"],\n",
    "    answer=dataset[\"train\"][0][\"answer\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "train_examples = [\n",
    "    tokenize_single_turn(tokenizer=tokenizer, **item)\n",
    "    for item in tqdm(dataset[\"train\"].to_list())\n",
    "]\n",
    "\n",
    "\n",
    "prepared_train_examples = []\n",
    "for item in tqdm(train_examples):\n",
    "    for example in generate_train_examples(\n",
    "        dataset_batch=[item], tokenizer=tokenizer, window_size=4\n",
    "    ):\n",
    "        prepared_train_examples.append(example)\n",
    "\n",
    "print(\n",
    "    \"max_len\", max([len(item[\"original_tokens\"]) for item in prepared_train_examples])\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "new_dataset = Dataset.from_list(prepared_train_examples)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from more_itertools import chunked\n",
    "\n",
    "# batch_size = 4\n",
    "# train_examples_batches = [\n",
    "#     pad_train_examples(\n",
    "#         train_examples=item,\n",
    "#         tokenizer=tokenizer,\n",
    "#     )\n",
    "#     for item in tqdm(\n",
    "#         list(\n",
    "#             chunked(\n",
    "#                 prepared_train_examples,\n",
    "#                 batch_size,\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_examples_batches_clean = []\n",
    "# for item in train_examples_batches:\n",
    "#     train_examples_batches_clean.append(\n",
    "#         {\n",
    "#             \"replaced_original_tokens\": item[\"replaced_original_tokens\"][\"input_ids\"],\n",
    "#             \"compressed_input_ids\": item[\"compressed_input_ids\"][\"input_ids\"],\n",
    "#             \"original_tokens\": item[\"original_tokens\"][\"input_ids\"],\n",
    "#             \"attention_mask\": item[\"compressed_input_ids\"][\"attention_mask\"],\n",
    "#             \"labels\": item[\"compressed_input_ids\"][\"input_ids\"],\n",
    "#         }\n",
    "#     )\n",
    "# print(len(train_examples_batches_clean))\n",
    "# # train_examples_batches_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test one Pass (for debug only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1230,  1.2969, -0.4004,  ..., -1.5781,  1.8594,  1.7734]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hidden_capacity_reasoning.utils import EOS_TOKEN_ID, TEXT_TOKEN_ID, WINDOW_SIZE\n",
    "\n",
    "kwargs = {}\n",
    "for k in prepared_train_examples[0].keys():\n",
    "    kwargs[k] = torch.tensor(\n",
    "        prepared_train_examples[0][k],\n",
    "        device=\"cuda\",\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "# kwargs\n",
    "\n",
    "original_tokens_torch = kwargs[\"original_tokens\"].to(model.device)\n",
    "replaced_tokens_torch = kwargs[\"replaced_original_tokens\"].to(model.device)\n",
    "compressed_tokens_torch = kwargs[\"compressed_input_ids\"].to(model.device)\n",
    "\n",
    "original_embeds = model.get_input_embeddings()(original_tokens_torch)\n",
    "# replaced_embeds = self.model.get_input_embeddings()(replaced_tokens_torch)\n",
    "compressed_embeds_template = model.get_input_embeddings()(compressed_tokens_torch)\n",
    "\n",
    "tokens_for_compression_mask = replaced_tokens_torch == TEXT_TOKEN_ID\n",
    "compressed_tokens_mask = compressed_tokens_torch == TEXT_TOKEN_ID\n",
    "embeds_for_compression = original_embeds[tokens_for_compression_mask].reshape(\n",
    "    -1,\n",
    "    WINDOW_SIZE,\n",
    "    original_embeds.shape[-1],\n",
    ")\n",
    "pooled_embeds = model.embed_pooler(embeds_for_compression)\n",
    "pooled_embeds = pooled_embeds.to(compressed_embeds_template.dtype)\n",
    "compressed_embeds_template = compressed_embeds_template.masked_scatter(\n",
    "    compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "    pooled_embeds,\n",
    ")\n",
    "pooled_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_input_embeddings().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025_03_13_14_23_53_931398'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "formatted_date = datetime.fromtimestamp(time.time()).strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/trl/blob/main/examples/scripts/sft_video_llm.py\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "from unsloth import is_bfloat16_supported\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from hidden_capacity_reasoning.utils import EOS_TOKEN_ID, TEXT_TOKEN_ID, WINDOW_SIZE\n",
    "\n",
    "\n",
    "def find_all_linear_names_v2(model):\n",
    "    lora_module_names = set()\n",
    "    target_modules = set(\n",
    "        [\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "        ]\n",
    "    )\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if \"embed_pooler\" in name:\n",
    "                names = name.split(\".\")[-1]\n",
    "                if names in target_modules:\n",
    "                    lora_module_names.add(name)\n",
    "    return lora_module_names\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    padded_batch = pad_train_examples(\n",
    "        train_examples=batch,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    padded_batch = {\n",
    "        \"replaced_original_tokens\": padded_batch[\"replaced_original_tokens\"][\n",
    "            \"input_ids\"\n",
    "        ],\n",
    "        \"compressed_input_ids\": padded_batch[\"compressed_input_ids\"][\"input_ids\"],\n",
    "        \"original_tokens\": padded_batch[\"original_tokens\"][\"input_ids\"],\n",
    "        \"attention_mask\": padded_batch[\"compressed_input_ids\"][\"attention_mask\"],\n",
    "        \"labels\": padded_batch[\"compressed_input_ids\"][\"input_ids\"],\n",
    "    }\n",
    "    for key in padded_batch.keys():\n",
    "        padded_batch[key] = torch.tensor(padded_batch[key])\n",
    "    skip_ids = [TEXT_TOKEN_ID, EOS_TOKEN_ID]\n",
    "    for skip_id in skip_ids:\n",
    "        padded_batch[\"labels\"][padded_batch[\"labels\"] == skip_id] = -100\n",
    "    # print(padded_batch)\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "import types\n",
    "\n",
    "\n",
    "def forward(\n",
    "    self,\n",
    "    input_ids=None,\n",
    "    attention_mask=None,\n",
    "    position_ids=None,\n",
    "    past_key_values=None,\n",
    "    inputs_embeds=None,\n",
    "    labels=None,\n",
    "    use_cache=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    return_dict=None,\n",
    "    cache_position=None,\n",
    "    logits_to_keep=0,\n",
    "    **kwargs,\n",
    "):\n",
    "    if \"replaced_original_tokens\" in kwargs:\n",
    "        original_tokens_torch = kwargs[\"original_tokens\"].to(self.model.device)\n",
    "        replaced_tokens_torch = kwargs[\"replaced_original_tokens\"].to(self.model.device)\n",
    "        compressed_tokens_torch = kwargs[\"compressed_input_ids\"].to(self.model.device)\n",
    "\n",
    "        original_embeds = self.model.get_input_embeddings()(original_tokens_torch)\n",
    "        # replaced_embeds = self.model.get_input_embeddings()(replaced_tokens_torch)\n",
    "        compressed_embeds_template = self.model.get_input_embeddings()(\n",
    "            compressed_tokens_torch\n",
    "        )\n",
    "\n",
    "        tokens_for_compression_mask = replaced_tokens_torch == TEXT_TOKEN_ID\n",
    "        compressed_tokens_mask = compressed_tokens_torch == TEXT_TOKEN_ID\n",
    "        embeds_for_compression = original_embeds[tokens_for_compression_mask].reshape(\n",
    "            -1,\n",
    "            WINDOW_SIZE,\n",
    "            original_embeds.shape[-1],\n",
    "        )\n",
    "        pooled_embeds = self.embed_pooler(embeds_for_compression)\n",
    "        pooled_embeds = pooled_embeds.to(compressed_embeds_template.dtype)\n",
    "        compressed_embeds_template = compressed_embeds_template.masked_scatter(\n",
    "            compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "            pooled_embeds,\n",
    "        )\n",
    "        inputs_embeds = compressed_embeds_template\n",
    "\n",
    "    return super(type(self), self).forward(\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        position_ids,\n",
    "        past_key_values,\n",
    "        inputs_embeds,\n",
    "        labels,\n",
    "        use_cache,\n",
    "        output_attentions,\n",
    "        output_hidden_states,\n",
    "        return_dict,\n",
    "        cache_position,\n",
    "        logits_to_keep,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "# model.forward = forward\n",
    "model.forward = types.MethodType(forward, model)\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    target_modules=find_all_linear_names_v2(model=model),\n",
    ")\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "formatted_date = datetime.fromtimestamp(time.time()).strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=new_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps=10000,\n",
    "        learning_rate=2e-4,\n",
    "        # fp16=not is_bfloat16_supported(),\n",
    "        # bf16=is_bfloat16_supported(),\n",
    "        bf16=model.dtype == torch.bfloat16,\n",
    "        fp16=model.dtype == torch.float16,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=f\"outputs/{formatted_date}\",\n",
    "        # report_to=\"none\",  # Use this for WandB etc\n",
    "        report_to=\"wandb\",  # Use this for WandB etc\n",
    "        remove_unused_columns=False,\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "        gradient_checkpointing=True,\n",
    "    ),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embed_pooler.model.layers.0.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.0.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.1.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.2.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.3.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.4.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.5.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.6.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.7.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.8.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.9.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.10.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.11.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.12.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.13.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.14.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.15.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.16.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.17.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.18.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.19.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.20.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.21.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.22.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.23.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.24.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.25.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.26.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.q_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.q_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.k_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.k_proj.lora_B.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.v_proj.lora_A.default.weight, Requires Gradient: True\n",
      "Layer: embed_pooler.model.layers.27.self_attn.v_proj.lora_B.default.weight, Requires Gradient: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in trainer.model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, Requires Gradient: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      " Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForCausalLMCompressionV1 were not initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B and are newly initialized: ['embed_pooler.model.layers.0.input_layernorm.weight', 'embed_pooler.model.layers.0.mlp.down_proj.weight', 'embed_pooler.model.layers.0.mlp.gate_proj.weight', 'embed_pooler.model.layers.0.mlp.up_proj.weight', 'embed_pooler.model.layers.0.post_attention_layernorm.weight', 'embed_pooler.model.layers.0.self_attn.k_proj.bias', 'embed_pooler.model.layers.0.self_attn.k_proj.weight', 'embed_pooler.model.layers.0.self_attn.o_proj.weight', 'embed_pooler.model.layers.0.self_attn.q_proj.bias', 'embed_pooler.model.layers.0.self_attn.q_proj.weight', 'embed_pooler.model.layers.0.self_attn.v_proj.bias', 'embed_pooler.model.layers.0.self_attn.v_proj.weight', 'embed_pooler.model.layers.1.input_layernorm.weight', 'embed_pooler.model.layers.1.mlp.down_proj.weight', 'embed_pooler.model.layers.1.mlp.gate_proj.weight', 'embed_pooler.model.layers.1.mlp.up_proj.weight', 'embed_pooler.model.layers.1.post_attention_layernorm.weight', 'embed_pooler.model.layers.1.self_attn.k_proj.bias', 'embed_pooler.model.layers.1.self_attn.k_proj.weight', 'embed_pooler.model.layers.1.self_attn.o_proj.weight', 'embed_pooler.model.layers.1.self_attn.q_proj.bias', 'embed_pooler.model.layers.1.self_attn.q_proj.weight', 'embed_pooler.model.layers.1.self_attn.v_proj.bias', 'embed_pooler.model.layers.1.self_attn.v_proj.weight', 'embed_pooler.model.layers.10.input_layernorm.weight', 'embed_pooler.model.layers.10.mlp.down_proj.weight', 'embed_pooler.model.layers.10.mlp.gate_proj.weight', 'embed_pooler.model.layers.10.mlp.up_proj.weight', 'embed_pooler.model.layers.10.post_attention_layernorm.weight', 'embed_pooler.model.layers.10.self_attn.k_proj.bias', 'embed_pooler.model.layers.10.self_attn.k_proj.weight', 'embed_pooler.model.layers.10.self_attn.o_proj.weight', 'embed_pooler.model.layers.10.self_attn.q_proj.bias', 'embed_pooler.model.layers.10.self_attn.q_proj.weight', 'embed_pooler.model.layers.10.self_attn.v_proj.bias', 'embed_pooler.model.layers.10.self_attn.v_proj.weight', 'embed_pooler.model.layers.11.input_layernorm.weight', 'embed_pooler.model.layers.11.mlp.down_proj.weight', 'embed_pooler.model.layers.11.mlp.gate_proj.weight', 'embed_pooler.model.layers.11.mlp.up_proj.weight', 'embed_pooler.model.layers.11.post_attention_layernorm.weight', 'embed_pooler.model.layers.11.self_attn.k_proj.bias', 'embed_pooler.model.layers.11.self_attn.k_proj.weight', 'embed_pooler.model.layers.11.self_attn.o_proj.weight', 'embed_pooler.model.layers.11.self_attn.q_proj.bias', 'embed_pooler.model.layers.11.self_attn.q_proj.weight', 'embed_pooler.model.layers.11.self_attn.v_proj.bias', 'embed_pooler.model.layers.11.self_attn.v_proj.weight', 'embed_pooler.model.layers.12.input_layernorm.weight', 'embed_pooler.model.layers.12.mlp.down_proj.weight', 'embed_pooler.model.layers.12.mlp.gate_proj.weight', 'embed_pooler.model.layers.12.mlp.up_proj.weight', 'embed_pooler.model.layers.12.post_attention_layernorm.weight', 'embed_pooler.model.layers.12.self_attn.k_proj.bias', 'embed_pooler.model.layers.12.self_attn.k_proj.weight', 'embed_pooler.model.layers.12.self_attn.o_proj.weight', 'embed_pooler.model.layers.12.self_attn.q_proj.bias', 'embed_pooler.model.layers.12.self_attn.q_proj.weight', 'embed_pooler.model.layers.12.self_attn.v_proj.bias', 'embed_pooler.model.layers.12.self_attn.v_proj.weight', 'embed_pooler.model.layers.13.input_layernorm.weight', 'embed_pooler.model.layers.13.mlp.down_proj.weight', 'embed_pooler.model.layers.13.mlp.gate_proj.weight', 'embed_pooler.model.layers.13.mlp.up_proj.weight', 'embed_pooler.model.layers.13.post_attention_layernorm.weight', 'embed_pooler.model.layers.13.self_attn.k_proj.bias', 'embed_pooler.model.layers.13.self_attn.k_proj.weight', 'embed_pooler.model.layers.13.self_attn.o_proj.weight', 'embed_pooler.model.layers.13.self_attn.q_proj.bias', 'embed_pooler.model.layers.13.self_attn.q_proj.weight', 'embed_pooler.model.layers.13.self_attn.v_proj.bias', 'embed_pooler.model.layers.13.self_attn.v_proj.weight', 'embed_pooler.model.layers.14.input_layernorm.weight', 'embed_pooler.model.layers.14.mlp.down_proj.weight', 'embed_pooler.model.layers.14.mlp.gate_proj.weight', 'embed_pooler.model.layers.14.mlp.up_proj.weight', 'embed_pooler.model.layers.14.post_attention_layernorm.weight', 'embed_pooler.model.layers.14.self_attn.k_proj.bias', 'embed_pooler.model.layers.14.self_attn.k_proj.weight', 'embed_pooler.model.layers.14.self_attn.o_proj.weight', 'embed_pooler.model.layers.14.self_attn.q_proj.bias', 'embed_pooler.model.layers.14.self_attn.q_proj.weight', 'embed_pooler.model.layers.14.self_attn.v_proj.bias', 'embed_pooler.model.layers.14.self_attn.v_proj.weight', 'embed_pooler.model.layers.15.input_layernorm.weight', 'embed_pooler.model.layers.15.mlp.down_proj.weight', 'embed_pooler.model.layers.15.mlp.gate_proj.weight', 'embed_pooler.model.layers.15.mlp.up_proj.weight', 'embed_pooler.model.layers.15.post_attention_layernorm.weight', 'embed_pooler.model.layers.15.self_attn.k_proj.bias', 'embed_pooler.model.layers.15.self_attn.k_proj.weight', 'embed_pooler.model.layers.15.self_attn.o_proj.weight', 'embed_pooler.model.layers.15.self_attn.q_proj.bias', 'embed_pooler.model.layers.15.self_attn.q_proj.weight', 'embed_pooler.model.layers.15.self_attn.v_proj.bias', 'embed_pooler.model.layers.15.self_attn.v_proj.weight', 'embed_pooler.model.layers.16.input_layernorm.weight', 'embed_pooler.model.layers.16.mlp.down_proj.weight', 'embed_pooler.model.layers.16.mlp.gate_proj.weight', 'embed_pooler.model.layers.16.mlp.up_proj.weight', 'embed_pooler.model.layers.16.post_attention_layernorm.weight', 'embed_pooler.model.layers.16.self_attn.k_proj.bias', 'embed_pooler.model.layers.16.self_attn.k_proj.weight', 'embed_pooler.model.layers.16.self_attn.o_proj.weight', 'embed_pooler.model.layers.16.self_attn.q_proj.bias', 'embed_pooler.model.layers.16.self_attn.q_proj.weight', 'embed_pooler.model.layers.16.self_attn.v_proj.bias', 'embed_pooler.model.layers.16.self_attn.v_proj.weight', 'embed_pooler.model.layers.17.input_layernorm.weight', 'embed_pooler.model.layers.17.mlp.down_proj.weight', 'embed_pooler.model.layers.17.mlp.gate_proj.weight', 'embed_pooler.model.layers.17.mlp.up_proj.weight', 'embed_pooler.model.layers.17.post_attention_layernorm.weight', 'embed_pooler.model.layers.17.self_attn.k_proj.bias', 'embed_pooler.model.layers.17.self_attn.k_proj.weight', 'embed_pooler.model.layers.17.self_attn.o_proj.weight', 'embed_pooler.model.layers.17.self_attn.q_proj.bias', 'embed_pooler.model.layers.17.self_attn.q_proj.weight', 'embed_pooler.model.layers.17.self_attn.v_proj.bias', 'embed_pooler.model.layers.17.self_attn.v_proj.weight', 'embed_pooler.model.layers.18.input_layernorm.weight', 'embed_pooler.model.layers.18.mlp.down_proj.weight', 'embed_pooler.model.layers.18.mlp.gate_proj.weight', 'embed_pooler.model.layers.18.mlp.up_proj.weight', 'embed_pooler.model.layers.18.post_attention_layernorm.weight', 'embed_pooler.model.layers.18.self_attn.k_proj.bias', 'embed_pooler.model.layers.18.self_attn.k_proj.weight', 'embed_pooler.model.layers.18.self_attn.o_proj.weight', 'embed_pooler.model.layers.18.self_attn.q_proj.bias', 'embed_pooler.model.layers.18.self_attn.q_proj.weight', 'embed_pooler.model.layers.18.self_attn.v_proj.bias', 'embed_pooler.model.layers.18.self_attn.v_proj.weight', 'embed_pooler.model.layers.19.input_layernorm.weight', 'embed_pooler.model.layers.19.mlp.down_proj.weight', 'embed_pooler.model.layers.19.mlp.gate_proj.weight', 'embed_pooler.model.layers.19.mlp.up_proj.weight', 'embed_pooler.model.layers.19.post_attention_layernorm.weight', 'embed_pooler.model.layers.19.self_attn.k_proj.bias', 'embed_pooler.model.layers.19.self_attn.k_proj.weight', 'embed_pooler.model.layers.19.self_attn.o_proj.weight', 'embed_pooler.model.layers.19.self_attn.q_proj.bias', 'embed_pooler.model.layers.19.self_attn.q_proj.weight', 'embed_pooler.model.layers.19.self_attn.v_proj.bias', 'embed_pooler.model.layers.19.self_attn.v_proj.weight', 'embed_pooler.model.layers.2.input_layernorm.weight', 'embed_pooler.model.layers.2.mlp.down_proj.weight', 'embed_pooler.model.layers.2.mlp.gate_proj.weight', 'embed_pooler.model.layers.2.mlp.up_proj.weight', 'embed_pooler.model.layers.2.post_attention_layernorm.weight', 'embed_pooler.model.layers.2.self_attn.k_proj.bias', 'embed_pooler.model.layers.2.self_attn.k_proj.weight', 'embed_pooler.model.layers.2.self_attn.o_proj.weight', 'embed_pooler.model.layers.2.self_attn.q_proj.bias', 'embed_pooler.model.layers.2.self_attn.q_proj.weight', 'embed_pooler.model.layers.2.self_attn.v_proj.bias', 'embed_pooler.model.layers.2.self_attn.v_proj.weight', 'embed_pooler.model.layers.20.input_layernorm.weight', 'embed_pooler.model.layers.20.mlp.down_proj.weight', 'embed_pooler.model.layers.20.mlp.gate_proj.weight', 'embed_pooler.model.layers.20.mlp.up_proj.weight', 'embed_pooler.model.layers.20.post_attention_layernorm.weight', 'embed_pooler.model.layers.20.self_attn.k_proj.bias', 'embed_pooler.model.layers.20.self_attn.k_proj.weight', 'embed_pooler.model.layers.20.self_attn.o_proj.weight', 'embed_pooler.model.layers.20.self_attn.q_proj.bias', 'embed_pooler.model.layers.20.self_attn.q_proj.weight', 'embed_pooler.model.layers.20.self_attn.v_proj.bias', 'embed_pooler.model.layers.20.self_attn.v_proj.weight', 'embed_pooler.model.layers.21.input_layernorm.weight', 'embed_pooler.model.layers.21.mlp.down_proj.weight', 'embed_pooler.model.layers.21.mlp.gate_proj.weight', 'embed_pooler.model.layers.21.mlp.up_proj.weight', 'embed_pooler.model.layers.21.post_attention_layernorm.weight', 'embed_pooler.model.layers.21.self_attn.k_proj.bias', 'embed_pooler.model.layers.21.self_attn.k_proj.weight', 'embed_pooler.model.layers.21.self_attn.o_proj.weight', 'embed_pooler.model.layers.21.self_attn.q_proj.bias', 'embed_pooler.model.layers.21.self_attn.q_proj.weight', 'embed_pooler.model.layers.21.self_attn.v_proj.bias', 'embed_pooler.model.layers.21.self_attn.v_proj.weight', 'embed_pooler.model.layers.22.input_layernorm.weight', 'embed_pooler.model.layers.22.mlp.down_proj.weight', 'embed_pooler.model.layers.22.mlp.gate_proj.weight', 'embed_pooler.model.layers.22.mlp.up_proj.weight', 'embed_pooler.model.layers.22.post_attention_layernorm.weight', 'embed_pooler.model.layers.22.self_attn.k_proj.bias', 'embed_pooler.model.layers.22.self_attn.k_proj.weight', 'embed_pooler.model.layers.22.self_attn.o_proj.weight', 'embed_pooler.model.layers.22.self_attn.q_proj.bias', 'embed_pooler.model.layers.22.self_attn.q_proj.weight', 'embed_pooler.model.layers.22.self_attn.v_proj.bias', 'embed_pooler.model.layers.22.self_attn.v_proj.weight', 'embed_pooler.model.layers.23.input_layernorm.weight', 'embed_pooler.model.layers.23.mlp.down_proj.weight', 'embed_pooler.model.layers.23.mlp.gate_proj.weight', 'embed_pooler.model.layers.23.mlp.up_proj.weight', 'embed_pooler.model.layers.23.post_attention_layernorm.weight', 'embed_pooler.model.layers.23.self_attn.k_proj.bias', 'embed_pooler.model.layers.23.self_attn.k_proj.weight', 'embed_pooler.model.layers.23.self_attn.o_proj.weight', 'embed_pooler.model.layers.23.self_attn.q_proj.bias', 'embed_pooler.model.layers.23.self_attn.q_proj.weight', 'embed_pooler.model.layers.23.self_attn.v_proj.bias', 'embed_pooler.model.layers.23.self_attn.v_proj.weight', 'embed_pooler.model.layers.24.input_layernorm.weight', 'embed_pooler.model.layers.24.mlp.down_proj.weight', 'embed_pooler.model.layers.24.mlp.gate_proj.weight', 'embed_pooler.model.layers.24.mlp.up_proj.weight', 'embed_pooler.model.layers.24.post_attention_layernorm.weight', 'embed_pooler.model.layers.24.self_attn.k_proj.bias', 'embed_pooler.model.layers.24.self_attn.k_proj.weight', 'embed_pooler.model.layers.24.self_attn.o_proj.weight', 'embed_pooler.model.layers.24.self_attn.q_proj.bias', 'embed_pooler.model.layers.24.self_attn.q_proj.weight', 'embed_pooler.model.layers.24.self_attn.v_proj.bias', 'embed_pooler.model.layers.24.self_attn.v_proj.weight', 'embed_pooler.model.layers.25.input_layernorm.weight', 'embed_pooler.model.layers.25.mlp.down_proj.weight', 'embed_pooler.model.layers.25.mlp.gate_proj.weight', 'embed_pooler.model.layers.25.mlp.up_proj.weight', 'embed_pooler.model.layers.25.post_attention_layernorm.weight', 'embed_pooler.model.layers.25.self_attn.k_proj.bias', 'embed_pooler.model.layers.25.self_attn.k_proj.weight', 'embed_pooler.model.layers.25.self_attn.o_proj.weight', 'embed_pooler.model.layers.25.self_attn.q_proj.bias', 'embed_pooler.model.layers.25.self_attn.q_proj.weight', 'embed_pooler.model.layers.25.self_attn.v_proj.bias', 'embed_pooler.model.layers.25.self_attn.v_proj.weight', 'embed_pooler.model.layers.26.input_layernorm.weight', 'embed_pooler.model.layers.26.mlp.down_proj.weight', 'embed_pooler.model.layers.26.mlp.gate_proj.weight', 'embed_pooler.model.layers.26.mlp.up_proj.weight', 'embed_pooler.model.layers.26.post_attention_layernorm.weight', 'embed_pooler.model.layers.26.self_attn.k_proj.bias', 'embed_pooler.model.layers.26.self_attn.k_proj.weight', 'embed_pooler.model.layers.26.self_attn.o_proj.weight', 'embed_pooler.model.layers.26.self_attn.q_proj.bias', 'embed_pooler.model.layers.26.self_attn.q_proj.weight', 'embed_pooler.model.layers.26.self_attn.v_proj.bias', 'embed_pooler.model.layers.26.self_attn.v_proj.weight', 'embed_pooler.model.layers.27.input_layernorm.weight', 'embed_pooler.model.layers.27.mlp.down_proj.weight', 'embed_pooler.model.layers.27.mlp.gate_proj.weight', 'embed_pooler.model.layers.27.mlp.up_proj.weight', 'embed_pooler.model.layers.27.post_attention_layernorm.weight', 'embed_pooler.model.layers.27.self_attn.k_proj.bias', 'embed_pooler.model.layers.27.self_attn.k_proj.weight', 'embed_pooler.model.layers.27.self_attn.o_proj.weight', 'embed_pooler.model.layers.27.self_attn.q_proj.bias', 'embed_pooler.model.layers.27.self_attn.q_proj.weight', 'embed_pooler.model.layers.27.self_attn.v_proj.bias', 'embed_pooler.model.layers.27.self_attn.v_proj.weight', 'embed_pooler.model.layers.3.input_layernorm.weight', 'embed_pooler.model.layers.3.mlp.down_proj.weight', 'embed_pooler.model.layers.3.mlp.gate_proj.weight', 'embed_pooler.model.layers.3.mlp.up_proj.weight', 'embed_pooler.model.layers.3.post_attention_layernorm.weight', 'embed_pooler.model.layers.3.self_attn.k_proj.bias', 'embed_pooler.model.layers.3.self_attn.k_proj.weight', 'embed_pooler.model.layers.3.self_attn.o_proj.weight', 'embed_pooler.model.layers.3.self_attn.q_proj.bias', 'embed_pooler.model.layers.3.self_attn.q_proj.weight', 'embed_pooler.model.layers.3.self_attn.v_proj.bias', 'embed_pooler.model.layers.3.self_attn.v_proj.weight', 'embed_pooler.model.layers.4.input_layernorm.weight', 'embed_pooler.model.layers.4.mlp.down_proj.weight', 'embed_pooler.model.layers.4.mlp.gate_proj.weight', 'embed_pooler.model.layers.4.mlp.up_proj.weight', 'embed_pooler.model.layers.4.post_attention_layernorm.weight', 'embed_pooler.model.layers.4.self_attn.k_proj.bias', 'embed_pooler.model.layers.4.self_attn.k_proj.weight', 'embed_pooler.model.layers.4.self_attn.o_proj.weight', 'embed_pooler.model.layers.4.self_attn.q_proj.bias', 'embed_pooler.model.layers.4.self_attn.q_proj.weight', 'embed_pooler.model.layers.4.self_attn.v_proj.bias', 'embed_pooler.model.layers.4.self_attn.v_proj.weight', 'embed_pooler.model.layers.5.input_layernorm.weight', 'embed_pooler.model.layers.5.mlp.down_proj.weight', 'embed_pooler.model.layers.5.mlp.gate_proj.weight', 'embed_pooler.model.layers.5.mlp.up_proj.weight', 'embed_pooler.model.layers.5.post_attention_layernorm.weight', 'embed_pooler.model.layers.5.self_attn.k_proj.bias', 'embed_pooler.model.layers.5.self_attn.k_proj.weight', 'embed_pooler.model.layers.5.self_attn.o_proj.weight', 'embed_pooler.model.layers.5.self_attn.q_proj.bias', 'embed_pooler.model.layers.5.self_attn.q_proj.weight', 'embed_pooler.model.layers.5.self_attn.v_proj.bias', 'embed_pooler.model.layers.5.self_attn.v_proj.weight', 'embed_pooler.model.layers.6.input_layernorm.weight', 'embed_pooler.model.layers.6.mlp.down_proj.weight', 'embed_pooler.model.layers.6.mlp.gate_proj.weight', 'embed_pooler.model.layers.6.mlp.up_proj.weight', 'embed_pooler.model.layers.6.post_attention_layernorm.weight', 'embed_pooler.model.layers.6.self_attn.k_proj.bias', 'embed_pooler.model.layers.6.self_attn.k_proj.weight', 'embed_pooler.model.layers.6.self_attn.o_proj.weight', 'embed_pooler.model.layers.6.self_attn.q_proj.bias', 'embed_pooler.model.layers.6.self_attn.q_proj.weight', 'embed_pooler.model.layers.6.self_attn.v_proj.bias', 'embed_pooler.model.layers.6.self_attn.v_proj.weight', 'embed_pooler.model.layers.7.input_layernorm.weight', 'embed_pooler.model.layers.7.mlp.down_proj.weight', 'embed_pooler.model.layers.7.mlp.gate_proj.weight', 'embed_pooler.model.layers.7.mlp.up_proj.weight', 'embed_pooler.model.layers.7.post_attention_layernorm.weight', 'embed_pooler.model.layers.7.self_attn.k_proj.bias', 'embed_pooler.model.layers.7.self_attn.k_proj.weight', 'embed_pooler.model.layers.7.self_attn.o_proj.weight', 'embed_pooler.model.layers.7.self_attn.q_proj.bias', 'embed_pooler.model.layers.7.self_attn.q_proj.weight', 'embed_pooler.model.layers.7.self_attn.v_proj.bias', 'embed_pooler.model.layers.7.self_attn.v_proj.weight', 'embed_pooler.model.layers.8.input_layernorm.weight', 'embed_pooler.model.layers.8.mlp.down_proj.weight', 'embed_pooler.model.layers.8.mlp.gate_proj.weight', 'embed_pooler.model.layers.8.mlp.up_proj.weight', 'embed_pooler.model.layers.8.post_attention_layernorm.weight', 'embed_pooler.model.layers.8.self_attn.k_proj.bias', 'embed_pooler.model.layers.8.self_attn.k_proj.weight', 'embed_pooler.model.layers.8.self_attn.o_proj.weight', 'embed_pooler.model.layers.8.self_attn.q_proj.bias', 'embed_pooler.model.layers.8.self_attn.q_proj.weight', 'embed_pooler.model.layers.8.self_attn.v_proj.bias', 'embed_pooler.model.layers.8.self_attn.v_proj.weight', 'embed_pooler.model.layers.9.input_layernorm.weight', 'embed_pooler.model.layers.9.mlp.down_proj.weight', 'embed_pooler.model.layers.9.mlp.gate_proj.weight', 'embed_pooler.model.layers.9.mlp.up_proj.weight', 'embed_pooler.model.layers.9.post_attention_layernorm.weight', 'embed_pooler.model.layers.9.self_attn.k_proj.bias', 'embed_pooler.model.layers.9.self_attn.k_proj.weight', 'embed_pooler.model.layers.9.self_attn.o_proj.weight', 'embed_pooler.model.layers.9.self_attn.q_proj.bias', 'embed_pooler.model.layers.9.self_attn.q_proj.weight', 'embed_pooler.model.layers.9.self_attn.v_proj.bias', 'embed_pooler.model.layers.9.self_attn.v_proj.weight', 'embed_pooler.model.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from hidden_capacity_reasoning.models import Qwen2ForCausalLMCompressionV1\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = Qwen2ForCausalLMCompressionV1.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.model.requires_grad_(False)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "lora_name = \"outputs/2025_03_13_17_26_24_256272/checkpoint-95800\"\n",
    "model = PeftModel.from_pretrained(model, lora_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<beginofsentence>You are a helpful assistant.<User>how many wings has a bird?<Assistant><think>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Okay'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"how many wings has a bird?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    # generated_ids = model.generate(\n",
    "    #     model_inputs.input_ids,\n",
    "    #     max_new_tokens=1000,\n",
    "    #     do_sample=False,\n",
    "    # )\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### default embed generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "max_steps = 400\n",
    "for _ in range(max_steps):\n",
    "    logits = model(\n",
    "        inputs_embeds=generated_embeds,\n",
    "    ).logits\n",
    "    top_token = logits.argmax(-1)[-1][-1]\n",
    "    top_token_embed = model.get_input_embeddings()(top_token)\n",
    "    # print(top)\n",
    "    generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "    generated_embeds = torch.cat(\n",
    "        [generated_embeds, top_token_embed.reshape(1, 1, -1)], dim=1\n",
    "    )\n",
    "    # break\n",
    "# print(tokenizer.decode(generated_tokens[-1]))\n",
    "# break\n",
    "embeds_generation_tokens = generated_tokens[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generation with compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ModelEmbedPoolerV1(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): None\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8960, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): None\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.embed_pooler  # .embed_pooler_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(model.base_model, \"embed_pooler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dim/open_orca_4475_DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=1000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\\nAnswer:',\n",
       " 'answer': \"Okay, so I need to figure out whether the review is flattering or unflattering about the Rajah Medium Curry Powder product. Let me start by reading the review again carefully.\\n\\nThe user says the product's fulfillment was efficient and prompt. That's a positive point. Then they mention having curry powder in a tin is better than having it in a packet in a box. Hmm, that's interesting. I wonder why they think a tin is better. Maybe because it's more convenient or easier to use? I'm not sure if that's a positive or negative thing.\\n\\nNext, they note that the 2-pack is great because the sealed content remains fresh. That's another positive point. They also praise the company's product range and express looking forward to supporting them in the future. \\n\\nSo, the main points are positive: efficient delivery, convenience, fresh sealed content, and positive feedback about the company. The reviewer doesn't mention any negative aspects, so I don't have to worry about that.\\n\\nNow, the question is whether this review depicts the product in a flattering or unflattering light. Flattered would mean the review is positive and confident, while unflattering would be negative or critical.\\n\\nLooking at the review, it's all positive. The user is happy with the product, the delivery, and the company's reputation. There's no mention of any issues or complaints. So, it's more about the overall impression rather than any negative traits.\\n\\nTherefore, the review is likely to be flattering because it's positive and constructive. It's not just listing positives but also expressing enthusiasm about the product and the company.\\n</think>\\n\\nThe review is flattering. It highlights positive aspects such as efficient delivery, convenience, fresh sealed content, and positive feedback about the product and company. There are no negative comments, so it reflects a constructive and positive impression.\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay\n",
      "1 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay,\n",
      "2 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so\n",
      "3 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I\n",
      "4 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need\n",
      "5 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to\n",
      "6 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure\n",
      "7 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out\n",
      "8 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether\n",
      "9 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the\n",
      "1 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!\n",
      "2 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!\n",
      "3 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!\n",
      "4 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!\n",
      "5 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!\n",
      "6 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!\n",
      "7 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!\n",
      "8 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!\n",
      "9 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!\n",
      "1 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!\n",
      "2 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!\n",
      "3 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!\n",
      "4 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!\n",
      "5 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!\n",
      "6 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!\n",
      "7 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!\n",
      "8 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!\n",
      "9 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!\n",
      "1 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!\n",
      "2 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!\n",
      "3 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!\n",
      "4 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!\n",
      "5 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!\n",
      "6 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "7 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "8 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "9 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "1 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "2 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "3 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "4 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "5 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "6 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "7 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "8 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "9 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "1 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "2 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "3 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "4 <beginofsentence><User>Question: Title: Rajah Medium Curry Powder Product review: The fulfillment was efficient and prompt. Having curry powder in a tin is far better than having a packet in a box. The 2 Pack is great as the sealed content remains fresh. This company has a great range of products and I look forward to supporting them in the future! Would you say this review depicts the product in a flattering or unflattering light?\n",
      "Answer:<Assistant><think>\n",
      "Okay, so I need to figure out whether the!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "from hidden_capacity_reasoning.utils import WINDOW_SIZE\n",
    "\n",
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        # {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "        # {\"role\": \"user\", \"content\": \"how many wings has a bird?\"},\n",
    "        {\"role\": \"user\", \"content\": dataset[\"train\"][0][\"question\"]},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "max_steps = 50\n",
    "temp_gen_size = 0\n",
    "compression_started = False\n",
    "window_size = WINDOW_SIZE \n",
    "\n",
    "for _ in range(max_steps):\n",
    "    if temp_gen_size == window_size:\n",
    "        new_embeds_for_compression = generated_embeds[:, -window_size:]\n",
    "        if hasattr(model.base_model, \"embed_pooler\"):\n",
    "            compressed_part = model.base_model.embed_pooler(new_embeds_for_compression)\n",
    "        else:\n",
    "            compressed_part = model.embed_pooler(new_embeds_for_compression)\n",
    "\n",
    "        generated_embeds = torch.cat([generated_embeds, compressed_part], dim=1)\n",
    "        temp_gen_size = 1\n",
    "\n",
    "    logits = model(\n",
    "        inputs_embeds=generated_embeds,\n",
    "    ).logits\n",
    "    top_token = logits.argmax(-1)[-1][-1]\n",
    "    top_token_embed = model.get_input_embeddings()(top_token)\n",
    "    # print(top)\n",
    "    generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "\n",
    "    generated_embeds = torch.cat(\n",
    "        [generated_embeds, top_token_embed.reshape(1, 1, -1)], dim=1\n",
    "    )\n",
    "    print(temp_gen_size, tokenizer.decode(generated_tokens[-1]))\n",
    "    temp_gen_size += 1\n",
    "\n",
    "# print(tokenizer.decode(generated_tokens[-1]))\n",
    "\n",
    "# break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
