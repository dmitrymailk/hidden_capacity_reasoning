{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffdb004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "224 202 0.9017857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "# model = model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    # \"dim/hendrycks_math_train_12k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096\"\n",
    "    # \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    # \"dim/hendrycks_math_train_1k_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    "    \"dim/hendrycks_math_test_500_DeepSeek-R1-Distill-Qwen-1.5B_max_len_4096_greedy\"\n",
    ")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(\n",
    "    # test_size=250,\n",
    "    test_size=350,\n",
    "    # test_size=999,\n",
    "    # test_size=1,\n",
    "    seed=42,\n",
    ")\n",
    "dataset = dataset[\"test\"].filter(lambda x: x[\"model_answer\"].count(\"</think>\") == 1)\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "\n",
    "correct_dataset = []\n",
    "\n",
    "for pos, item in enumerate(dataset):\n",
    "    try:\n",
    "        answer = dataset_answer_filter(item[\"answer\"])\n",
    "        model_answer = model_answer_filter(item[\"model_answer\"])\n",
    "        # print(answer, model_answer)\n",
    "        # break\n",
    "        if is_equiv(answer, model_answer):\n",
    "            correct_dataset.append(item)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(dataset), len(correct_dataset), len(correct_dataset) / len(dataset))\n",
    "\n",
    "correct_dataset = correct_dataset[:30]\n",
    "len(correct_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30c55e",
   "metadata": {},
   "source": [
    "### Default generation from embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbfa0d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "1081 1081 1080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "correct_items = 0\n",
    "torch.manual_seed(0)\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "evaluation_dataset = []\n",
    "# dataset_pos = 0\n",
    "# for dataset_pos in tqdm(range(len(correct_dataset))):\n",
    "dataset_pos = 5\n",
    "input_ids = correct_dataset[dataset_pos][\"problem\"]\n",
    "# print(input_ids)\n",
    "# print(\"===\")\n",
    "# print(\"===\")\n",
    "\n",
    "input_ids = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": base_prompt.format(question=input_ids),\n",
    "            },\n",
    "        ],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "# input_ids = dataset_item[\"input_ids\"]\n",
    "# generated_ids = dataset_item[\"generated_ids\"]\n",
    "generated_ids = [\n",
    "    tokenizer.encode(\n",
    "        correct_dataset[dataset_pos][\"model_answer\"],\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "]\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    input_ids = torch.tensor(input_ids).cuda()\n",
    "    input_ids_embeds = model.get_input_embeddings()(input_ids)\n",
    "    windows_amount = 100\n",
    "\n",
    "    generated_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds,\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    # print(\"COMPRESSED PART\", tokenizer.decode(next_true_tokens[-1]))\n",
    "    # print(\"===\")\n",
    "    generated_ids_compressed = model.generate(\n",
    "        inputs_embeds=generated_embeds,\n",
    "        # attention_mask=torch.ones_like(generated_embeds),\n",
    "        attention_mask=torch.ones(\n",
    "            generated_embeds.shape[:2],\n",
    "            device=\"cuda\",\n",
    "        ).long(),\n",
    "        max_new_tokens=4096,\n",
    "        # max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "        # do_sample=True,\n",
    "        # temperature=0.6,\n",
    "        # top_p=0.95,\n",
    "    )\n",
    "    # break\n",
    "generated_result = tokenizer.decode(generated_ids_compressed[-1])\n",
    "# print()\n",
    "gold_answer = correct_dataset[dataset_pos][\"answer\"]\n",
    "answer = dataset_answer_filter(gold_answer)\n",
    "model_answer = model_answer_filter(generated_result)\n",
    "if is_equiv(answer, model_answer):\n",
    "    correct_items += 1\n",
    "    print(\"CORRECT\")\n",
    "else:\n",
    "    print(\"WRONG\", gold_answer)\n",
    "    print(generated_result)\n",
    "compressed_total_len = generated_ids_compressed.shape[1]\n",
    "print(\n",
    "    generated_ids_compressed.shape[1],\n",
    "    compressed_total_len,\n",
    "    torch.tensor(generated_ids).shape[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f058fd3",
   "metadata": {},
   "source": [
    "## test time train generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c50f87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'In right triangle $ABC$ with $\\\\angle B = 90^\\\\circ$, we have $\\\\sin A = 2\\\\cos A$.  What is $\\\\tan A$?',\n",
       " 'solution': 'The triangle is shown below:\\n\\n[asy]\\npair A,B,C;\\nA = (0,0);\\nB = (5,0);\\nC = (5,10);\\ndraw(A--B--C--A);\\ndraw(rightanglemark(C,B,A,16));\\nlabel(\"$A$\",A,SW);\\nlabel(\"$B$\",B,SE);\\nlabel(\"$C$\",C,N);\\n[/asy]\\n\\nWe have $\\\\sin A = \\\\frac{BC}{AC}$ and $\\\\cos A = \\\\frac{AB}{AC}$, so $\\\\sin A = 2\\\\cos A$ gives us $\\\\frac{BC}{AC} = 2\\\\cdot\\\\frac{AB}{AC}$.  Multiplying both sides by $AC$ gives $BC = 2AB$, so $\\\\frac{BC}{AB} = 2$.  Finally, we have $\\\\tan A = \\\\frac{BC}{AB} = \\\\boxed{2}$.\\n\\nWe also could have noted that $\\\\tan A = \\\\frac{\\\\sin A}{\\\\cos A} = \\\\frac{2\\\\cos A}{\\\\cos A } =\\\\boxed{2}$.',\n",
       " 'answer': '2',\n",
       " 'subject': 'Geometry',\n",
       " 'level': 3,\n",
       " 'unique_id': 'test/geometry/178.json',\n",
       " 'model_answer': \"Okay, so I have this problem here: In right triangle ABC with angle B equal to 90 degrees, we know that sin A equals 2 cos A. The question is asking for tan A. Hmm, let me think about how to approach this.\\n\\nFirst, I remember that in a right triangle, the sine of an angle is the ratio of the opposite side to the hypotenuse, and the cosine is the ratio of the adjacent side to the hypotenuse. So, if angle A is one of the non-right angles in triangle ABC, then side opposite to angle A is BC, the side adjacent is AC, and the hypotenuse is AB.\\n\\nGiven that sin A = 2 cos A, I can write that as:\\n\\nsin A = 2 cos A\\n\\nI also know that tan A is sin A over cos A, so tan A = sin A / cos A. Since sin A is equal to 2 cos A, substituting that into the tan A equation gives:\\n\\ntan A = (2 cos A) / cos A\\n\\nWait, that simplifies to 2, right? Because the cos A in the numerator and denominator cancel out. So, tan A is 2? Hmm, that seems straightforward, but let me make sure I didn't skip any steps or make any mistakes.\\n\\nAlternatively, maybe I should use the Pythagorean identity to verify this. I remember that sin² A + cos² A = 1. Since sin A = 2 cos A, I can substitute that into the identity:\\n\\n(2 cos A)² + cos² A = 1\\n\\nCalculating that, it becomes:\\n\\n4 cos² A + cos² A = 1\\n\\nWhich simplifies to:\\n\\n5 cos² A = 1\\n\\nSo, cos² A = 1/5, which means cos A = ±1/√5. But since angle A is in a right triangle, it must be between 0 and 90 degrees, so cosine is positive. Therefore, cos A = 1/√5.\\n\\nThen, sin A = 2 cos A = 2*(1/√5) = 2/√5.\\n\\nTherefore, tan A = sin A / cos A = (2/√5) / (1/√5) = 2. So, that confirms it. Tan A is indeed 2.\\n\\nWait, but just to make sure, let me think about the triangle. If angle A has sin A = 2/√5 and cos A = 1/√5, then the sides opposite and adjacent to angle A would be 2 and 1, respectively, and the hypotenuse would be √(1² + 2²) = √5. So, the sides are 1, 2, √5, which is a classic 1-2-√5 triangle. That makes sense because in such a triangle, the sides are in the ratio 1:2:√5, which is a Pythagorean triple.\\n\\nSo, if angle A is opposite the side of length 2 and adjacent to the side of length 1, then tan A is opposite over adjacent, which is 2/1 = 2. That's consistent with what I found earlier.\\n\\nI think that's solid. So, tan A is 2.\\n\\n**Final Answer**\\nThe value of $\\\\tan A$ is \\\\boxed{2}.\\n</think>\\n\\nGiven a right triangle \\\\(ABC\\\\) with \\\\(\\\\angle B = 90^\\\\circ\\\\), we know that \\\\(\\\\sin A = 2 \\\\cos A\\\\). We need to find \\\\(\\\\tan A\\\\).\\n\\nFirst, we use the given relationship:\\n\\\\[\\n\\\\sin A = 2 \\\\cos A\\n\\\\]\\nWe also know that \\\\(\\\\tan A = \\\\frac{\\\\sin A}{\\\\cos A}\\\\). Substituting \\\\(\\\\sin A = 2 \\\\cos A\\\\) into this, we get:\\n\\\\[\\n\\\\tan A = \\\\frac{2 \\\\cos A}{\\\\cos A} = 2\\n\\\\]\\n\\nTo verify, we use the Pythagorean identity \\\\(\\\\sin^2 A + \\\\cos^2 A = 1\\\\). Substituting \\\\(\\\\sin A = 2 \\\\cos A\\\\), we have:\\n\\\\[\\n(2 \\\\cos A)^2 + \\\\cos^2 A = 1\\n\\\\]\\n\\\\[\\n4 \\\\cos^2 A + \\\\cos^2 A = 1\\n\\\\]\\n\\\\[\\n5 \\\\cos^2 A = 1\\n\\\\]\\n\\\\[\\n\\\\cos^2 A = \\\\frac{1}{5}\\n\\\\]\\n\\\\[\\n\\\\cos A = \\\\frac{1}{\\\\sqrt{5}} \\\\quad (\\\\text{since } \\\\cos A \\\\text{ is positive})\\n\\\\]\\n\\nThen, \\\\(\\\\sin A = 2 \\\\cos A = \\\\frac{2}{\\\\sqrt{5}}\\\\). Therefore,\\n\\\\[\\n\\\\tan A = \\\\frac{\\\\sin A}{\\\\cos A} = \\\\frac{\\\\frac{2}{\\\\sqrt{5}}}{\\\\frac{1}{\\\\sqrt{5}}} = 2\\n\\\\]\\n\\nThus, the value of \\\\(\\\\tan A\\\\) is \\\\(\\\\boxed{2}\\\\).\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_dataset[dataset_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "303e6e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1197467/409245293.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  content_compression_mask = torch.tensor(tokenized_turn[\"content_compression_mask\"])\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Problem: The quadratic $x^2+(2.6)x+3.6$ can be written in the form $(x+b)^2+c$, where $b$ and $c$ are constants. What is $b+c$ (as a decimal)?\n",
      "\n",
      "Please reason step by step, and put your final answer within \\boxed{}.<｜Assistant｜><think>\n",
      "\n",
      "Okay, so I have this quadratic equation: \\( x^2 + 2.6x + 3.6 \\). The problem says I need to write it in the form \\( (x + b)^2 + c \\), and then find the sum of \\( b \\) and \\( c \\). Hmm, I think this is about completing the square. Let me recall how that works.\n",
      "\n",
      "Completing the square is a method used to convert a quadratic equation from standard form to vertex form, which is exactly what they're asking for here. The standard form is \\( ax^2 + bx + c \\), and the vertex form is \\( a(x - h)^2 + k \\). In this case, the quadratic is already in standard form, so I need to manipulate it to get it into the vertex form.\n",
      "\n",
      "The given quadratic is \\( x^2 + 2.6x + 3.6 \\). Since the coefficient of \\( x^2 \\) is 1, that should make things a bit easier. The vertex form is \\( (x + b)^2 + c \\). Wait, actually, in the standard vertex form, it's \\( (x - h)^2 + k \\). So, in this case, \\( h \\) would be negative \\( b \\), and \\( k \\) is \\( c \\). So, I need to find \\( h \\) and \\( k \\) such that \\( (x + h)^2 + k \\) equals the original quadratic.\n",
      "\n",
      "Let me write down the original quadratic:\n",
      "\n",
      "\\( x^2 + 2.6x + 3.6 \\)\n",
      "\n",
      "I need to express this as \\( (x + b)^2 + c \\). Let me expand \\( (x + b)^2 \\) to see what it looks like:\n",
      "\n",
      "\\( (x + b)^2 = x^2 + 2bx + b^2 \\)\n",
      "\n",
      "So, if I add \\( c \\) to\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from lm_eval.tasks.hendrycks_math.utils import strip_string, remove_boxed, is_equiv\n",
    "from hidden_capacity_reasoning.evaluation.math_500.utils import (\n",
    "    dataset_answer_filter,\n",
    "    model_answer_filter,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from hidden_capacity_reasoning.utils import tokenize_single_turn\n",
    "\n",
    "correct_items = 0\n",
    "torch.manual_seed(0)\n",
    "base_prompt = open(\n",
    "    \"hidden_capacity_reasoning/evaluation/math_500/math_500_prompt\"\n",
    ").read()\n",
    "\n",
    "# dataset_pos = 5\n",
    "dataset_pos = 6\n",
    "tokenized_turn = tokenize_single_turn(\n",
    "    question=base_prompt.format(question=correct_dataset[dataset_pos][\"problem\"]),\n",
    "    answer=correct_dataset[dataset_pos][\"model_answer\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "for key in tokenized_turn.keys():\n",
    "    tokenized_turn[key] = torch.tensor(tokenized_turn[key])\n",
    "\n",
    "evaluation_dataset = []\n",
    "\n",
    "\n",
    "input_ids = correct_dataset[dataset_pos][\"problem\"]\n",
    "\n",
    "input_ids = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": base_prompt.format(question=input_ids),\n",
    "            },\n",
    "        ],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "content_compression_mask = torch.tensor(tokenized_turn[\"content_compression_mask\"])\n",
    "\n",
    "input_part_end = (content_compression_mask == 0).nonzero()[-3][0]\n",
    "question_input_ids = tokenized_turn[\"input_ids\"][: int(input_part_end) + 1].unsqueeze(0)\n",
    "print(tokenizer.decode(question_input_ids[-1]))\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    input_ids = torch.tensor(input_ids).cuda()\n",
    "    input_ids_embeds = model.get_input_embeddings()(input_ids)\n",
    "    max_new_tokens = 400\n",
    "\n",
    "    inputs_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds,\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    generated_ids_new = model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=torch.ones(\n",
    "            inputs_embeds.shape[:2],\n",
    "            device=\"cuda\",\n",
    "        ).long(),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    # break\n",
    "generated_result = tokenizer.decode(generated_ids_new[-1])\n",
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3461495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Problem: The quadratic $x^2+(2.6)x+3.6$ can be written in the form $(x+b)^2+c$, where $b$ and $c$ are constants. What is $b+c$ (as a decimal)?\n",
      "\n",
      "Please reason step by step, and put your final answer within \\boxed{}.<｜Assistant｜><think>\n",
      "Okay, so I have this quadratic equation: \\( x^2 + 2.6x + 3.6 \\). The problem says I need to write it in the form \\( (x + b)^2 + c \\), and then find the sum of \\( b \\) and \\( c \\). Hmm, I think this is about completing the square. Let me recall how that works.\n",
      "\n",
      "Completing the square is a method used to convert a quadratic equation from standard form to vertex form, which is exactly what they're asking for here. The standard form is \\( ax^2 + bx + c \\), and the vertex form is \\( a(x - h)^2 + k \\). In this case, the quadratic is already in standard form, so I need to manipulate it to get it into the vertex form.\n",
      "\n",
      "The given quadratic is \\( x^2 + 2.6x + 3.6 \\). Since the coefficient of \\( x^2 \\) is 1, that should make things a bit easier. The vertex form is \\( (x + b)^2 + c \\). Wait, actually, in the standard vertex form, it's \\( (x - h)^2 + k \\). So, in this case, \\( h \\) would be negative \\( b \\), and \\( k \\) is \\( c \\). So, I need to find \\( h \\) and \\( k \\) such that \\( (x + h)^2 + k \\) equals the original quadratic.\n",
      "\n",
      "Let me write down the original quadratic:\n",
      "\n",
      "\\( x^2 + 2.6x + 3.6 \\)\n",
      "\n",
      "I need to express this as \\( (x + b)^2 + c \\). Let me expand \\( (x + b)^2 \\) to see what it looks like:\n",
      "\n",
      "\\( (x + b)^2 = x^2 + 2bx + b^2 \\)\n",
      "\n",
      "So, if I add \\( c \\) to\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[151646, 151644,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100, 151645, 151648,\n",
       "            198,  32313,     11,    773,    358,    614,    419,  79151,  23606,\n",
       "             25,  17767,    856,     61,     17,    488,    220,     17,     13,\n",
       "             21,     87,    488,    220,     18,     13,     21,   1124,    568,\n",
       "            576,   3491,   2727,    358,   1184,    311,   3270,    432,    304,\n",
       "            279,   1352,  17767,    320,     87,    488,    293,  29776,     17,\n",
       "            488,    272,   1124,    701,    323,   1221,   1477,    279,   2629,\n",
       "            315,  17767,    293,   1124,      8,    323,  17767,    272,   1124,\n",
       "            568,  88190,     11,    358,   1744,    419,    374,    911,  26568,\n",
       "            279,   9334,     13,   6771,    752,  19091,   1246,    429,   4278,\n",
       "            382,  66625,   1280,    279,   9334,    374,    264,   1714,   1483,\n",
       "            311,   5508,    264,  79151,  23606,    504,   5297,   1352,    311,\n",
       "          11936,   1352,     11,    892,    374,   6896,   1128,    807,   2299,\n",
       "          10161,    369,   1588,     13,    576,   5297,   1352,    374,  17767,\n",
       "           3859,     61,     17,    488,  44241,    488,    272,   1124,    701,\n",
       "            323,    279,  11936,   1352,    374,  17767,    264,   2075,    481,\n",
       "            305,  29776,     17,    488,    595,   1124,    568,    758,    419,\n",
       "           1142,     11,    279,  79151,    374,   2669,    304,   5297,   1352,\n",
       "             11,    773,    358,   1184,    311,  36635,    432,    311,    633,\n",
       "            432,   1119,    279,  11936,   1352,    382,    785,   2661,  79151,\n",
       "            374,  17767,    856,     61,     17,    488,    220,     17,     13,\n",
       "             21,     87,    488,    220,     18,     13,     21,   1124,    568,\n",
       "           8704,    279,  35606,    315,  17767,    856,     61,     17,   1124,\n",
       "              8,    374,    220,     16,     11,    429,   1265,   1281,   2513,\n",
       "            264,   2699,   8661,     13,    576,  11936,   1352,    374,  17767,\n",
       "            320,     87,    488,    293,  29776,     17,    488,    272,   1124,\n",
       "            568,  13824,     11,   3520,     11,    304,    279,   5297,  11936,\n",
       "           1352,     11,    432,    594,  17767,    320,     87,    481,    305,\n",
       "          29776,     17,    488,    595,   1124,    568,   2055,     11,    304,\n",
       "            419,   1142,     11,  17767,    305,   1124,      8,   1035,    387,\n",
       "           8225,  17767,    293,   1124,    701,    323,  17767,    595,   1124,\n",
       "              8,    374,  17767,    272,   1124,    568,   2055,     11,    358,\n",
       "           1184,    311,   1477,  17767,    305,   1124,      8,    323,  17767,\n",
       "            595,   1124,      8,   1741,    429,  17767,    320,     87,    488,\n",
       "            305,  29776,     17,    488,    595,   1124,      8,  16819,    279,\n",
       "           4024,  79151,    382,  10061,    752,   3270,   1495,    279,   4024,\n",
       "          79151,   1447,  44292,    856,     61,     17,    488,    220,     17,\n",
       "             13,     21,     87,    488,    220,     18,     13,     21,   1124,\n",
       "            692,     40,   1184,    311,   3158,    419,    438,  17767,    320,\n",
       "             87,    488,    293,  29776,     17,    488,    272,   1124,    568,\n",
       "           6771,    752,   9225,  17767,    320,     87,    488,    293,  29776,\n",
       "             17,   1124,      8,    311,   1490,   1128,    432,   5868,   1075,\n",
       "           1447,  44292,    320,     87,    488,    293,  29776,     17,    284,\n",
       "            856,     61,     17,    488,    220,     17,  21861,    488,    293,\n",
       "             61,     17,   1124,    692,   4416,     11,    421,    358,    912,\n",
       "          17767,    272,   1124,      8,    311]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.cat([question_input_ids.cuda(), generated_ids_new.cuda()], dim=1)\n",
    "print(tokenizer.decode(labels[-1]))\n",
    "\n",
    "question_content_mask = content_compression_mask[: int(input_part_end) + 1].clone()\n",
    "question_content_mask[question_content_mask == 0] = 4\n",
    "question_content_mask[question_content_mask == 1] = 0\n",
    "question_content_mask[question_content_mask == 4] = 1\n",
    "train_content_mask_new = torch.cat(\n",
    "    [\n",
    "        question_content_mask,\n",
    "        torch.ones(\n",
    "            generated_ids_new.shape[1],\n",
    "        ),\n",
    "    ]\n",
    ").long()\n",
    "print(question_content_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_embeds = model.get_input_embeddings()(generated_ids_new)\n",
    "    new_input_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds,\n",
    "            generated_embeds,\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "labels[:, train_content_mask_new == 0] = -100\n",
    "# new_input_embeds\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed1bb527",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_id = tokenizer.eos_token_id\n",
    "bos_id = tokenizer.bos_token_id\n",
    "end_think_id = torch.tensor(tokenizer.encode(\"</think>\", add_special_tokens=False)[0])\n",
    "start_think_id = torch.tensor(\n",
    "    tokenizer.encode(\"<｜Assistant｜><think>\\n\", add_special_tokens=False)\n",
    ")\n",
    "user_id = torch.tensor(tokenizer.encode(\"<｜User｜>\", add_special_tokens=False)[0])\n",
    "\n",
    "# train_attention_mask = (\n",
    "#     input_ids == bos_id | input_ids == start_think_id | input_ids == user_id\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20f0154a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2947, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    original_loss = model(\n",
    "        inputs_embeds=new_input_embeds,\n",
    "        # attention_mask=train_attention_mask.unsqueeze(0).cuda(),\n",
    "        # attention_mask=torch.ones(new_input_embeds.size()[:2]).cuda().long(),\n",
    "        labels=labels,\n",
    "    ).loss\n",
    "original_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7bc65f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compression_tokens = 1\n",
    "# compression_tokens = max_new_tokens // 2\n",
    "compression_tokens = max_new_tokens // 8\n",
    "compression_tensor = torch.nn.Parameter(\n",
    "    torch.rand_like(\n",
    "        new_input_embeds[:, :compression_tokens, :],\n",
    "    )\n",
    "    * model.get_input_embeddings().weight.data.std(),\n",
    "    requires_grad=True,\n",
    ")\n",
    "compressed_inputs_embeds = torch.cat(\n",
    "    [\n",
    "        input_ids_embeds.detach(),\n",
    "        compression_tensor,\n",
    "        generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "    ],\n",
    "    dim=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7665c653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 50, 1536]), 50)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_tensor.shape, compression_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6d56cc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0275,  0.0029, -0.0052,  ...,  0.0306, -0.0159,  0.0216],\n",
       "         [-0.0344,  0.0019, -0.0027,  ...,  0.0216, -0.0178,  0.0244],\n",
       "         [-0.0068, -0.0247, -0.0413,  ..., -0.0012,  0.0081,  0.0493],\n",
       "         ...,\n",
       "         [-0.0242,  0.0099,  0.0051,  ...,  0.0217, -0.0115,  0.0197],\n",
       "         [-0.0203,  0.0073, -0.0091,  ...,  0.0225, -0.0061,  0.0275],\n",
       "         [ 0.0398, -0.0708,  0.0342,  ..., -0.0579,  0.0427, -0.0625]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4ca63fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0.0253, 0.0171, 0.0290,  ..., 0.0164, 0.0102, 0.0168],\n",
       "         [0.0051, 0.0215, 0.0075,  ..., 0.0093, 0.0031, 0.0203],\n",
       "         [0.0280, 0.0290, 0.0146,  ..., 0.0178, 0.0261, 0.0190],\n",
       "         ...,\n",
       "         [0.0139, 0.0011, 0.0014,  ..., 0.0242, 0.0159, 0.0255],\n",
       "         [0.0244, 0.0278, 0.0026,  ..., 0.0048, 0.0257, 0.0191],\n",
       "         [0.0091, 0.0152, 0.0013,  ..., 0.0037, 0.0016, 0.0016]]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "16c792ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 0.0274,  0.0208,  0.0213,  ...,  0.0144,  0.0100,  0.0129],\n",
       "         [-0.0014,  0.0313, -0.0052,  ...,  0.0343,  0.0071,  0.0129],\n",
       "         [ 0.0346,  0.0378,  0.0188,  ...,  0.0149,  0.0327,  0.0096],\n",
       "         ...,\n",
       "         [ 0.0293, -0.0169, -0.0487,  ...,  0.0626,  0.0307,  0.0178],\n",
       "         [ 0.0277,  0.0375, -0.0113,  ...,  0.0166,  0.0140, -0.0069],\n",
       "         [ 0.0301,  0.0065, -0.0088,  ..., -0.0039,  0.0399, -0.0068]]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2762d6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0275,  0.0029, -0.0052,  ...,  0.0306, -0.0159,  0.0216],\n",
       "         [-0.0344,  0.0019, -0.0027,  ...,  0.0216, -0.0178,  0.0244],\n",
       "         [-0.0068, -0.0247, -0.0413,  ..., -0.0012,  0.0081,  0.0493],\n",
       "         ...,\n",
       "         [-0.0242,  0.0099,  0.0051,  ...,  0.0217, -0.0115,  0.0197],\n",
       "         [-0.0203,  0.0073, -0.0091,  ...,  0.0225, -0.0061,  0.0275],\n",
       "         [ 0.0398, -0.0708,  0.0342,  ..., -0.0579,  0.0427, -0.0625]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3dc0d749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0505,  0.0034, -0.0226,  ..., -0.0679,  0.0032,  0.0082],\n",
       "         [ 0.0483, -0.0315,  0.0374,  ..., -0.0537,  0.0205, -0.0469],\n",
       "         [ 0.0110,  0.0070,  0.0408,  ..., -0.0236, -0.0127, -0.0437],\n",
       "         ...,\n",
       "         [ 0.0215, -0.0376,  0.0186,  ...,  0.0107,  0.0139, -0.0659],\n",
       "         [ 0.0432,  0.0038,  0.0366,  ..., -0.0649,  0.0442,  0.0317],\n",
       "         [ 0.0087, -0.0172,  0.0258,  ..., -0.0123,  0.0122, -0.0039]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_embeds[:, -(max_new_tokens // 2) :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1572f13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0505,  0.0034, -0.0226,  ..., -0.0679,  0.0032,  0.0082],\n",
       "         [ 0.0483, -0.0315,  0.0374,  ..., -0.0537,  0.0205, -0.0469],\n",
       "         [ 0.0110,  0.0070,  0.0408,  ..., -0.0236, -0.0127, -0.0437],\n",
       "         ...,\n",
       "         [ 0.0215, -0.0376,  0.0186,  ...,  0.0107,  0.0139, -0.0659],\n",
       "         [ 0.0432,  0.0038,  0.0366,  ..., -0.0649,  0.0442,  0.0317],\n",
       "         [ 0.0087, -0.0172,  0.0258,  ..., -0.0123,  0.0122, -0.0039]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_embeds[:, -(max_new_tokens // 2) :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e4f2ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed_labels = question_input_ids\n",
    "# comp\n",
    "question_labels = question_input_ids.clone()\n",
    "question_labels[0][question_content_mask == 0] = -100\n",
    "question_labels = question_labels.cuda()\n",
    "compressed_part = torch.ones(compression_tensor.shape[:2]).long().cuda()\n",
    "\n",
    "compressed_labels = torch.cat(\n",
    "    [\n",
    "        question_labels,\n",
    "        compressed_part,\n",
    "        generated_ids_new[:, -(max_new_tokens // 2) :],\n",
    "    ],\n",
    "    dim=-1,\n",
    ")\n",
    "# compressed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f2b53704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1431, device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    compression_loss = model(\n",
    "        inputs_embeds=compressed_inputs_embeds,\n",
    "        labels=compressed_labels,\n",
    "    ).loss\n",
    "\n",
    "compression_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c8bb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, Requires Gradient: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e7080fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_loss.item() <= original_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c6b6e216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(2.1431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "1\n",
      "tensor(1.5753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "2\n",
      "tensor(1.4054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "3\n",
      "tensor(1.1314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "4\n",
      "tensor(0.9445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "5\n",
      "tensor(0.8756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "6\n",
      "tensor(0.8547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "7\n",
      "tensor(0.8284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "8\n",
      "tensor(0.7996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "9\n",
      "tensor(0.7731, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "10\n",
      "tensor(0.7524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "11\n",
      "tensor(0.7396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "12\n",
      "tensor(0.7249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "13\n",
      "tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "14\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "15\n",
      "tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "16\n",
      "tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "17\n",
      "tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "18\n",
      "tensor(0.6631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "19\n",
      "tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "20\n",
      "tensor(0.6484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "21\n",
      "tensor(0.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "22\n",
      "tensor(0.6349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "23\n",
      "tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "24\n",
      "tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "25\n",
      "tensor(0.6179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "26\n",
      "tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "27\n",
      "tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "28\n",
      "tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "29\n",
      "tensor(0.5995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "30\n",
      "tensor(0.5954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "31\n",
      "tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "32\n",
      "tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "33\n",
      "tensor(0.5836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "34\n",
      "tensor(0.5800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "35\n",
      "tensor(0.5765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "36\n",
      "tensor(0.5730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "37\n",
      "tensor(0.5694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "38\n",
      "tensor(0.5656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "39\n",
      "tensor(0.5617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "40\n",
      "tensor(0.5576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "41\n",
      "tensor(0.5532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "42\n",
      "tensor(0.5484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "43\n",
      "tensor(0.5435, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "44\n",
      "tensor(0.5385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "45\n",
      "tensor(0.5329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "46\n",
      "tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "47\n",
      "tensor(0.5211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "48\n",
      "tensor(0.5147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "49\n",
      "tensor(0.5081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "50\n",
      "tensor(0.5028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "51\n",
      "tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "52\n",
      "tensor(0.4950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "53\n",
      "tensor(0.4882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "54\n",
      "tensor(0.4854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "55\n",
      "tensor(0.4801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "56\n",
      "tensor(0.4725, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "57\n",
      "tensor(0.4666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "58\n",
      "tensor(0.4621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "59\n",
      "tensor(0.4559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "60\n",
      "tensor(0.4499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "61\n",
      "tensor(0.4453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "62\n",
      "tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "63\n",
      "tensor(0.4342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "64\n",
      "tensor(0.4292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "65\n",
      "tensor(0.4234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "66\n",
      "tensor(0.4185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "67\n",
      "tensor(0.4138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "68\n",
      "tensor(0.4083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "69\n",
      "tensor(0.4028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "70\n",
      "tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "71\n",
      "tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "72\n",
      "tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "73\n",
      "tensor(0.3766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "74\n",
      "tensor(0.3696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "75\n",
      "tensor(0.3628, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "76\n",
      "tensor(0.3567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "77\n",
      "tensor(0.3516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "78\n",
      "tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "79\n",
      "tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "80\n",
      "tensor(0.3586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "81\n",
      "tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "82\n",
      "tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "83\n",
      "tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "84\n",
      "tensor(0.3563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "85\n",
      "tensor(0.3542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "86\n",
      "tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "87\n",
      "tensor(0.3492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "88\n",
      "tensor(0.3467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "89\n",
      "tensor(0.3532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "90\n",
      "tensor(0.3563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "91\n",
      "tensor(0.3430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "92\n",
      "tensor(0.3383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "93\n",
      "tensor(0.3365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "94\n",
      "tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "95\n",
      "tensor(0.3219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "96\n",
      "tensor(0.3150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "97\n",
      "tensor(0.3112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "98\n",
      "tensor(0.3021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "99\n",
      "tensor(0.2962, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epoch_amount = 100\n",
    "# epoch_amount = 500\n",
    "\n",
    "optimizer = torch.optim.AdamW([compression_tensor], lr=0.001)\n",
    "for epoch in range(epoch_amount):\n",
    "    print(epoch)\n",
    "    compressed_inputs_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds.detach(),\n",
    "            compression_tensor,\n",
    "            generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    compression_loss = model(\n",
    "        inputs_embeds=compressed_inputs_embeds,\n",
    "        labels=compressed_labels,\n",
    "    ).loss\n",
    "    print(compression_loss)\n",
    "    compression_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if compression_loss.item() <= original_loss.item():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0689821b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 0.1333, -0.0525,  0.0161,  ...,  0.0050,  0.0536,  0.0615],\n",
       "         [ 0.0280, -0.0860, -0.0381,  ..., -0.0432,  0.0546, -0.0791],\n",
       "         [-0.0107,  0.0577, -0.0345,  ..., -0.0124,  0.1097,  0.0769],\n",
       "         ...,\n",
       "         [ 0.1454,  0.0869,  0.1768,  ...,  0.0386,  0.0551, -0.0129],\n",
       "         [ 0.1206, -0.0238,  0.0889,  ...,  0.0130,  0.0191,  0.0909],\n",
       "         [ 0.1462,  0.2155, -0.0516,  ...,  0.1534,  0.2087,  0.1910]]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "536ffc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_input_embeds.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "19bbdbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5967, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_loss #/ compressed_inputs_embeds.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04da0317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2947, device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_loss #/ new_input_embeds.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "00ec7578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that, it becomes:\n",
      "\n",
      "\\( x^2 + 2bx + b^2 + c \\)\n",
      "\n",
      "Now, I can compare this with the original quadratic:\n",
      "\n",
      "Original: \\( x^2 + 2.6x + 3.6 \\)\n",
      "\n",
      "Expanded vertex form: \\( x^2 + 2bx + (b^2 + c) \\)\n",
      "\n",
      "So, by comparing coefficients, I can set up equations:\n",
      "\n",
      "1. Coefficient of \\( x \\): \\( 2b = 2.6 \\)\n",
      "2. Constant term: \\( b^2 + c = 3.6 \\)\n",
      "\n",
      "From the first equation, I can solve for \\( b \\):\n",
      "\n",
      "\\( 2b = 2.6 \\)\n",
      "\n",
      "Divide both sides by 2:\n",
      "\n",
      "\\( b = 2.6 / 2 \\)\n",
      "\n",
      "Calculating that:\n",
      "\n",
      "\\( 2.6 divided by 2 is 1.3 \\). So, \\( b = 1.3 \\)\n",
      "\n",
      "Now, plug this value of \\( b \\) into the second equation to find \\( c \\):\n",
      "\n",
      "\\( (1.3)^2 + c = 3.6 \\)\n",
      "\n",
      "Calculate \\( (1.3)^2 \\):\n",
      "\n",
      "\\( 1.3 * 1.3 = 1.69 \\)\n",
      "\n",
      "So, the equation becomes:\n",
      "\n",
      "\\( 1.69 + c = 3.6 \\)\n",
      "\n",
      "Subtract 1.69 from both sides:\n",
      "\n",
      "\\( c = 3.6 - 1.69 \\)\n",
      "\n",
      "Calculating that:\n",
      "\n",
      "\\( 3.6 minus 1.69 is 1.91 \\). So, \\( c = 1.91 \\)\n",
      "\n",
      "Now, the problem asks for \\( b + c \\). So, adding them together:\n",
      "\n",
      "\\( b + c = 1.3 + 1.91 \\)\n",
      "\n",
      "Adding those:\n",
      "\n",
      "\\( 1.3 + 1.91 = 3.21 \\)\n",
      "\n",
      "So, \\( b + c = 3.21 \\)\n",
      "\n",
      "Wait, let me double-check my calculations to make sure I didn't make any mistakes.\n",
      "\n",
      "First, solving for \\( b \\):\n",
      "\n",
      "\\( 2b = 2.6 \\) => \\( b = 1.3 \\). That seems correct.\n",
      "\n",
      "Then, \\( b^2 = (1.3)^2 = 1.69 \\). Correct.\n",
      "\n",
      "Then, \\( 1.69 + c = 3.6 \\) => \\( c = 3.6 - 1.69 = 1.91 \\). That also seems correct.\n",
      "\n",
      "Adding \\( b + c = 1.3 + 1.91 = 3.21 \\). Yes, that's correct.\n",
      "\n",
      "Alternatively, I can verify by expanding \\( (x + 1.3)^2 + 1.91 \\) and see if it equals the original quadratic.\n",
      "\n",
      "Expanding \\( (x + 1.3)^2 \\):\n",
      "\n",
      "\\( x^2 + 2*1.3x + (1.3)^2 = x^2 + 2.6x + 1.69 \\)\n",
      "\n",
      "Adding 1.91:\n",
      "\n",
      "\\( x^2 + 2.6x + 1.69 + 1.91 = x^2 + 2.6x + 3.6 \\)\n",
      "\n",
      "Which matches the original quadratic. So, my calculations are correct.\n",
      "\n",
      "Therefore, \\( b + c = 3.21 \\).\n",
      "\n",
      "**Final Answer**\n",
      "The value of \\( b + c \\) is \\boxed{3.21}.\n",
      "</think>\n",
      "\n",
      "To express the quadratic \\(x^2 + 2.6x + 3.6\\) in the form \\((x + b)^2 + c\\), we start by expanding \\((x + b)^2\\):\n",
      "\n",
      "\\[\n",
      "(x + b)^2 = x^2 + 2bx + b^2\n",
      "\\]\n",
      "\n",
      "Adding \\(c\\) to this, we get:\n",
      "\n",
      "\\[\n",
      "(x + b)^2 + c = x^2 + 2bx + b^2 + c\n",
      "\\]\n",
      "\n",
      "We compare this with the original quadratic \\(x^2 + 2.6x + 3.6\\). By matching the coefficients, we set up the following equations:\n",
      "\n",
      "1. Coefficient of \\(x\\): \\(2b = 2.6\\)\n",
      "2. Constant term: \\(b^2 + c = 3.6\\)\n",
      "\n",
      "Solving for \\(b\\) from the first equation:\n",
      "\n",
      "\\[\n",
      "2b = 2.6 \\implies b = \\frac{2.6}{2} = 1.3\n",
      "\\]\n",
      "\n",
      "Next, we substitute \\(b = 1.3\\) into the second equation to find \\(c\\):\n",
      "\n",
      "\\[\n",
      "(1.3)^2 + c = 3.6 \\implies 1.69 + c = 3.6 \\implies c = 3.6 - 1.69 = 1.91\n",
      "\\]\n",
      "\n",
      "Thus, \\(b = 1.3\\) and \\(c = 1.91\\). Adding these together:\n",
      "\n",
      "\\[\n",
      "b + c = 1.3 + 1.91 = 3.21\n",
      "\\]\n",
      "\n",
      "The value of \\(b + c\\) is \\(\\boxed{3.21}\\).<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    compressed_inputs_embeds = torch.cat(\n",
    "        [\n",
    "            input_ids_embeds.detach(),\n",
    "            # compression_tensor,\n",
    "            torch.rand_like(compression_tensor).cuda(),\n",
    "            # torch.zeros_like(compression_tensor).cuda(),\n",
    "            generated_embeds[:, -(max_new_tokens // 2) :, :].detach(),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    generated_ids_compressed = model.generate(\n",
    "        inputs_embeds=compressed_inputs_embeds,\n",
    "        attention_mask=torch.ones(\n",
    "            compressed_inputs_embeds.shape[:2],\n",
    "            device=\"cuda\",\n",
    "        ).long(),\n",
    "        max_new_tokens=4096,\n",
    "        # max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "        # do_sample=True,\n",
    "        # temperature=0.6,\n",
    "        # top_p=0.95,\n",
    "    )\n",
    "    # break\n",
    "generated_result = tokenizer.decode(generated_ids_compressed[-1])\n",
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e00a35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "1111 1111\n"
     ]
    }
   ],
   "source": [
    "# print()\n",
    "gold_answer = correct_dataset[dataset_pos][\"answer\"]\n",
    "answer = dataset_answer_filter(gold_answer)\n",
    "model_answer = model_answer_filter(generated_result)\n",
    "if is_equiv(answer, model_answer):\n",
    "    correct_items += 1\n",
    "    print(\"CORRECT\")\n",
    "else:\n",
    "    print(\"WRONG\", gold_answer)\n",
    "    print(generated_result)\n",
    "compressed_total_len = generated_ids_compressed.shape[1]\n",
    "print(\n",
    "    generated_ids_compressed.shape[1],\n",
    "    compressed_total_len,\n",
    "    # torch.tensor(generated_ids).shape[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab30b57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1361"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    compression_tensor.shape[1]\n",
    "    + generated_embeds[:, -(max_new_tokens // 2) :, :].shape[1]\n",
    "    + generated_ids_compressed.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9d2414c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1639"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(\n",
    "    correct_dataset[dataset_pos][\"model_answer\"],\n",
    "    add_special_tokens=False,\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
