{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 10100010: [0]\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 10100011: [1]\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 11000101: [2]\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 11010010: [3]\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 11010011: [4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def lsh_hashing(vectors, k=16):\n",
    "    \"\"\"\n",
    "    –•–µ—à–∏—Ä—É–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Locality Sensitive Hashing.\n",
    "\n",
    "    Args:\n",
    "        vectors: –ú–∞—Å—Å–∏–≤ NumPy, –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - –±–∏–Ω–∞—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä.\n",
    "        k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö–µ—à-—Ñ—É–Ω–∫—Ü–∏–π (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ö–µ—à-–∫–ª—é—á–∞).\n",
    "\n",
    "    Returns:\n",
    "        –°–ª–æ–≤–∞—Ä—å (—Ö–µ—à-—Ç–∞–±–ª–∏—Ü–∞), –≥–¥–µ –∫–ª—é—á–∏ - —Ö–µ—à-–∫–ª—é—á–∏, –∞ –∑–Ω–∞—á–µ–Ω–∏—è - —Å–ø–∏—Å–∫–∏ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –±–∞–∫–µ—Ç–µ.\n",
    "    \"\"\"\n",
    "\n",
    "    num_vectors = vectors.shape[0]\n",
    "    vector_dimension = vectors.shape[1]\n",
    "\n",
    "    # 1. –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã (—Ö–µ—à-—Ñ—É–Ω–∫—Ü–∏–∏)\n",
    "    hash_functions = np.random.normal(0, 2, size=(k, vector_dimension))\n",
    "    # hash_functions = np.random.rand(k, vector_dimension) - 0.5\n",
    "\n",
    "    # 2. –•–µ—à–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –≤—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä\n",
    "    hash_table = {}\n",
    "    for i in range(num_vectors):\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º dot product —Å –∫–∞–∂–¥–æ–π —Ö–µ—à-—Ñ—É–Ω–∫—Ü–∏–µ–π\n",
    "        dot_products = np.dot(hash_functions, vectors[i])\n",
    "        # print(dot_products)\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º dot product –≤ –±–∏–Ω–∞—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "        hash_bits = (dot_products >= 0).astype(\n",
    "            int\n",
    "        )  # >= 0, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –æ—á–µ–Ω—å –º–∞–ª—ã–º–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ —á–∏—Å–ª–∞–º–∏\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–∏—Ç—ã –≤ —Ö–µ—à-–∫–ª—é—á (–≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏)\n",
    "        hash_key = \"\".join(str(bit) for bit in hash_bits)\n",
    "        # print(hash_key)\n",
    "        # 3. –†–∞–∑–º–µ—â–∞–µ–º –≤–µ–∫—Ç–æ—Ä –≤ –±–∞–∫–µ—Ç\n",
    "        if hash_key not in hash_table:\n",
    "            hash_table[hash_key] = []\n",
    "        hash_table[hash_key].append(i)\n",
    "\n",
    "    return hash_table\n",
    "\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "vectors = np.array(\n",
    "    [\n",
    "        [1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
    "        [1, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    ],\n",
    ")\n",
    "\n",
    "hash_table = lsh_hashing(vectors, k=8)\n",
    "\n",
    "# –ü–µ—á–∞—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ö–µ—à-—Ç–∞–±–ª–∏—Ü—ã\n",
    "for hash_key, vector_indices in hash_table.items():\n",
    "    print(f\"–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º {hash_key}: {vector_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'111111111': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# The sentences to encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0100101011: [0]\n",
      "The weather is lovely today.\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0110101011: [1]\n",
      "The weather is lovely today\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0100101001: [2]\n",
      "The weather is lovely\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0110101111: [3]\n",
      "The weather is good today.\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0110101110: [4]\n",
      "The weather is bad today.\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0100111001: [5]\n",
      "It's so sunny outside!\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 1111101111: [6, 7, 8]\n",
      "He drove to the stadium.\n",
      "He drove to the stadium\n",
      "He drove to the stadium in the night\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0101111001: [9]\n",
      "He loves stadiums\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 1101101011: [10]\n",
      "I love cats\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 1101111001: [11]\n",
      "I love dogs\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0101011101: [12]\n",
      "He is programming on python.\n",
      "===\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"The weather is lovely today\",\n",
    "    \"The weather is lovely\",\n",
    "    \"The weather is good today.\",\n",
    "    \"The weather is bad today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "    \"He drove to the stadium\",\n",
    "    \"He drove to the stadium in the night\",\n",
    "    \"He loves stadiums\",\n",
    "    \"I love cats\",\n",
    "    \"I love dogs\",\n",
    "    \"He is programming on python.\",\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(\n",
    "    sentences,\n",
    "    # precision=\"binary\",\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "hash_table = lsh_hashing(embeddings, k=10)\n",
    "\n",
    "# –ü–µ—á–∞—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ö–µ—à-—Ç–∞–±–ª–∏—Ü—ã\n",
    "for hash_key, vector_indices in hash_table.items():\n",
    "    print(f\"–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º {hash_key}: {vector_indices}\")\n",
    "    for i in vector_indices:\n",
    "        print(sentences[i])\n",
    "    print(\"===\")\n",
    "    print(\"===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try On real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac7e6a599bb4ac6b6095a8fa77b55fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a335f737efd4d2babd4a61da0c6e2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quora_duplicate_triplets.jsonl:   0%|          | 0.00/183M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9125c80bc2747678c35a7d4c445e84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/101762 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"embedding-data/QQP_triplets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Why in India do we not have one on one political debate as in USA?',\n",
       " 'pos': ['Why cant we have a public debate between politicians in India like the one in US?'],\n",
       " 'neg': ['Can people on Quora stop India Pakistan debate? We are sick and tired seeing this everyday in bulk?',\n",
       "  'Why do politicians, instead of having a decent debate on issues going in and around the world, end up fighting always?',\n",
       "  'Can educated politicians make a difference in India?',\n",
       "  'What are some unusual aspects about politics and government in India?',\n",
       "  'What is debate?',\n",
       "  'Why does civic public communication and discourse seem so hollow in modern India?',\n",
       "  'What is a Parliamentary debate?',\n",
       "  \"Why do we always have two candidates at the U.S. presidential debate. yet the ballot has about 7 candidates? Isn't that a misrepresentation of democracy?\",\n",
       "  'Why is civic public communication and discourse so hollow in modern India?',\n",
       "  \"Aren't the Presidential debates teaching our whole country terrible communication skills and why is deliberate misrepresentation even allowed?\",\n",
       "  'Why are most Indian politicians uneducated?',\n",
       "  'Does Indian political leaders capable of doing face to face debates while running for office?',\n",
       "  'What is wrong with the Indian political system and the environment it has built in connection with the people of India? Have parties divided people more?',\n",
       "  'What is a debate?',\n",
       "  'Why do we have legislative council in india?',\n",
       "  'Why does the office of president of India, being politically neutral, not ask for Population control in India?',\n",
       "  \"Why don't we discuss tax and foreign policies more in Indian elections but are instead obsessed with socialist schemes?\",\n",
       "  'Why do Indian politicians lack nationalist thinking?',\n",
       "  'Do you hate indian politicians?',\n",
       "  'Is India facing more stessful times and politically charged atmosphere when compared to Congress regime?',\n",
       "  'Who is the best politician in India? Why?',\n",
       "  \"We all know about the present condition of Indian politicians; they are all just using us to run their train, but still, they win elections and rule over us. Why aren't people giving their vote to NOTA?\",\n",
       "  'Who are clean politicians in India?',\n",
       "  'Why are you not believing in Democracy of India?',\n",
       "  'What does politics in India mean? What are they actually doing?',\n",
       "  'What are the strongest arguments for a debate in favour of brain drain in India and what sources must be used for making a good short speech?',\n",
       "  'Do we really need an election commission in India?',\n",
       "  'Why is there no concept of political correctness in India? Is it a good thing or a bad thing?',\n",
       "  'Why is population control not on agenda of any political party in India?',\n",
       "  'Who are some of the most dangerous or worst politicians in India?']}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 1001111101: [0]\n",
      "POSITIVE\n",
      "Does imaginary gravity exist?\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0000101101: [1, 7]\n",
      "QUERY\n",
      "Can imaginary time, energy and gravity exist?\n",
      "Are Cengage books good for JEE Advanced?\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0101100101: [2]\n",
      "What are some good books for IIT JEE preparation for class 10?\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0100000101: [3]\n",
      "What should be the order of books to read for JEE preparations?\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0100111101: [4]\n",
      "How is Cengage Books for JEE Advanced?\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0001010101: [5]\n",
      "Which books did JEE Advanced 2016 AIR 2 Bhavesh Dhingra use in preparation for JEE?\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0000110001: [6, 9]\n",
      "What are some good books for JEE Chemistry?\n",
      "What are the best books for JEE in chemistry?\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 0100110101: [8]\n",
      "What are some good books for JEE Mains only?\n",
      "===\n",
      "===\n",
      "–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º 1000100001: [10, 11]\n",
      "What are the best mathematics books for the IIT-JEE preparation?\n",
      "What are the best books for theory in math for IIT-JEE preparation?\n",
      "===\n",
      "===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What were the books Aman Bansal used for his Jee preparation?',\n",
       " 'pos': ['Which books were used my Aman Bansal for JEE preparation?'],\n",
       " 'neg': ['What are some good books for IIT JEE preparation for class 10?',\n",
       "  'What should be the order of books to read for JEE preparations?',\n",
       "  'How is Cengage Books for JEE Advanced?',\n",
       "  'Which books did JEE Advanced 2016 AIR 2 Bhavesh Dhingra use in preparation for JEE?',\n",
       "  'What are some good books for JEE Chemistry?',\n",
       "  'Are Cengage books good for JEE Advanced?',\n",
       "  'What are some good books for JEE Mains only?',\n",
       "  'What are the best books for JEE in chemistry?',\n",
       "  'What are the best mathematics books for the IIT-JEE preparation?',\n",
       "  'What are the best books for theory in math for IIT-JEE preparation?',\n",
       "  'Are NCERT books enough for the JEE Main?',\n",
       "  'How are JEE Advanced papers prepared?',\n",
       "  'How should I study for JEE?',\n",
       "  'Which book should i use for JEE  organic chemistry?',\n",
       "  'Which book should I use for JEE organic chemistry?',\n",
       "  'What are the best books for the JEE (Main) and 12th boards (PCM) CBSE?',\n",
       "  'How to study for JEE advance?',\n",
       "  'What are the most preferred books by an IITian to crack IIT-JEE?',\n",
       "  'Are Booklet of Lakshya enough for JEE mains preparations?',\n",
       "  'Are the books of JE (cargo) and JE (commercial) for Airport authority of India (AAI) exam available in the market? Please help?',\n",
       "  'What are the best books for both IIT JEE Mains & Advanced?',\n",
       "  'Where can I buy IIT-JEE correspondence coaching material from the famous Agrawal Classes?',\n",
       "  'Which book is best for organic chemistry for JEE?',\n",
       "  'Of which things should I make notes for revision during JEE preparation?',\n",
       "  'How can I prepare for the JEE Advanced ?',\n",
       "  'Which book should I study for JEE advanced mathematics?',\n",
       "  'What is the best book for physical chemistry for JEE?',\n",
       "  \"I've not yet started studying 11th syllabus of JEE Main, is this book good for the same?\",\n",
       "  'Which book is the best for organic chemistry for JEE?',\n",
       "  'Which is one of the best best mathematics book for IIT JEE preparation?']}"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_pos = 10\n",
    "diff_pos = 20\n",
    "sentences = [\n",
    "    dataset[\"train\"][initial_pos][\"set\"][\"pos\"][0],\n",
    "    dataset[\"train\"][initial_pos][\"set\"][\"query\"],\n",
    "    *dataset[\"train\"][diff_pos][\"set\"][\"neg\"][:10],\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(\n",
    "    sentences,\n",
    "    # precision=\"binary\",\n",
    "    # normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "hash_table = lsh_hashing(embeddings, k=10)\n",
    "\n",
    "# –ü–µ—á–∞—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ö–µ—à-—Ç–∞–±–ª–∏—Ü—ã\n",
    "for hash_key, vector_indices in hash_table.items():\n",
    "    print(f\"–ë–∞–∫–µ—Ç —Å —Ö–µ—à-–∫–ª—é—á–æ–º {hash_key}: {vector_indices}\")\n",
    "    for i in vector_indices:\n",
    "        if i == 0:\n",
    "            print(\"POSITIVE\")\n",
    "        if i == 1:\n",
    "            print(\"QUERY\")\n",
    "        print(sentences[i])\n",
    "    print(\"===\")\n",
    "    print(\"===\")\n",
    "\n",
    "dataset[\"train\"][diff_pos][\"set\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–µ—Ç —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–º–µ—Å—Ç–æ LSH –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –∏–∑ faiss? https://github.com/facebookresearch/faiss/blob/main/tutorial/python/2-IVFFlat.py\n",
    "\n",
    "- —Ç–∏–ø–∞ IVFADC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- —ç—Ç–æ—Ç –∫–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö, –∞ –ø–æ—Ç–æ–º –±—ã—Å—Ç—Ä–æ –ø–æ–ª—É—á–∞—Ç—å –∏–Ω–¥–µ–∫—Å –Ω—É–∂–Ω–æ–≥–æ –Ω–∞–º –≤–µ–∫—Ç–æ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 1000 points to 100 centroids: please provide at least 3900 training points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[22]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "d = 128  # Dimensionality of the vectors\n",
    "nlist = 100  # Number of Voronoi cells (buckets)\n",
    "quantizer = faiss.IndexFlatL2(d)  # Replace with other quantizers as needed\n",
    "\n",
    "#  Using a GPU index.\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)\n",
    "\n",
    "# Generate some random data for training\n",
    "xt = np.random.random((1000, d)).astype(\"float32\")\n",
    "\n",
    "# Train the index\n",
    "index.train(xt)\n",
    "\n",
    "# Add some vectors to the index (training data)\n",
    "index.add(xt)\n",
    "\n",
    "# Create a query vector\n",
    "xq = np.random.random((1, d)).astype(\"float32\")\n",
    "\n",
    "\n",
    "quantizer.assign(xq, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Spacy for sentence tokenization(—Å–ø–æ—Ä–Ω–æ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"Open-Orca/OpenOrca\")\n",
    "# dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep Learning: A Deep Dive into the Engine of Modern AI\\n\\nDeep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?\\n\\nAt its core, deep learning relies on artificial neural networks with multiple layers (hence the \"deep\"). These networks are inspired by the structure and function of the human brain, attempting to mimic the interconnected web of neurons that allows us to learn and process information. Unlike traditional machine learning algorithms that often require hand-engineered features, deep learning excels at learning these features directly from raw data. This ability to automatically extract complex patterns is a key differentiator and a major contributor to its superior performance in many tasks.\\n\\nUnderstanding the Building Blocks: Artificial Neural Networks\\n\\nBefore diving into the \"deep\" part, let\\'s establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include:\\n\\nInput Layer: Receives the raw data as input. The number of neurons in this layer corresponds to the number of features in the data.\\n\\nHidden Layers: Perform the actual processing of the input data. Deep learning is characterized by having multiple hidden layers, allowing for the creation of complex and hierarchical representations.\\n\\nOutput Layer: Produces the final prediction or classification based on the processed information. The number of neurons in this layer corresponds to the number of classes or the range of the prediction.\\n\\nEach connection between neurons has an associated weight, which represents the strength of the connection. When data flows through the network, each neuron receives inputs from the neurons in the previous layer, multiplies those inputs by their corresponding weights, sums the weighted inputs, and then applies an activation function.\\n\\nActivation Functions: Introducing Non-Linearity\\n\\nActivation functions are crucial for introducing non-linearity into the network. Without them, the entire network would simply be a linear combination of its inputs, severely limiting its ability to learn complex patterns. Common activation functions include:\\n\\nSigmoid: Outputs a value between 0 and 1, often used in the output layer for binary classification.\\n\\nReLU (Rectified Linear Unit): Outputs the input directly if it\\'s positive, otherwise outputs 0. It\\'s computationally efficient and commonly used in hidden layers.\\n\\nTanh (Hyperbolic Tangent): Outputs a value between -1 and 1, similar to sigmoid but centered around 0.\\n\\nThe choice of activation function can significantly impact the performance of the network, and experimentation is often required to find the optimal one for a given task.\\n\\nLearning and Optimization: Training the Network\\n\\nThe learning process in deep learning involves adjusting the weights and biases of the network to minimize the difference between the network\\'s predictions and the actual target values. This is achieved through a process called backpropagation.\\n\\nBackpropagation involves the following steps:\\n\\nForward Pass: The input data is fed forward through the network to generate a prediction.\\n\\nLoss Calculation: A loss function (e.g., mean squared error for regression or cross-entropy for classification) measures the difference between the prediction and the actual target value.\\n\\nBackward Pass: The gradient of the loss function with respect to each weight and bias in the network is calculated using the chain rule of calculus. This gradient indicates the direction in which each weight and bias should be adjusted to reduce the loss.\\n\\nWeight Update: An optimization algorithm (e.g., stochastic gradient descent (SGD), Adam, RMSprop) uses the calculated gradients to update the weights and biases of the network. The learning rate, a hyperparameter that controls the size of the weight updates, plays a crucial role in the speed and stability of the training process.\\n\\nThis process is repeated iteratively over many batches of training data until the network converges to a point where it can accurately predict the target values.\\n\\nWhy Deep Learning is Deep: The Power of Multiple Layers\\n\\nThe \"deep\" in deep learning refers to the presence of multiple hidden layers in the neural network. This depth allows the network to learn hierarchical representations of the data, with each layer learning increasingly complex features.\\n\\nEarly Layers: Typically learn low-level features, such as edges and corners in images or phonemes in audio.\\n\\nLater Layers: Combine these low-level features to learn higher-level concepts, such as objects in images or words in speech.\\n\\nThis hierarchical feature extraction is what allows deep learning models to outperform traditional machine learning algorithms in tasks involving complex and unstructured data. The ability to automatically learn these features eliminates the need for manual feature engineering, which can be a time-consuming and expertise-intensive process.\\n\\nDifferent Architectures for Different Tasks: A Deep Learning Zoo\\n\\nOver the years, researchers have developed various deep learning architectures tailored to specific tasks and data types. Some of the most popular and impactful architectures include:\\n\\nConvolutional Neural Networks (CNNs): Designed for processing images and videos. They use convolutional layers to extract spatial features and pooling layers to reduce the dimensionality of the data. CNNs have achieved remarkable success in image classification, object detection, and image segmentation.\\n\\nRecurrent Neural Networks (RNNs): Designed for processing sequential data, such as text and time series. They have recurrent connections that allow them to maintain a hidden state, which captures information about the past. RNNs are commonly used in natural language processing tasks, such as machine translation and text generation.\\n\\nLong Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs): Special types of RNNs that are better at handling long-range dependencies in sequential data. They use memory cells and gating mechanisms to selectively remember and forget information.\\n\\nTransformers: A more recent architecture that has revolutionized natural language processing. They rely on self-attention mechanisms to learn relationships between different parts of the input sequence. Transformers have achieved state-of-the-art results in a wide range of NLP tasks, including machine translation, text summarization, and question answering.\\n\\nGenerative Adversarial Networks (GANs): Composed of two networks, a generator and a discriminator, that are trained against each other. The generator tries to create realistic synthetic data, while the discriminator tries to distinguish between the real data and the generated data. GANs are used for image generation, image editing, and other creative tasks.\\n\\nAutoencoders: Designed to learn compressed representations of the input data. They consist of an encoder that maps the input to a lower-dimensional latent space and a decoder that reconstructs the input from the latent space. Autoencoders can be used for dimensionality reduction, anomaly detection, and image denoising.\\n\\nChallenges and Considerations: The Dark Side of Deep Learning\\n\\nWhile deep learning has achieved impressive results, it also faces several challenges and limitations:\\n\\nData Dependency: Deep learning models require large amounts of labeled data to train effectively. The lack of sufficient data can lead to overfitting, where the model performs well on the training data but poorly on unseen data.\\n\\nComputational Cost: Training deep learning models can be computationally expensive, requiring powerful hardware and significant training time.\\n\\nBlack Box Nature: Deep learning models are often considered \"black boxes\" because it\\'s difficult to understand why they make specific predictions. This lack of interpretability can be a concern in critical applications, such as healthcare and finance.\\n\\nAdversarial Attacks: Deep learning models can be vulnerable to adversarial attacks, where small, carefully crafted perturbations to the input data can cause the model to make incorrect predictions.\\n\\nBias and Fairness: Deep learning models can inherit biases from the training data, leading to unfair or discriminatory outcomes. It\\'s crucial to address these biases to ensure that deep learning models are used responsibly.\\n\\nThe Future of Deep Learning: Continuing the Revolution\\n\\nDespite these challenges, deep learning continues to evolve and advance at a rapid pace. Ongoing research is focused on:\\n\\nDeveloping more efficient and robust architectures.\\n\\nImproving the interpretability and explainability of deep learning models.\\n\\nAddressing the challenges of data dependency and bias.\\n\\nExploring new applications of deep learning in various fields.\\n\\nDeveloping more efficient training techniques such as federated learning and self-supervised learning.\\n\\nDeep learning has already had a profound impact on artificial intelligence, and its future is bright. As research continues to advance and new applications are discovered, deep learning will undoubtedly play an even more important role in shaping the world around us. From personalized recommendations to groundbreaking scientific discoveries, the potential of deep learning is vast and still largely untapped. Understanding its principles, limitations, and future directions is essential for anyone seeking to navigate the increasingly AI-driven world.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text = open('./long_text_example.txt').read()\n",
    "long_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "[2025-02-25 15:04:25,573] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 15:04:26 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.546 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name=\"unsloth/Llama-3.2-3B-Instruct\",  # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    model_name=\"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\",  # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    # fix_tokenizer=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function unsloth.models.llama._wrap_fast_inference.<locals>._fast_generate(*args, **kwargs)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model).generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SamplingParams\n\u001b[1;32m     12\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m     13\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m     14\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m     15\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# sampling_params=sampling_params,\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# max_tokens=1024,\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/unsloth/models/llama.py:1596\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;66;03m# Set pad token\u001b[39;00m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m \n\u001b[1;32m   1594\u001b[0m \u001b[38;5;66;03m# Autocasted\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m-> 1596\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;66;03m# Revert\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m \n\u001b[1;32m   1602\u001b[0m \u001b[38;5;66;03m# Unset a flag for generation!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:2001\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1998\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   1999\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   2000\u001b[0m )\n\u001b[0;32m-> 2001\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2003\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "text = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"How many r's are in strawberry?\"},\n",
    "    ],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "output = (\n",
    "    model.generate(\n",
    "        text,\n",
    "        # sampling_params=sampling_params,\n",
    "        lora_request=None,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        # max_tokens=1024,\n",
    "    )[0]\n",
    "    .outputs[0]\n",
    "    .text\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
