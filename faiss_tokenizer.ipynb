{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"The weather is lovely today\",\n",
    "    \"The weather is lovely\",\n",
    "    \"The weather is good today.\",\n",
    "    \"The weather is bad today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "    \"He drove to the stadium\",\n",
    "    \"He drove to the stadium in the night\",\n",
    "    \"He loves stadiums\",\n",
    "    \"I love cats\",\n",
    "    \"I love dogs\",\n",
    "    \"He is programming on python.\",\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(\n",
    "    sentences,\n",
    "    # precision=\"binary\",\n",
    "    normalize_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 1000 points to 100 centroids: please provide at least 3900 training points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "d = 128  # Dimensionality of the vectors\n",
    "nlist = 100  # Number of Voronoi cells (buckets)\n",
    "quantizer = faiss.IndexFlatL2(d)  # Replace with other quantizers as needed\n",
    "\n",
    "#  Using a GPU index.\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)\n",
    "\n",
    "# Generate some random data for training\n",
    "xt = np.random.random((1000, d)).astype(\"float32\")\n",
    "\n",
    "# Train the index\n",
    "index.train(xt)\n",
    "\n",
    "# Add some vectors to the index (training data)\n",
    "index.add(xt)\n",
    "\n",
    "# Create a query vector\n",
    "xq = np.random.random((1, d)).astype(\"float32\")\n",
    "\n",
    "\n",
    "quantizer.assign(xq, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faiss+sentence transformer+tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep Learning: A Deep Dive into the Engine of Modern AI\\n\\nDeep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?\\n\\nAt its core, deep learning relies on artificial neural networks with multiple layers (hence the \"deep\"). These networks are inspired by the structure and function of the human brain, attempting to mimic the interconnected web of neurons that allows us to learn and process information. Unlike traditional machine learning algorithms that often require hand-engineered features, deep learning excels at learning these features directly from raw data. This ability to automatically extract complex patterns is a key differentiator and a major contributor to its superior performance in many tasks.\\n\\nUnderstanding the Building Blocks: Artificial Neural Networks\\n\\nBefore diving into the \"deep\" part, let\\'s establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include:\\n\\nInput Layer: Receives the raw data as input. The number of neurons in this layer corresponds to the number of features in the data.\\n\\nHidden Layers: Perform the actual processing of the input data. Deep learning is characterized by having multiple hidden layers, allowing for the creation of complex and hierarchical representations.\\n\\nOutput Layer: Produces the final prediction or classification based on the processed information. The number of neurons in this layer corresponds to the number of classes or the range of the prediction.\\n\\nEach connection between neurons has an associated weight, which represents the strength of the connection. When data flows through the network, each neuron receives inputs from the neurons in the previous layer, multiplies those inputs by their corresponding weights, sums the weighted inputs, and then applies an activation function.\\n\\nActivation Functions: Introducing Non-Linearity\\n\\nActivation functions are crucial for introducing non-linearity into the network. Without them, the entire network would simply be a linear combination of its inputs, severely limiting its ability to learn complex patterns. Common activation functions include:\\n\\nSigmoid: Outputs a value between 0 and 1, often used in the output layer for binary classification.\\n\\nReLU (Rectified Linear Unit): Outputs the input directly if it\\'s positive, otherwise outputs 0. It\\'s computationally efficient and commonly used in hidden layers.\\n\\nTanh (Hyperbolic Tangent): Outputs a value between -1 and 1, similar to sigmoid but centered around 0.\\n\\nThe choice of activation function can significantly impact the performance of the network, and experimentation is often required to find the optimal one for a given task.\\n\\nLearning and Optimization: Training the Network\\n\\nThe learning process in deep learning involves adjusting the weights and biases of the network to minimize the difference between the network\\'s predictions and the actual target values. This is achieved through a process called backpropagation.\\n\\nBackpropagation involves the following steps:\\n\\nForward Pass: The input data is fed forward through the network to generate a prediction.\\n\\nLoss Calculation: A loss function (e.g., mean squared error for regression or cross-entropy for classification) measures the difference between the prediction and the actual target value.\\n\\nBackward Pass: The gradient of the loss function with respect to each weight and bias in the network is calculated using the chain rule of calculus. This gradient indicates the direction in which each weight and bias should be adjusted to reduce the loss.\\n\\nWeight Update: An optimization algorithm (e.g., stochastic gradient descent (SGD), Adam, RMSprop) uses the calculated gradients to update the weights and biases of the network. The learning rate, a hyperparameter that controls the size of the weight updates, plays a crucial role in the speed and stability of the training process.\\n\\nThis process is repeated iteratively over many batches of training data until the network converges to a point where it can accurately predict the target values.\\n\\nWhy Deep Learning is Deep: The Power of Multiple Layers\\n\\nThe \"deep\" in deep learning refers to the presence of multiple hidden layers in the neural network. This depth allows the network to learn hierarchical representations of the data, with each layer learning increasingly complex features.\\n\\nEarly Layers: Typically learn low-level features, such as edges and corners in images or phonemes in audio.\\n\\nLater Layers: Combine these low-level features to learn higher-level concepts, such as objects in images or words in speech.\\n\\nThis hierarchical feature extraction is what allows deep learning models to outperform traditional machine learning algorithms in tasks involving complex and unstructured data. The ability to automatically learn these features eliminates the need for manual feature engineering, which can be a time-consuming and expertise-intensive process.\\n\\nDifferent Architectures for Different Tasks: A Deep Learning Zoo\\n\\nOver the years, researchers have developed various deep learning architectures tailored to specific tasks and data types. Some of the most popular and impactful architectures include:\\n\\nConvolutional Neural Networks (CNNs): Designed for processing images and videos. They use convolutional layers to extract spatial features and pooling layers to reduce the dimensionality of the data. CNNs have achieved remarkable success in image classification, object detection, and image segmentation.\\n\\nRecurrent Neural Networks (RNNs): Designed for processing sequential data, such as text and time series. They have recurrent connections that allow them to maintain a hidden state, which captures information about the past. RNNs are commonly used in natural language processing tasks, such as machine translation and text generation.\\n\\nLong Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs): Special types of RNNs that are better at handling long-range dependencies in sequential data. They use memory cells and gating mechanisms to selectively remember and forget information.\\n\\nTransformers: A more recent architecture that has revolutionized natural language processing. They rely on self-attention mechanisms to learn relationships between different parts of the input sequence. Transformers have achieved state-of-the-art results in a wide range of NLP tasks, including machine translation, text summarization, and question answering.\\n\\nGenerative Adversarial Networks (GANs): Composed of two networks, a generator and a discriminator, that are trained against each other. The generator tries to create realistic synthetic data, while the discriminator tries to distinguish between the real data and the generated data. GANs are used for image generation, image editing, and other creative tasks.\\n\\nAutoencoders: Designed to learn compressed representations of the input data. They consist of an encoder that maps the input to a lower-dimensional latent space and a decoder that reconstructs the input from the latent space. Autoencoders can be used for dimensionality reduction, anomaly detection, and image denoising.\\n\\nChallenges and Considerations: The Dark Side of Deep Learning\\n\\nWhile deep learning has achieved impressive results, it also faces several challenges and limitations:\\n\\nData Dependency: Deep learning models require large amounts of labeled data to train effectively. The lack of sufficient data can lead to overfitting, where the model performs well on the training data but poorly on unseen data.\\n\\nComputational Cost: Training deep learning models can be computationally expensive, requiring powerful hardware and significant training time.\\n\\nBlack Box Nature: Deep learning models are often considered \"black boxes\" because it\\'s difficult to understand why they make specific predictions. This lack of interpretability can be a concern in critical applications, such as healthcare and finance.\\n\\nAdversarial Attacks: Deep learning models can be vulnerable to adversarial attacks, where small, carefully crafted perturbations to the input data can cause the model to make incorrect predictions.\\n\\nBias and Fairness: Deep learning models can inherit biases from the training data, leading to unfair or discriminatory outcomes. It\\'s crucial to address these biases to ensure that deep learning models are used responsibly.\\n\\nThe Future of Deep Learning: Continuing the Revolution\\n\\nDespite these challenges, deep learning continues to evolve and advance at a rapid pace. Ongoing research is focused on:\\n\\nDeveloping more efficient and robust architectures.\\n\\nImproving the interpretability and explainability of deep learning models.\\n\\nAddressing the challenges of data dependency and bias.\\n\\nExploring new applications of deep learning in various fields.\\n\\nDeveloping more efficient training techniques such as federated learning and self-supervised learning.\\n\\nDeep learning has already had a profound impact on artificial intelligence, and its future is bright. As research continues to advance and new applications are discovered, deep learning will undoubtedly play an even more important role in shaping the world around us. From personalized recommendations to groundbreaking scientific discoveries, the potential of deep learning is vast and still largely untapped. Understanding its principles, limitations, and future directions is essential for anyone seeking to navigate the increasingly AI-driven world.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text = open('./long_text_example.txt').read()\n",
    "long_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33464, 20909, 25, 362, 18183, 88517, 1119, 279, 8200, 315]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tokens = tokenizer.encode(long_text)\n",
    "original_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import chunked\n",
    "\n",
    "chunk_size = 100\n",
    "text_chunks = [tokenizer.decode(chunk) for chunk in chunked(original_tokens, chunk_size)]\n",
    "\n",
    "embeddings = model.encode(\n",
    "    text_chunks,\n",
    "    # precision=\"binary\",\n",
    "    normalize_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = model.encode(\n",
    "    text_chunks[0],\n",
    "    # precision=\"binary\",\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape[0] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 18 points to 9 centroids: please provide at least 351 training points\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "d = embeddings[0].shape[0]  # Dimensionality of the vectors\n",
    "# Количество сжимающих токенов памяти\n",
    "nlist = embeddings.shape[0] // 2  # Number of Voronoi cells (buckets)\n",
    "quantizer = faiss.IndexFlatL2(d)  # Replace with other quantizers as needed\n",
    "\n",
    "#  Using a GPU index.\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)\n",
    "\n",
    "# Train the index\n",
    "index.train(embeddings)\n",
    "\n",
    "# Add some vectors to the index (training data)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a query vector\n",
    "xq = np.expand_dims(embeddings[0], axis=0)\n",
    "\n",
    "\n",
    "quantizer.assign(xq, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 6]]\n",
      "[[7 4]]\n",
      "[[7 4]]\n",
      "[[2 1]]\n",
      "[[2 7]]\n",
      "[[1 2]]\n",
      "[[5 1]]\n",
      "[[1 4]]\n",
      "[[4 7]]\n",
      "[[8 4]]\n",
      "[[8 7]]\n",
      "[[7 8]]\n",
      "[[3 7]]\n",
      "[[0 6]]\n",
      "[[4 6]]\n",
      "[[6 4]]\n",
      "[[6 4]]\n",
      "[[6 4]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(embeddings.shape[0]):\n",
    "    xq = np.expand_dims(embeddings[i], axis=0)\n",
    "    # print cluster\n",
    "    print(quantizer.assign(xq, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_tokenization(text):\n",
    "    emb = model.encode(\n",
    "    text,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    xq = np.expand_dims(emb, axis=0)\n",
    "    return quantizer.assign(xq, 2)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фрагмент текстовый фрагмент теперь принадлежит какому-то кластеру(индексу сжимающего токена)\n",
    "\n",
    "<!-- ![image.png](./faiss.png) -->\n",
    "<div>\n",
    "<img src=\"./faiss.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем обучающую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "memory_token_template = '<mem_tok_{num}>'\n",
    "mask_probs = [\n",
    "    0.1,\n",
    "    0.2,\n",
    "    0.5,\n",
    "    0.8,\n",
    "    0.9,\n",
    "]\n",
    "for prob in mask_probs:\n",
    "    random_mask = np.random.random(len(text_chunks))\n",
    "    mask = random_mask < prob\n",
    "    chunks_for_tokenization = np.where(mask)[0].tolist()\n",
    "    chunks_for_tokenization = set(chunks_for_tokenization)\n",
    "    train_text = \"\"\n",
    "\n",
    "    for chunk_id, text in enumerate(text_chunks):\n",
    "        if chunk_id in chunks_for_tokenization:\n",
    "            cluster_id = cluster_tokenization(text)\n",
    "            # print(cluster_id)\n",
    "            train_text += f\" {memory_token_template.format(num=cluster_id)} \"\n",
    "        else:\n",
    "            train_text += text\n",
    "\n",
    "    train_dataset.append(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning: A Deep Dive into the Engine of Modern AI\n",
      "\n",
      "Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?\n",
      "\n",
      "At its core, deep learning relies on artificial neural networks with multiple layers (hence the \"deep\"). These networks are inspired by the structure and function of the human brain, attempting to mimic the interconnected web of neurons that allows us to learn and process information. Unlike traditional machine learning algorithms that often require hand-engineered features, deep learning excels at learning these features directly from raw data. This ability to automatically extract complex patterns is a key differentiator and a major contributor to its superior performance in many tasks.\n",
      "\n",
      "Understanding the Building Blocks: Artificial Neural Networks\n",
      "\n",
      "Before diving into the \"deep\" part, let's establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include:\n",
      "\n",
      "Input Layer: Receives the raw data as input. The number of neurons in this layer corresponds to the number of features in the data.\n",
      "\n",
      "Hidden Layers: Perform the actual processing of the input data. Deep learning is characterized by having multiple hidden layers, allowing for the creation of complex and hierarchical representations.\n",
      "\n",
      "Output Layer: Produces the final prediction or classification based on the processed information. The number of neurons in this layer corresponds to the number of classes or the range of the prediction.\n",
      "\n",
      "Each connection between neurons has an associated weight, which represents the strength of the connection. When data flows through the network, each neuron receives inputs from the neurons in the previous layer, multiplies those inputs by their corresponding weights, sums the weighted inputs, and then applies an activation function.\n",
      "\n",
      "Activation Functions: Introducing Non-Linearity\n",
      "\n",
      "Activation functions are crucial for introducing non-linearity into the network. Without them, the entire network would simply be a linear combination of its inputs, severely limiting its ability to learn complex patterns. Common activation functions include:\n",
      "\n",
      "Sigmoid: Outputs a value between 0 and 1, often used in the output layer for binary classification.\n",
      "\n",
      "ReLU (Rectified Linear Unit): Outputs the input directly if it's positive, otherwise outputs 0. It's computationally efficient and commonly used in hidden layers.\n",
      "\n",
      "Tanh (Hyperbolic Tangent): Outputs a value between -1 and 1, similar <mem_tok_1>  is fed forward through the network to generate a prediction.\n",
      "\n",
      "Loss Calculation: A loss function (e.g., mean squared error for regression or cross-entropy for classification) measures the difference between the prediction and the actual target value.\n",
      "\n",
      "Backward Pass: The gradient of the loss function with respect to each weight and bias in the network is calculated using the chain rule of calculus. This gradient indicates the direction in which each weight and bias should be adjusted to reduce the loss.\n",
      "\n",
      "Weight Update: An optimization algorithm (e.g., stochastic gradient descent (SGD), Adam, RMSprop) uses the calculated gradients to update the weights and biases of the network. The learning rate, a hyperparameter that controls the size of the weight updates, plays a crucial role in the speed and stability of the training process.\n",
      "\n",
      "This process is repeated iteratively over many batches of training data until the network converges to a point where it can accurately predict the target values.\n",
      "\n",
      "Why Deep Learning is Deep: The Power of Multiple Layers\n",
      "\n",
      "The \"deep\" in deep learning refers to the presence of multiple hidden layers in the neural network. This depth allows the network to learn hierarchical representations of the data, with each layer learning increasingly complex features.\n",
      "\n",
      "Early Layers: Typically learn low-level features, such as edges and corners in images or phonemes in audio.\n",
      "\n",
      "Later Layers: Combine these low-level features to learn higher-level concepts, such as objects in images or words in speech.\n",
      "\n",
      "This hierarchical feature extraction is what allows deep learning models to outperform <mem_tok_8>  layers to extract spatial features and pooling layers to reduce the dimensionality of the data. CNNs have achieved remarkable success in image classification, object detection, and image segmentation.\n",
      "\n",
      "Recurrent Neural Networks (RNNs): Designed for processing sequential data, such as text and time series. They have recurrent connections that allow them to maintain a hidden state, which captures information about the past. RNNs are commonly used in natural language processing tasks, such as machine translation and text generation.\n",
      "\n",
      "Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs): Special types of RNNs that are better at handling long-range dependencies in sequential data. They use memory cells and gating mechanisms to selectively remember and forget information.\n",
      "\n",
      "Transformers: A more recent architecture that has revolutionized natural language processing. They rely on self-attention mechanisms to learn relationships between different parts of the input sequence. Transformers have achieved state-of-the-art results in a wide range of NLP tasks, including machine translation, text summarization, and question answering.\n",
      "\n",
      "Generative Adversarial Networks (GANs): Composed of two networks, a generator and a discriminator, that are trained against each other. The generator tries to create realistic synthetic data, while the discriminator tries to distinguish between the real data and the generated data. GANs are used for image generation, image editing, and other creative tasks.\n",
      "\n",
      "Autoencoders: Designed to learn compressed representations of the input data. They consist of an encoder that maps the input to a lower-dimensional latent space and a decoder that reconstructs the input from the latent space. Autoencoders can be used for dimensionality reduction, anomaly detection, and image denoising.\n",
      "\n",
      "Challenges and Considerations: The Dark Side of Deep Learning\n",
      "\n",
      "While deep learning has achieved impressive results, it also faces several challenges and limitations:\n",
      "\n",
      "Data Dependency: Deep learning models require large amounts of labeled data to train effectively. The lack of sufficient data can lead to overfitting, where the model performs well on the training data but poorly on unseen data.\n",
      "\n",
      "Computational Cost: Training deep learning models can be computationally expensive, requiring powerful hardware and significant training time.\n",
      "\n",
      "Black Box Nature: Deep learning models are often considered \"black boxes\" because it's difficult to understand why they make specific predictions. This lack of interpretability can be a concern in critical applications, such as healthcare and finance.\n",
      "\n",
      "Adversarial Attacks: Deep learning models can be vulnerable to adversarial attacks, where small, carefully crafted perturbations to the input data can cause the model to make incorrect predictions.\n",
      "\n",
      "Bias and Fairness: Deep learning models can inherit biases from the training data, leading to unfair or discriminatory outcomes. It's crucial to address these biases to ensure that deep learning models are used responsibly.\n",
      "\n",
      "The Future of Deep Learning: Continuing the Revolution\n",
      "\n",
      "Despite these challenges, deep learning continues to evolve and advance at a rapid pace. Ongoing research is focused on:\n",
      "\n",
      "Developing more efficient and robust architectures.\n",
      "\n",
      "Improving <mem_tok_6>  <mem_tok_6> \n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning: A Deep Dive into the Engine of Modern AI\n",
      "\n",
      "Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?\n",
      "\n",
      "At its core, deep learning relies on artificial neural networks with multiple layers (hence the \"deep\"). These networks are inspired by the structure and function of the human brain, attempting to mimic the interconnected web of neurons that allows us to learn and process information. Unlike traditional machine learning algorithms that often require hand-engineered features, deep learning excels at learning these features directly from raw data. This ability to automatically extract complex patterns is a key differentiator and a major contributor to its superior performance in many tasks.\n",
      "\n",
      "Understanding the Building Blocks: Artificial Neural Networks\n",
      "\n",
      "Before diving into the \"deep\" part, let's establish a foundation with the basics of artificial neural networks (ANNs). <mem_tok_7>  <mem_tok_2>  <mem_tok_2>  <mem_tok_1>  <mem_tok_5>  <mem_tok_1>  <mem_tok_4>  traditional machine learning algorithms in tasks involving complex and unstructured data. The ability to automatically learn these features eliminates the need for manual feature engineering, which can be a time-consuming and expertise-intensive process.\n",
      "\n",
      "Different Architectures for Different Tasks: A Deep Learning Zoo\n",
      "\n",
      "Over the years, researchers have developed various deep learning architectures tailored to specific tasks and data types. Some of the most popular and impactful architectures include:\n",
      "\n",
      "Convolutional Neural Networks (CNNs): Designed for processing images and videos. They use convolutional <mem_tok_8>  <mem_tok_7>  <mem_tok_3>  <mem_tok_0>  <mem_tok_4>  crafted perturbations to the input data can cause the model to make incorrect predictions.\n",
      "\n",
      "Bias and Fairness: Deep learning models can inherit biases from the training data, leading to unfair or discriminatory outcomes. It's crucial to address these biases to ensure that deep learning models are used responsibly.\n",
      "\n",
      "The Future of Deep Learning: Continuing the Revolution\n",
      "\n",
      "Despite these challenges, deep learning continues to evolve and advance at a rapid pace. Ongoing research is focused on:\n",
      "\n",
      "Developing more efficient and robust architectures.\n",
      "\n",
      "Improving the interpretability and explainability of deep learning models.\n",
      "\n",
      "Addressing the challenges of data dependency and bias.\n",
      "\n",
      "Exploring new applications of deep learning in various fields.\n",
      "\n",
      "Developing more efficient training techniques such as federated learning and self-supervised learning.\n",
      "\n",
      "Deep learning has already had a profound impact on artificial intelligence, and its future is bright. As research continues to advance and new applications are discovered, deep learning will undoubtedly play an even more important role in shaping the world around us. From personalized recommendations to groundbreaking <mem_tok_6> \n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = [memory_token_template.format(num=i) for i in range(nlist)]\n",
    "tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <mem_tok_4>  <mem_tok_7>  An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include:\n",
      "\n",
      "Input Layer: Receives the raw data as input. The number of neurons in this layer corresponds to the number of features in the data.\n",
      "\n",
      "Hidden Layers: Perform the actual processing of the input data. Deep learning is characterized by having multiple hidden layers, allowing for the creation of complex and hierarchical representations.\n",
      "\n",
      "Output Layer: Produces the final prediction or classification based on the processed information. The number of neurons in this layer corresponds to the number of classes or the range of the prediction.\n",
      "\n",
      "Each connection between neurons has an associated weight, which represents the strength of the connection. When data flows through the network, each neuron receives inputs from the neurons in the previous layer, multiplies those inputs by their corresponding weights, sums the weighted inputs, and then applies an activation function.\n",
      "\n",
      "Activation Functions: Introducing Non-Linearity\n",
      "\n",
      "Activation functions are crucial for introducing non-linearity into the network. Without them, the entire network <mem_tok_2>  <mem_tok_1>  <mem_tok_5>  <mem_tok_1>  <mem_tok_4>  traditional machine learning algorithms in tasks involving complex and unstructured data. The ability to automatically learn these features eliminates the need for manual feature engineering, which can be a time-consuming and expertise-intensive process.\n",
      "\n",
      "Different Architectures for Different Tasks: A Deep Learning Zoo\n",
      "\n",
      "Over the years, researchers have developed various deep learning architectures tailored to specific tasks and data types. Some of the most popular and impactful architectures include:\n",
      "\n",
      "Convolutional Neural Networks (CNNs): Designed for processing images and videos. They use convolutional <mem_tok_8>  <mem_tok_7>  <mem_tok_3>  <mem_tok_0>  <mem_tok_4>  <mem_tok_6>  <mem_tok_6>  <mem_tok_6> \n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <mem_tok_4>  <mem_tok_7>  An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include:\n",
      "\n",
      "Input Layer: Receives the raw data as input. The number of neurons in this layer corresponds to the number of features in the data.\n",
      "\n",
      "Hidden Layers: Perform the actual processing of the input data. Deep learning is characterized by having multiple hidden layers, allowing for the creation of complex and hierarchical representations.\n",
      "\n",
      "Output Layer: Produces the final prediction or classification based on the processed information. The number of neurons in this layer corresponds to the number of classes or the range of the prediction.\n",
      "\n",
      "Each connection between neurons has an associated weight, which represents the strength of the connection. When data flows through the network, each neuron receives inputs from the neurons in the previous layer, multiplies those inputs by their corresponding weights, sums the weighted inputs, and then applies an activation function.\n",
      "\n",
      "Activation Functions: Introducing Non-Linearity\n",
      "\n",
      "Activation functions are crucial for introducing non-linearity into the network. Without them, the entire network <mem_tok_2>  <mem_tok_1>  <mem_tok_5>  <mem_tok_1>  <mem_tok_4>  traditional machine learning algorithms in tasks involving complex and unstructured data. The ability to automatically learn these features eliminates the need for manual feature engineering, which can be a time-consuming and expertise-intensive process.\n",
      "\n",
      "Different Architectures for Different Tasks: A Deep Learning Zoo\n",
      "\n",
      "Over the years, researchers have developed various deep learning architectures tailored to specific tasks and data types. Some of the most popular and impactful architectures include:\n",
      "\n",
      "Convolutional Neural Networks (CNNs): Designed for processing images and videos. They use convolutional <mem_tok_8>  <mem_tok_7>  <mem_tok_3>  <mem_tok_0>  <mem_tok_4>  <mem_tok_6>  <mem_tok_6>  <mem_tok_6> \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(train_dataset[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[220, 151669, 256, 151672, 256]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\" <mem_tok_4>  <mem_tok_7>  \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
