{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A large language model is a type of artificial intelligence that can generate human-like text, including written and spoken language. These models use massive amounts of data and complex algorithms to understand the context and meaning behind natural language input. Large language models have been trained on vast datasets such as Wikipedia, books, articles, and other texts, allowing them to process and generate coherent and meaningful responses. Some examples of popular large language models include GPT, BERT, and Qwen.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-1.5B-Instruct\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model..1\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 896])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    **model_inputs,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30, 896])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_2 = model(\n",
    "    inputs_embeds=torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    attention_mask=model_inputs[\"attention_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_2.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_2 = model(\n",
    "    inputs_embeds=torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    attention_mask=model_inputs[\"attention_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 3, 896])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1.hidden_states[-1].reshape(1, 10, 3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(\n",
    "#     inputs_embeds=model_output_1.hidden_states[-1].reshape(1, 10, 3, -1),\n",
    "#     attention_mask=model_inputs[\"attention_mask\"],\n",
    "#     output_hidden_states=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 1)"
     ]
    }
   ],
   "source": [
    "torch.tensor(\n",
    "    [\n",
    "        [1, 2],\n",
    "        [\n",
    "            1,\n",
    "        ],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 896])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Qwen2ModelEmbedPooler(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds, attention_mask, window_size=3):\n",
    "        # разбиваем входящие эмбединги на бакеты и усредняем их\n",
    "        sum_mask = attention_mask.reshape(\n",
    "            attention_mask.shape[0],\n",
    "            window_size,\n",
    "            attention_mask.shape[1] // window_size,\n",
    "            -1,\n",
    "        ).sum(1)\n",
    "        embeds_sum = input_embeds.reshape(\n",
    "            attention_mask.shape[0],\n",
    "            window_size,\n",
    "            attention_mask.shape[1] // window_size,\n",
    "            -1,\n",
    "        ).sum(1)\n",
    "        return embeds_sum / sum_mask\n",
    "\n",
    "\n",
    "embed_pooler = Qwen2ModelEmbedPooler.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "result = embed_pooler(\n",
    "    # model_output_1.hidden_states[-1],\n",
    "    torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    torch.cat(\n",
    "        [\n",
    "            model_inputs[\"attention_mask\"],\n",
    "            model_inputs[\"attention_mask\"],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    ")\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3, 4, 5, 6]).reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from transformers.cache_utils import (\n",
    "    Cache,\n",
    "    DynamicCache,\n",
    "    SlidingWindowCache,\n",
    "    StaticCache,\n",
    ")\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.utils import LossKwargs\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "\n",
    "\n",
    "class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n",
    "\n",
    "\n",
    "class Qwen2ForCausalEmbedModeling(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = torch.nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.embed_pooler = Qwen2ModelEmbedPooler.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        # pooled_mask: Optional[torch.Tensor] = None,\n",
    "        **kwargs: Unpack[KwargsForCausalLM],\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        inputs_embeds_tokens = self.model.embed_tokens(input_ids)\n",
    "        # if pixel_values is not None:\n",
    "        #     pixel_values = pixel_values.type(self.visual.get_dtype())\n",
    "        #     image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "        #     n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n",
    "        #     n_image_features = image_embeds.shape[0]\n",
    "        #     if n_image_tokens != n_image_features:\n",
    "        #         raise ValueError(\n",
    "        #             f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "        #         )\n",
    "        #     image_mask = (\n",
    "        #         (input_ids == self.config.image_token_id)\n",
    "        #         .unsqueeze(-1)\n",
    "        #         .expand_as(inputs_embeds)\n",
    "        #         .to(inputs_embeds.device)\n",
    "        #     )\n",
    "        #     image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "        #     inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "        window_size = torch.tensor(\n",
    "            [\n",
    "                [4],\n",
    "                [3],\n",
    "            ]\n",
    "        ).cuda()\n",
    "        tokens_amount = torch.tensor(\n",
    "            [\n",
    "                [2],\n",
    "                [4],\n",
    "            ]\n",
    "        ).cuda()\n",
    "        lengths = window_size * tokens_amount\n",
    "        # max_len = lengths.max()\n",
    "        max_len = inputs_embeds_tokens.shape[1]\n",
    "        batch_size = window_size.shape[0]\n",
    "        pooled_mask = (\n",
    "            torch.arange(max_len, device=device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(batch_size, max_len)\n",
    "        )\n",
    "        pooled_mask = (lengths >= pooled_mask).to(torch.long)\n",
    "        pooled_embeds = inputs_embeds * pooled_mask.to(inputs_embeds.dtype)\n",
    "        pooled_embeds = self.embed_pooler(pooled_embeds, pooled_mask)\n",
    "        embed_mask = (\n",
    "            (input_ids == self.config.image_token_id)\n",
    "            .unsqueeze(-1)\n",
    "            .expand_as(inputs_embeds)\n",
    "            .to(inputs_embeds.device)\n",
    "        )\n",
    "        inputs_embeds = inputs_embeds.masked_scatter(embed_mask, pooled_embeds)\n",
    "\n",
    "        # Из-за смешанной структуры, мы будем всегда подавать только эмбединги\n",
    "        # Идея позаимствована из qwen2vl\n",
    "        outputs = self.model(\n",
    "            input_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = (\n",
    "            slice(-logits_to_keep, None)\n",
    "            if isinstance(logits_to_keep, int)\n",
    "            else logits_to_keep\n",
    "        )\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(\n",
    "                logits=logits,\n",
    "                labels=labels,\n",
    "                vocab_size=self.config.vocab_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151645]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|im_end|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = torch.tensor(\n",
    "    [\n",
    "        [4],\n",
    "        [6],\n",
    "    ]\n",
    ").cuda()\n",
    "tokens_amount = torch.tensor(\n",
    "    [\n",
    "        [2],\n",
    "        [2],\n",
    "    ]\n",
    ").cuda()\n",
    "lengths = window_size * tokens_amount\n",
    "max_len = lengths.max()\n",
    "batch_size = window_size.shape[0]\n",
    "pooled_mask = (\n",
    "    torch.arange(max_len, device=device).unsqueeze(0).expand(batch_size, max_len)\n",
    ")\n",
    "pooled_mask = (lengths >= pooled_mask).to(torch.long)\n",
    "pooled_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8],\n",
       "        [12]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
