{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings().weight.dtype, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A large language model is a type of artificial intelligence (AI) system that can generate human-like text, including written and spoken words. These models are typically trained on large amounts of data using techniques such as deep learning and natural language processing. Large language models have been used in a wide range of applications, from translation and summarization to chatbots and virtual assistants. Some examples of popular large language models include GPT-3 and Qwen.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model..1\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 30, 896]), torch.bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    **model_inputs,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.hidden_states[-1].shape, model_output_1.hidden_states[-1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30, 896])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_2 = model(\n",
    "    inputs_embeds=torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    attention_mask=model_inputs[\"attention_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_2.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 896])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 896])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1.hidden_states[-1].reshape(10, 3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 30, 896]), torch.bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_2 = model(\n",
    "    inputs_embeds=torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    attention_mask=model_inputs[\"attention_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_2.hidden_states[-1].shape, model_output_2.hidden_states[-1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 896])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_3 = model(\n",
    "    inputs_embeds=model_output_1.hidden_states[-1].reshape(10, 3, -1),\n",
    "    # attention_mask=model_inputs[\"attention_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_3.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 3, 896])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1.hidden_states[-1].reshape(1, 10, 3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(\n",
    "#     inputs_embeds=model_output_1.hidden_states[-1].reshape(1, 10, 3, -1),\n",
    "#     attention_mask=model_inputs[\"attention_mask\"],\n",
    "#     output_hidden_states=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 896])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Qwen2ModelEmbedPooler(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config).cuda()\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds, attention_mask, window_size=3):\n",
    "        # разбиваем входящие эмбединги на бакеты и усредняем их\n",
    "        sum_mask = attention_mask.reshape(\n",
    "            attention_mask.shape[0],\n",
    "            window_size,\n",
    "            attention_mask.shape[1] // window_size,\n",
    "            -1,\n",
    "        ).sum(1)\n",
    "        embeds_sum = input_embeds.reshape(\n",
    "            attention_mask.shape[0],\n",
    "            window_size,\n",
    "            attention_mask.shape[1] // window_size,\n",
    "            -1,\n",
    "        ).sum(1)\n",
    "        input_embeds = embeds_sum / sum_mask\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "embed_pooler = Qwen2ModelEmbedPooler.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "result = embed_pooler(\n",
    "    # model_output_1.hidden_states[-1],\n",
    "    torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    torch.cat(\n",
    "        [\n",
    "            model_inputs[\"attention_mask\"],\n",
    "            model_inputs[\"attention_mask\"],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    ")\n",
    "result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 896])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3, 4, 5, 6]).reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from transformers.cache_utils import (\n",
    "    Cache,\n",
    "    DynamicCache,\n",
    "    SlidingWindowCache,\n",
    "    StaticCache,\n",
    ")\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.utils import LossKwargs\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "\n",
    "\n",
    "class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n",
    "\n",
    "\n",
    "class Qwen2ForCausalEmbedModeling(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = torch.nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.embed_pooler = Qwen2ModelEmbedPooler.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        # pooled_mask: Optional[torch.Tensor] = None,\n",
    "        **kwargs: Unpack[KwargsForCausalLM],\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        inputs_embeds_tokens = self.model.embed_tokens(input_ids)\n",
    "        # if pixel_values is not None:\n",
    "        #     pixel_values = pixel_values.type(self.visual.get_dtype())\n",
    "        #     image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "        #     n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n",
    "        #     n_image_features = image_embeds.shape[0]\n",
    "        #     if n_image_tokens != n_image_features:\n",
    "        #         raise ValueError(\n",
    "        #             f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "        #         )\n",
    "        #     image_mask = (\n",
    "        #         (input_ids == self.config.image_token_id)\n",
    "        #         .unsqueeze(-1)\n",
    "        #         .expand_as(inputs_embeds)\n",
    "        #         .to(inputs_embeds.device)\n",
    "        #     )\n",
    "        #     image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "        #     inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "        window_size = torch.tensor(\n",
    "            [\n",
    "                [4],\n",
    "                [3],\n",
    "            ]\n",
    "        ).cuda()\n",
    "        tokens_amount = torch.tensor(\n",
    "            [\n",
    "                [2],\n",
    "                [4],\n",
    "            ]\n",
    "        ).cuda()\n",
    "        lengths = window_size * tokens_amount\n",
    "        # max_len = lengths.max()\n",
    "        max_len = inputs_embeds_tokens.shape[1]\n",
    "        batch_size = window_size.shape[0]\n",
    "        pooled_mask = (\n",
    "            torch.arange(max_len, device=device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(batch_size, max_len)\n",
    "        )\n",
    "        pooled_mask = (lengths >= pooled_mask).to(torch.long)\n",
    "        pooled_embeds = inputs_embeds * pooled_mask.to(inputs_embeds.dtype)\n",
    "        pooled_embeds = self.embed_pooler(pooled_embeds, pooled_mask)\n",
    "        embed_mask = (\n",
    "            (input_ids == self.config.image_token_id)\n",
    "            .unsqueeze(-1)\n",
    "            .expand_as(inputs_embeds)\n",
    "            .to(inputs_embeds.device)\n",
    "        )\n",
    "        inputs_embeds = inputs_embeds.masked_scatter(embed_mask, pooled_embeds)\n",
    "\n",
    "        # Из-за смешанной структуры, мы будем всегда подавать только эмбединги\n",
    "        # Идея позаимствована из qwen2vl\n",
    "        outputs = self.model(\n",
    "            input_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = (\n",
    "            slice(-logits_to_keep, None)\n",
    "            if isinstance(logits_to_keep, int)\n",
    "            else logits_to_keep\n",
    "        )\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(\n",
    "                logits=logits,\n",
    "                labels=labels,\n",
    "                vocab_size=self.config.vocab_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование концепции qwen2vl для текстовых токенов и эмбедингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = \"Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?\"\n",
    "\n",
    "tokenizer.encode(text_example)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151655]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|image_pad|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1318, 39304]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"text_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Случай первый - мы сжимаем только текстовые токены (максимально упрощенный вариант)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|>ized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 3\n",
    "chunk_size = 4\n",
    "new_tokens = []\n",
    "original_tokens = tokenizer.encode(text_example)\n",
    "new_tokens += tokenizer.encode(\"<|object_ref_start|>\") * chunk_size\n",
    "new_tokens += original_tokens[chunk_size * window_size :]\n",
    "tokenizer.decode(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(original_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 43, 896]), torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tokens_torch = torch.tensor(\n",
    "    [\n",
    "        original_tokens,\n",
    "        original_tokens,\n",
    "    ],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "new_tokens_torch = torch.tensor(\n",
    "    [\n",
    "        new_tokens,\n",
    "        new_tokens,\n",
    "    ],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "# torch.Size([2, 51, 896])\n",
    "original_embeds = model.get_input_embeddings()(original_tokens_torch)\n",
    "# torch.Size([2, 43, 896]) 51 - 3*4 + 4\n",
    "compressed_embeds_template = model.get_input_embeddings()(new_tokens_torch)\n",
    "compressed_embeds_template.shape, compressed_embeds_template.dtype, original_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 43, 896]), torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Qwen2ModelEmbedPoolerV2(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds):\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )[0]\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.sum(1) / torch.tensor(\n",
    "            input_embeds.shape[1],\n",
    "            device=input_embeds.device,\n",
    "        )\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.unsqueeze(1)\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "embed_pooler_v2 = Qwen2ModelEmbedPoolerV2.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# torch.Size([8, 3, 896])\n",
    "compressed_embeds = original_embeds[:, : chunk_size * window_size].reshape(\n",
    "    chunk_size * original_embeds.shape[0],\n",
    "    window_size,\n",
    "    -1,\n",
    ")\n",
    "compressed_embeds.shape\n",
    "# torch.Size([8, 1, 896])\n",
    "pooled_embeds = embed_pooler_v2(compressed_embeds)\n",
    "# torch.Size([2, 4, 896])\n",
    "pooled_embeds = pooled_embeds.reshape(\n",
    "    original_embeds.shape[0],\n",
    "    chunk_size,\n",
    "    -1,\n",
    ")\n",
    "# torch.Size([2, 43, 896])\n",
    "compressed_embeds_template[:, :chunk_size] = pooled_embeds\n",
    "compressed_embeds_template.shape, compressed_embeds_template.dtype, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_pooler_v2.model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  1506,   279, 18414,   315, 20443, 11229,\n",
       "           304,  3213,  1635,    13,  5542,   656, 59711,  9331,   311, 34549,\n",
       "         15712,    11,  1181,  8357,   525, 10454, 14756, 70767,    13,  1988,\n",
       "          1128,  6896,   374,  5538,  6832,    30,  1597,  1128,  3643,   432,\n",
       "           773,  7988,    30],\n",
       "        [ -100,  -100,  -100,  -100,  1506,   279, 18414,   315, 20443, 11229,\n",
       "           304,  3213,  1635,    13,  5542,   656, 59711,  9331,   311, 34549,\n",
       "         15712,    11,  1181,  8357,   525, 10454, 14756, 70767,    13,  1988,\n",
       "          1128,  6896,   374,  5538,  6832,    30,  1597,  1128,  3643,   432,\n",
       "           773,  7988,    30]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = new_tokens_torch.clone()\n",
    "text_token_id = tokenizer.encode(\"<|object_ref_start|>\")[0]\n",
    "labels[labels == text_token_id] = -100\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7288, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    inputs_embeds=compressed_embeds_template,\n",
    "    labels=labels,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Усложненная версия токенизации текста. Сжатые токены возникают между обычным тектом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входная последовательность может быть 5 видов.\n",
    "\n",
    "- когда у нас на входе только текст, который мы просто моделируем \n",
    "- когда на входе текст, но мы хотим сжать его некоторые части\n",
    "- когда на входе текст и эмбединги с последнего слоя, где мы хотим сжать только части текста\n",
    "- когда на входе текст и эмбединги с последнего слоя, где мы хотим сжать, часть текста и эмбедингов, совместно (токены переводим в эмбединги и сжимаем вместе с hidden states)\n",
    "- когда на входе текст и эмбединги с последнего слоя, где мы хотим сжать, только эмбединги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы решаем какие токены хотим сжать?\n",
    "- никак. Просто говорим сожми с такого по такой.\n",
    "- это норм только на этапе обучения, так как мы можем перебрать все комбинации.\n",
    "- не норм на этапе инференса. например мы можем сжимать после каждых сгенеренных 10 токенов или 100, 1000. а какое окно контекста? 3, 10, 100? А что если менять стратегию. Сначала мы сжимали с окном 5 токенов, потом 20?\n",
    "- Кажется что эти гиперпараметры можно найти простым перебором на валидации. Однако перебор стратегии уже не кажется таким очевидным. Напрашивается RL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11, 702, 13791, 1506, 279, 18414, 315, 20443, 11229, 304, 3213, 1635, 13, 5542, 656, 59711, 9331, 311, 34549, 15712, 11, 1181, 8357, 525, 10454, 14756, 70767, 13, 1988, 1128, 6896, 374, 5538, 6832, 30, 1597, 1128, 3643, 432, 773, 7988, 30, 220, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643], [10227, 42415, 1119, 279, 330, 32880, 1, 949, 11, 1077, 594, 5695, 264, 16266, 448, 279, 31774, 315, 20443, 29728, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = [\n",
    "    \"Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful? \",\n",
    "    \"\"\"Before diving into the \"deep\" part, let's establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include:\"\"\",\n",
    "]\n",
    "\n",
    "tokenizer.batch_encode_plus(\n",
    "    text_example,\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 122])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(\n",
    "    tokenizer.batch_encode_plus(\n",
    "        text_example,\n",
    "        padding=True,\n",
    "    )[\"input_ids\"]\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(151643)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11, 702, 13791, 1506, 279, 18414, 315, 20443, 11229, 304, 3213, 1635, 13, 5542, 656, 59711, 9331, 311, 34549, 15712, 11, 1181, 8357, 525, 10454, 14756, 70767, 13, 1988, 1128, 6896, 374, 5538, 6832, 30, 1597, 1128, 3643, 432, 773, 7988, 30, 220]\n",
      "4\n",
      "[[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11, 702, 13791], [1506, 279, 18414, 315, 20443, 11229, 304, 3213, 1635, 13, 5542, 656], [59711, 9331, 311, 34549, 15712, 11, 1181, 8357, 525, 10454, 14756, 70767], [13, 1988, 1128, 6896, 374, 5538, 6832, 30, 1597, 1128, 3643, 432], [773, 7988, 30, 220]]\n",
      "chunks_for_tokenization  {2}\n",
      "=== replaced_original_tokens\n",
      "Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|>. But what exactly is deep learning? And what makes it so powerful? \n",
      "=== new_input_tokens\n",
      "Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|>. But what exactly is deep learning? And what makes it so powerful? \n",
      "44\n",
      "==\n",
      "==\n",
      "==\n",
      "[10227, 42415, 1119, 279, 330, 32880, 1, 949, 11, 1077, 594, 5695, 264, 16266, 448, 279, 31774, 315, 20443, 29728, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25]\n",
      "10\n",
      "[[10227, 42415, 1119, 279, 330, 32880, 1, 949, 11, 1077, 594, 5695], [264, 16266, 448, 279, 31774, 315, 20443, 29728, 14155, 320, 12003, 82], [568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645], [304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924], [25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527], [76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617], [13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220], [13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167], [315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220], [13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136], [2924, 25]]\n",
      "chunks_for_tokenization  {7}\n",
      "=== replaced_original_tokens\n",
      "Before diving into the \"deep\" part, let's establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|> of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include:\n",
      "=== new_input_tokens\n",
      "Before diving into the \"deep\" part, let's establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|> of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include:\n",
      "114\n",
      "==\n",
      "==\n",
      "==\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2, 114), (2, 122), (2, 122))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from more_itertools import chunked\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "text_token_id = tokenizer.encode(\"<|object_ref_start|>\")[0]\n",
    "eos_token_id = tokenizer.encode(\"<|endoftext|>\")[0]\n",
    "\n",
    "window_size = 3\n",
    "chunk_size = 4\n",
    "new_tokens = []\n",
    "original_tokens = tokenizer.batch_encode_plus(\n",
    "    text_example,\n",
    "    padding=True,\n",
    ")\n",
    "compressed_tokens = []\n",
    "replaced_original_tokens_batch = []\n",
    "# original_tokens\n",
    "for tokens in original_tokens[\"input_ids\"]:\n",
    "    original_lines = np.array(tokens)\n",
    "    pure_tokens = original_lines[original_lines != eos_token_id].tolist()\n",
    "    print(pure_tokens)\n",
    "    full_chunks_amount = len(pure_tokens) // (window_size * chunk_size)\n",
    "    print(full_chunks_amount)\n",
    "    max_percent = 0.8\n",
    "    pure_tokens_chunks = list(chunked(pure_tokens, window_size * chunk_size))\n",
    "    print(pure_tokens_chunks)\n",
    "    prob = 0.3\n",
    "    random_mask = np.random.random(int(full_chunks_amount * max_percent))\n",
    "    mask = random_mask < prob\n",
    "    chunks_for_tokenization = np.where(mask)[0].tolist()\n",
    "    chunks_for_tokenization = set(chunks_for_tokenization)\n",
    "    if len(chunks_for_tokenization) == 0:\n",
    "        chunks_for_tokenization = set(\n",
    "            [\n",
    "                random.randint(\n",
    "                    0,\n",
    "                    int(full_chunks_amount * max_percent),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    print(\"chunks_for_tokenization \", chunks_for_tokenization)\n",
    "    replaced_original_tokens = []\n",
    "    new_input_tokens = []\n",
    "    for i, tokens in enumerate(pure_tokens_chunks):\n",
    "        if i in chunks_for_tokenization:\n",
    "            replaced_original_tokens.extend([text_token_id] * len(tokens))\n",
    "            new_input_tokens.extend([text_token_id] * chunk_size)\n",
    "        else:\n",
    "            replaced_original_tokens.extend(tokens)\n",
    "            new_input_tokens.extend(tokens)\n",
    "    print(\"=== replaced_original_tokens\")\n",
    "    print(tokenizer.decode(replaced_original_tokens))\n",
    "    print(\"=== new_input_tokens\")\n",
    "    print(tokenizer.decode(new_input_tokens))\n",
    "    print(len(new_input_tokens))\n",
    "    compressed_tokens.append(new_input_tokens)\n",
    "    replaced_original_tokens_batch.append(replaced_original_tokens)\n",
    "    print(\"==\")\n",
    "    print(\"==\")\n",
    "    print(\"==\")\n",
    "\n",
    "compressed_tokens_attention = []\n",
    "max_compressed_len = max([len(item) for item in compressed_tokens])\n",
    "max_replaced_len = max([len(item) for item in replaced_original_tokens_batch])\n",
    "\n",
    "for compressed_seq, replaced_seq in zip(\n",
    "    compressed_tokens,\n",
    "    replaced_original_tokens_batch,\n",
    "):\n",
    "    compressed_seq_len = len(compressed_seq)\n",
    "    replaced_seq_len = len(replaced_seq)\n",
    "    attention_mask = [1] * (compressed_seq_len)\n",
    "\n",
    "    if compressed_seq_len < max_compressed_len:\n",
    "        compressed_seq += [eos_token_id] * (max_compressed_len - compressed_seq_len)\n",
    "        attention_mask += [0] * (max_compressed_len - compressed_seq_len)\n",
    "\n",
    "    if compressed_seq_len < max_replaced_len:\n",
    "        replaced_seq += [eos_token_id] * (max_replaced_len - replaced_seq_len)\n",
    "\n",
    "    compressed_tokens_attention.append(attention_mask)\n",
    "# len(compressed_tokens[0]), len(compressed_tokens[1])\n",
    "# compressed_tokens = torch.tensor(compressed_tokens)\n",
    "# compressed_tokens_attention = torch.tensor(compressed_tokens_attention)\n",
    "np.array(compressed_tokens).shape, np.array(\n",
    "    replaced_original_tokens_batch\n",
    ").shape, np.array(original_tokens[\"input_ids\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 114, 896]),\n",
       " torch.Size([2, 122, 896]),\n",
       " torch.Size([2, 122, 896]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tokens_torch = torch.tensor(\n",
    "    original_tokens[\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "replaced_tokens_torch = torch.tensor(\n",
    "    replaced_original_tokens_batch,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "compressed_tokens_torch = torch.tensor(\n",
    "    compressed_tokens,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "# torch.Size([2, 51, 896])\n",
    "original_embeds = model.get_input_embeddings()(original_tokens_torch)\n",
    "replaced_embeds = model.get_input_embeddings()(replaced_tokens_torch)\n",
    "# torch.Size([2, 35, 896]) 51 - 3*4*2 + 4*2\n",
    "compressed_embeds_template = model.get_input_embeddings()(compressed_tokens_torch)\n",
    "compressed_embeds_template.shape, original_embeds.shape, replaced_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.dtype, original_embeds.dtype, replaced_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151646"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 896]), torch.Size([8]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_for_compression_mask = replaced_tokens_torch == text_token_id\n",
    "compressed_tokens_mask = compressed_tokens_torch == text_token_id\n",
    "# original_tokens_torch[tokens_for_compression_mask].shape\n",
    "original_embeds[tokens_for_compression_mask].shape, compressed_tokens_torch[\n",
    "    compressed_tokens_mask\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 896])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds_for_compression = original_embeds[tokens_for_compression_mask].reshape(\n",
    "    -1,\n",
    "    window_size,\n",
    "    original_embeds.shape[-1],\n",
    ")\n",
    "embeds_for_compression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 896]), torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_embeds = embed_pooler_v2(embeds_for_compression)\n",
    "# pooled_embeds = pooled_embeds.reshape(pooled_embeds.shape[0], -1)\n",
    "pooled_embeds.shape, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1],\n",
       "        [2, 3, 0, 4, 5]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "self = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [1, 1, 0, 1, 1],\n",
    "    ],\n",
    "    dtype=torch.bool,\n",
    ")\n",
    "source = torch.tensor(\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    ")\n",
    "self.masked_scatter_(mask, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98, 896])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_tokens_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98, 896])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_tokens_mask.shape\n",
    "# .expand_as(compressed_embeds_template).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 114, 896])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, torch.float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.dtype, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 114, 896])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.masked_scatter(\n",
    "    compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "    pooled_embeds,\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template = compressed_embeds_template.masked_scatter_(\n",
    "    compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "    pooled_embeds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template[compressed_tokens_mask][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template[compressed_tokens_mask][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = compressed_tokens_torch.clone()\n",
    "text_token_id = tokenizer.encode(\"<|object_ref_start|>\")[0]\n",
    "labels[labels == text_token_id] = -100\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0430, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    inputs_embeds=compressed_embeds_template,\n",
    "    labels=labels,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R1 Chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>how many wings has a bird?<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Okay, so I need to figure out how many wings a bird has. Hmm, let me think about this. I know that birds are birds, so they're animals, but I'm not exactly sure about their wings. I remember that birds have wings, but I'm not certain how many there are. Maybe I should start by recalling what I know about birds.\\n\\nFirst, I know that birds are divided into different types, like birds of prey, birds of free flight, and maybe some others. But the question is about the number of wings, so I think it's a general question. I remember that most birds have wings, but I'm not sure if they have more than two. I think most birds have two wings, one on each side, right? So, maybe that's the answer.\\n\\nWait, but I should make sure. Let me think about some specific examples. For instance, a typical bird like a sparrow has two wings, one on each side. A crow also has two wings. What about a penguin? Oh, wait, penguins don't have wings, they have flippers. So that's different. But the question is about how many wings a bird has, so I guess penguins are out of the picture here.\\n\\nAnother example: a ostrich. I think it has two wings, one on each side. So, that's consistent. How about a peacock? I think peacocks have two wings, one on each side. So, that's another example. Maybe a flamingo? Wait, flamingos don't have wings either, they have a tail and a head. So, that's not relevant here.\\n\\nSo, from these examples, it seems like most birds have two wings. But I should also consider if there are any birds that have more than two wings. I'm not sure. Maybe some birds have more, but I can't think of any. I think most birds have two wings, so the answer is two.\\n\\nWait, but I should make sure I'm not missing any birds that have more wings. Let me think about some other birds. A duck has two wings, right? And a seagull, which is a type of bird, has two wings too. So, yeah, all the birds I can think of have two wings. So, I think the answer is two wings per bird.\\n</think>\\n\\nA bird has two wings.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"how many wings has a bird?\"\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>how many wings has a bird?<｜Assistant｜><think>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out how many wings a bird has. Hmm, let me think about this. I know that birds are birds, so they're animals, but I'm not exactly sure about their wings. I remember that birds have wings, but I'm not certain how many there are. Maybe I should start by recalling what I know about birds.\n",
      "\n",
      "First, I know that birds are divided into different types, like birds of prey, birds of free flight, and maybe some others. But the question is about the number of wings, so I think it's a general question. I remember that most birds have wings, but I'm not sure if they have more than two. I think most birds have two wings, one on each side, right? So, maybe that's the answer.\n",
      "\n",
      "Wait, but I should make sure. Let me think about some specific examples. For instance, a typical bird like a sparrow has two wings, one on each side. A crow also has two wings. What about a penguin? Oh, wait, penguins don't have wings, they have flippers. So that's different. But the question is about how many wings a bird has, so I guess penguins are out of the picture here.\n",
      "\n",
      "Another example: a ostrich. I think it has two wings, one on each side. So, that's consistent. How about a peacock? I think peacocks have two wings, one on each side. So, that's another example. Maybe a flamingo? Wait, flamingos don't have wings either, they have a tail and a head. So, that's not relevant here.\n",
      "\n",
      "So, from these examples, it seems like most birds have two wings. But I should also consider if there are any birds that have more than two wings. I'm not sure. Maybe some birds have more, but I can't think of any. I think most birds have two wings, so the answer is two.\n",
      "\n",
      "Wait, but I should make sure I'm not missing any birds that have more wings. Let me think about some other birds. A duck has two wings, right? And a seagull, which is a type of bird, has two wings too. So, yeah, all the birds I can think of have two wings. So, I think the answer is two wings per bird.\n",
      "</think>\n",
      "\n",
      "A bird has two wings.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151645: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151646: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151662"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|fim_pad|>\", add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "client = InferenceClient(f\"http://{open('ip').read()}:1338\")\n",
    "\n",
    "user_prompt = \"9.11 and 9.9 -- which is bigger? Let's think step by step.\"\n",
    "output = client.chat.completions.create(\n",
    "    model=\"tgi\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        },\n",
    "    ],\n",
    "    stream=False,\n",
    "    max_tokens=10000,\n",
    "    temperature=0.0,\n",
    ")\n",
    "result = output.choices[0].message.content\n",
    "print(\"total_len\", len(tokenizer.encode(result)))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: 'Okay, so I\\'m trying to figure out what the user named is. They just said \"Hello, my name is\" and then stopped. Hmm, that\\'s a bit confusing. Maybe they forgot to finish the sentence. I should probably ask them to complete it so I can help better. I don\\'t want to assume anything about their identity or name. It\\'s better to be safe than sorry. I\\'ll let them know I\\'m here to help once they provide their full name.\\n</think>\\n\\nIt seems like your name is incomplete. Could you please provide your full name so I can assist you better?'\n",
      "Prompt: 'The president of the United States is', Generated text: \"Okay, so I need to figure out what the president of the United States is. Hmm, I'm not exactly sure, but I think the president is the head of the executive branch of the United States. I remember hearing that they're usually named after a person, like George W. Bush or Barack Obama. Maybe I should look up the current president to get the most accurate information.\\n\\nWait, I'm not sure if I can access the internet right now. Maybe I can think of some famous presidents and their names. I know George Washington was the first president, and he's often called the first president. Then there's John Adams, who was the second president. After that, Thomas Jefferson was the third president. I think that's right because he was the first president of the states, not the country.\\n\\nAfter that, James Madison and James Monroe were the fourth and fifth presidents, respectively. Then, after that, Abraham Lincoln was the sixth president. He was the first president of the United States, but he was also the first president of the Confederate States. After him, Andrew Jackson was the seventh president. He was a significant figure in the Civil Rights Movement.\\n\\nThen, after Andrew Jackson, Martin Van Buren was the eighth president. He was a statesman and played a role in the Civil War. After him, James K. Polk was the ninth president. He was known for his policies, like the transcontinental railroad. Following Polk, William Henry Harrison was the tenth president, but he died in office, so he's often excluded from the list.\\n\\nAfter Harrison, John Tyler was the eleventh president, and he was a statesman and military leader. Then, Franklin Pierce was the twelfth president. He was a key figure in the Civil War. After that, James Buchanan was the thirteenth president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the fourteenth president, the first president of the United States. Then, Andrew Johnson was the fifteenth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the sixteenth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the seventeenth president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the eighteenth president again, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler\"\n",
      "Prompt: 'The capital of France is', Generated text: \"Okay, so I need to figure out the capital of France. I remember that France is a country in Europe, but I'm not exactly sure where its capital is. I think it's somewhere in the north, maybe in the Pyrenees or something. I've heard of Paris before, but I'm not sure if it's the only capital. I should probably start by recalling what I know about capitals.\\n\\nI know that the capital of France is the most populous city, so it's probably a major city. Paris comes to mind because it's the capital of many countries, but I'm not certain if it's the only one. I think there are other capitals, but I'm not sure which ones. Maybe I should think about the countries that are connected to France. France borders several countries, so maybe each of those has their own capital.\\n\\nI remember that the capital of France is also called the capital of the European Union, but I'm not sure if that's correct. I think it's actually the capital of the European Union, but I might be mixing that up with something else. I should double-check that. Oh, right, the European Union's capital is also Paris, so that makes sense.\\n\\nNow, thinking about the geography, France is a large country, so its capital should be in a significant geographical location. The Pyrenees are a mountain range in western Europe, so maybe Paris is near there. I think Paris is in the northern part of France, so it's probably in the Pyrenean region. That would make sense because the Pyrenees are known for their mountains and the surrounding areas.\\n\\nI also recall that Paris is the capital of many European countries, so it's likely the most central and important city in France. It's not just a random city; it's the hub for a lot of activities, trade, and political life. I think it's also the capital of the Seine River, which is a major river in France, so that adds to its importance.\\n\\nWait, I should make sure I'm not confusing it with another city. I think I've heard of other capitals, like Berlin or London, but those are in different countries. So, France's capital is definitely Paris. I can't think of any other cities that are more central or have such a significant role in France as Paris does.\\n\\nTo summarize, I believe the capital of France is Paris. It's the most populous city, the capital of the European Union, and it's located in the Pyrenean region, making it a major hub for various aspects of life in France.\\n</think>\\n\\nThe capital of France is Paris. It is the most populous city in the country, the capital of the European Union, and is located in the Pyrenean region, making it a significant hub for various aspects of life in France.\"\n",
      "Prompt: 'The future of AI is', Generated text: \"Okay, so I'm trying to understand the future of AI. I know a bit about AI from school, but I'm not an expert. Let me start by breaking down what I know and then think about what might happen next.\\n\\nFirst, I remember that AI is about machines learning and doing tasks that usually require human intelligence. So, things like language translation, image recognition, and even playing games at a superhuman level. But I'm not sure how far we've come yet.\\n\\nI think there are different types of AI. There's general AI, which is the broad term, and then there's narrow AI, which is more specific. Narrow AI is good for things like medical diagnosis or stock trading because it's very precise. But I'm not sure if that's the future. Maybe AI will become more like general AI, handling a wider range of tasks.\\n\\nAnother thing I've heard is about the ethical implications. I know that AI can cause problems like job displacement or discrimination if it makes people do tasks that humans do less. There's also the issue of privacy—AI systems can collect and analyze data without consent. So, there's a lot of debate about how to handle AI responsibly.\\n\\nI've also heard about the concept of superintelligent AI. I think that's when AI surpasses human intelligence, but I'm not sure how that's going to happen. Maybe through more efficient algorithms or better understanding of human intelligence. But I'm not certain if that's realistic.\\n\\nThen there's the question of how AI will affect society. I imagine it could lead to more automation in industries, but it might also create new jobs or change how people work. There's also the possibility of AI being used in ways that exacerbate inequality, like in hiring or lending decisions.\\n\\nI'm also thinking about the future of human creativity. I know that AI can generate art, but will it replace human creativity? Or will it complement it? I think it's a mix, but maybe AI will become more of a tool than a creator in the long run.\\n\\nAnother area is the integration of AI with other technologies. I've heard about AI augmenting human capabilities, like in healthcare or education. But there's also the risk of AI becoming too dependent on humans, like in surveillance or control systems.\\n\\nI'm curious about the timeline for AI development. I know that some technologies like self-driving cars are already in the works, but it's still in the early stages. Maybe AI will become more advanced as we develop better algorithms and data.\\n\\nI also wonder about the role of humans in the future. Will AI take over certain jobs, or will humans continue to be essential for things like education, healthcare, and creativity? There's a lot of debate about this.\\n\\nThere's also the issue of job displacement. If AI becomes too powerful, it could replace many jobs, leading to a new kind of labor force. I'm not sure how that will be managed or what the job market will look like.\\n\\nI'm trying to think about the potential for AI to solve problems that are currently unsolvable. For example, climate change or pandemics. AI could help model the spread of diseases or predict environmental changes, which could lead to better policies or solutions.\\n\\nBut I'm also concerned about the limitations of AI. It can't process emotions or learn from scratch data, so it might struggle with complex human-like tasks. There's a need for more human-like AI, like in creative problem-solving or decision-making.\\n\\nI'm also thinking about the ethical guidelines for AI. There's a lot of talk about privacy, bias, and transparency. Ensuring that AI systems are fair and transparent is crucial, but it's a big challenge.\\n\\nIn summary, the future of AI seems to be a mix of advancements, ethical considerations, societal impacts, and the balance between human and AI capabilities. It's a complex and evolving field with many possibilities and challenges. I'm not entirely sure about all these aspects, but I think it's clear that AI will play a significant role in the future, but we'll need to be careful about how it's developed and used.\\n</think>\\n\\nThe future of AI is a multifaceted and evolving field that encompasses several key considerations and possibilities. Here's a structured overview:\\n\\n1. **Types of AI**: \\n   - **General AI**: The broad term representing machines capable of performing tasks that require human intelligence, such as language translation and game playing.\\n   - **Narrow AI**: More specific, like medical diagnosis or stock trading, which excel in precise tasks.\\n\\n2. **Ethical Implications**:\\n   - **Privacy and Bias**: Concerns about data collection and the potential for bias in AI systems.\\n   - ** job displacement**: Risks of automation leading to job loss and the need for job displacement strategies.\\n\\n3. **Superintelligent AI**:\\n   - Theoretical possibility of AI surpassing human intelligence, driven by advancements in algorithms and understanding of human intelligence.\\n\\n4. **Societal Impact**:\\n   - Potential for AI to exacerbate inequality, affecting areas like hiring and lending.\\n   - Integration with other technologies, such as healthcare and education, and risks of dependency on humans.\\n\\n5. **Human Creativity**:\\n   - A blend of AI-generated art and human creativity, with AI potentially becoming more of a tool than a creator.\\n\\n6. **Integration with Other Technologies**:\\n   - AI augmenting human capabilities in healthcare and education, but risks of dependence on humans in surveillance and control systems.\\n\\n7. **Timeline and Development**:\\n   - Early stages of AI development, such as self-driving cars, are in progress, with potential for more advanced technologies.\\n\\n8. **Job Market and Labor Force**:\\n   - Concerns about job displacement and the potential for a new labor force, requiring management strategies.\\n\\n9. **Problem Solving and Complex Tasks**:\\n   - AI's role in solving climate change and pandemics through modeling and prediction, leading to better policies.\\n\\n10. **Limitations and Challenges**:\\n    - AI's limitations in processing emotions and learning from data, necessitating more human-like capabilities.\\n\\n11. **Ethical Guidelines**:\\n    - Emphasis on fairness, transparency, and privacy, crucial for ensuring ethical AI development.\\n\\nIn conclusion, the future of AI is poised for significant impact, with ethical considerations, societal implications, and the balance between human and AI capabilities shaping its trajectory. Addressing these challenges will be essential for the responsible development and deployment of AI.\"\n"
     ]
    }
   ],
   "source": [
    "import concurrent\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(f\"http://{open('ip').read()}:1338\")\n",
    "\n",
    "\n",
    "def gen_text(text):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"tgi\",\n",
    "        messages=[{\"role\": \"user\", \"content\": text}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=5_000,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "\n",
    "def batch_generation(prompts):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(prompts)) as executor:\n",
    "        prompts_results = list(executor.map(gen_text, prompts))\n",
    "    return prompts_results\n",
    "\n",
    "\n",
    "prompts_results = batch_generation(prompts)\n",
    "\n",
    "for prompt, prompts_result in zip(prompts, prompts_results):\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {prompts_result!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'system_prompt', 'question', 'response'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Open-Orca/OpenOrca\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=5000, seed=42)\n",
    "dataset = dataset[\"test\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fec9cc698f24fe4a4ce8b7f17e180d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from more_itertools import chunked\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "batch_size = 32 * 2 * 2\n",
    "questions = list(\n",
    "    chunked(\n",
    "        dataset[\"question\"],\n",
    "        batch_size,\n",
    "    )\n",
    ")\n",
    "# 5 min 26 sec - 1000\n",
    "# 28 min 6 sec - 5000\n",
    "correct_qa_pairs = []\n",
    "for question_chunk in tqdm(questions):\n",
    "    answers = batch_generation(question_chunk)\n",
    "    for question, answer in zip(question_chunk, answers):\n",
    "        if answer.count(\"</think>\") == 1:\n",
    "            correct_qa_pairs.append(\n",
    "                [\n",
    "                    question,\n",
    "                    answer,\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4475, 5000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_qa_pairs), len(dataset[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can\\'t complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio\\'s death came after Peterson\\'s fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\\n\\nWrite an article based on these highlights.',\n",
       " 'answer': \"Okay, so I need to write an article based on the highlights provided about Drew Peterson and his fourth wife, Kathleen Savio. The user has given me some specific points to include, so I should make sure to cover all of them.\\n\\nFirst, I should start by introducing Drew Peterson and his fourth wife, Kathleen Savio. I need to mention that he's been arrested in the deaths of his first and fourth wives. That's a significant event because it's the second time he's been involved in such a death, which could be a big deal.\\n\\nI should also note that the renewed interest in Savio's death came after his fourth wife disappeared. That's an important detail because it shows that the case isn't over and that there's still a lot to explore.\\n\\nNext, the user mentioned that Peterson's attorney has denied any wrongdoing in either case. That's crucial because it sets the tone that the case is being handled fairly, and there's no evidence of wrongdoing. I should make sure to highlight that part.\\n\\nI should also touch on the legal implications. Peterson is facing charges related to the deaths, and his attorney's denial is a positive sign. Maybe I can mention that the case is being handled with fairness and transparency.\\n\\nI need to structure the article in a way that flows well. Maybe start with an introduction about Peterson and his fourth wife, then move on to the legal developments, the renewed interest, and the attorney's denial. Finally, conclude with the implications of the case.\\n\\nI should also consider the tone. It should be informative but not too technical, so it's accessible to a general audience. I'll need to make sure the language is clear and concise.\\n\\nI should also think about any potential counterarguments or questions that might arise. For example, why is there renewed interest in Savio's death? It could be because of the fourth wife's disappearance, which is a significant event. Maybe I can mention that as a point of interest.\\n\\nI should also ensure that the article doesn't just list the facts but also provides context and analysis. For example, discussing the legal process, the significance of the deaths, and the impact on the community.\\n\\nI need to make sure that the article is balanced, presenting both the positive developments and any potential challenges. Maybe mention that while the case is being handled fairly, there could be future legal challenges, but for now, it's a positive situation.\\n\\nI should also check for any recent developments or updates that might affect the article. For instance, if there have been any court proceedings or legal strategies mentioned, I should include that.\\n\\nFinally, I should wrap up the article by summarizing the key points and emphasizing the importance of the case in the community. Maybe end with a statement about the impact of these deaths on the lives of the victims and their families.\\n\\nI think that's a good outline. Now, I'll start drafting the article, making sure to cover all these points and present them in a logical and engaging manner.\\n</think>\\n\\n**Drew Peterson's Fourth Wife: A New Dawn in the Case**\\n\\nDrew Peterson, the controversial actor and former President of the United States, has recently come to light as the fourth victim of his wife, Kathleen Savio. In a significant development, Peterson has been arrested in the deaths of his first and fourth wives, marking a new chapter in his legal troubles. His attorney has denied any wrongdoing in either case, promising a fair and transparent handling of the matter.\\n\\nThe renewed interest in Savio's death stems from the disappearance of his fourth wife, which has sparked a wave of attention and speculation. Peterson's attorney has denied any wrongdoing, emphasizing that the case is being handled with fairness and transparency. This stance is a positive sign, as it suggests that the legal process is being conducted with integrity.\\n\\nPeterson's legal troubles are complex, involving charges related to the deaths of his first and fourth wives. The case highlights the legal challenges faced by individuals involved in such high-profile deaths, particularly when the victims are not immediately accounted for. Peterson's attorney's denial of wrongdoing is a testament to the efforts being made to ensure justice is served.\\n\\nThe case has also sparked discussions about the legal process and the impact of such high-profile deaths on the community. While the case is being handled fairly, there could be future legal challenges, but for now, it is a positive situation. The deaths of these women have left a significant impact on their families and communities, underscoring the importance of addressing such cases with fairness and transparency.\\n\\nIn conclusion, Drew Peterson's fourth wife, Kathleen Savio, has brought a new perspective to the legal landscape, with his legal troubles being handled with fairness and transparency. The case highlights the ongoing efforts to ensure justice and the importance of addressing such high-profile deaths with care and consideration.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = []\n",
    "for question, answer in correct_qa_pairs:\n",
    "    new_dataset.append(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "        }\n",
    "    )\n",
    "new_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "new_dataset = Dataset.from_list(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58237a7b20794de08223bd210c02e365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14b26aaf4ae4f099412a736c74d1a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/dim/open_orca_4475_DeepSeek-R1-Distill-Qwen-1.5B/commit/ca6edb6c3da6791bddb190338648e6c999f1d1e3', commit_message='Upload dataset', commit_description='', oid='ca6edb6c3da6791bddb190338648e6c999f1d1e3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/dim/open_orca_4475_DeepSeek-R1-Distill-Qwen-1.5B', endpoint='https://huggingface.co', repo_type='dataset', repo_id='dim/open_orca_4475_DeepSeek-R1-Distill-Qwen-1.5B'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.push_to_hub(\n",
    "    f\"dim/open_orca_{len(correct_qa_pairs)}_DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 905\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151646"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "think_end_id = tokenizer.encode(\"</think>\")[0]\n",
    "think_end_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can\\'t complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio\\'s death came after Peterson\\'s fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\\n\\nWrite an article based on these highlights.<｜Assistant｜><think>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": dataset[0][\"question\"]},\n",
    "    # {\"role\": \"assistant\", \"content\": result},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "# print(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can\\'t complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio\\'s death came after Peterson\\'s fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\\n\\nWrite an article based on these highlights.<｜Assistant｜><think>\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151646, 151644]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<｜begin▁of▁sentence｜><｜User｜>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can't complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio's death came after Peterson's fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\n",
      "\n",
      "Write an article based on these highlights.<｜Assistant｜><think>\n",
      "Okay, so I need to write an article based on the highlights provided about Drew Peterson and his fourth wife, Kathleen Savio. The user has given me some specific points to include, so I should make sure to cover all of them.\n",
      "\n",
      "First, I should start by introducing Drew Peterson and his fourth wife, Kathleen Savio. I need to mention that he's been arrested in the deaths of his first and fourth wives. That's a significant event because it's the second time he's been involved in such a death, which could be a big deal.\n",
      "\n",
      "I should also note that the renewed interest in Savio's death came after his fourth wife disappeared. That's an important detail because it shows that the case isn't over and that there's still a lot to explore.\n",
      "\n",
      "Next, the user mentioned that Peterson's attorney has denied any wrongdoing in either case. That's crucial because it sets the tone that the case is being handled fairly, and there's no evidence of wrongdoing. I should make sure to highlight that part.\n",
      "\n",
      "I should also touch on the legal implications. Peterson is facing charges related to the deaths, and his attorney's denial is a positive sign. Maybe I can mention that the case is being handled with fairness and transparency.\n",
      "\n",
      "I need to structure the article in a way that flows well. Maybe start with an introduction about Peterson and his fourth wife, then move on to the legal developments, the renewed interest, and the attorney's denial. Finally, conclude with the implications of the case.\n",
      "\n",
      "I should also consider the tone. It should be informative but not too technical, so it's accessible to a general audience. I'll need to make sure the language is clear and concise.\n",
      "\n",
      "I should also think about any potential counterarguments or questions that might arise. For example, why is there renewed interest in Savio's death? It could be because of the fourth wife's disappearance, which is a significant event. Maybe I can mention that as a point of interest.\n",
      "\n",
      "I should also ensure that the article doesn't just list the facts but also provides context and analysis. For example, discussing the legal process, the significance of the deaths, and the impact on the community.\n",
      "\n",
      "I need to make sure that the article is balanced, presenting both the positive developments and any potential challenges. Maybe mention that while the case is being handled fairly, there could be future legal challenges, but for now, it's a positive situation.\n",
      "\n",
      "I should also check for any recent developments or updates. The user didn't provide specific dates, so I'll have to keep it general, mentioning that the case is ongoing and that the attorney's denial is a key point.\n",
      "\n",
      "Finally, I should conclude by summarizing the main points and emphasizing the importance of the case in the community. Maybe suggest that the legal community should support Peterson and his attorney in handling the case.\n",
      "\n",
      "I think that's a good outline. Now, I'll structure the article accordingly, making sure each section flows into the next and covers all the key points the user provided.\n",
      "</think>\n",
      "\n",
      "**Drew Peterson's Fourth Wife: A Legal Journey and Renewed Interest**\n",
      "\n",
      "Drew Peterson, a controversial figure known for his controversial past, has recently come to light as the subject of legal proceedings involving his fourth wife, Kathleen Savio. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges related to these tragic incidents. His attorney has denied any wrongdoing in either case, presenting a positive narrative for the legal community.\n",
      "\n",
      "**Introduction to Drew Peterson and His Fourth Wife**\n",
      "\n",
      "Drew Peterson, a controversial figure, has been the subject of legal scrutiny for his past actions, including his controversial death of his first wife, Jane Savio. Now, his fourth wife, Kathleen Savio, has come into the limelight. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges of murder and multiple counts of intentional killing. His attorney has denied any wrongdoing in either case, highlighting a positive stance in the legal community.\n",
      "\n",
      "**Legal Developments and Renewed Interest**\n",
      "\n",
      "Peterson's attorney has denied any wrongdoing in either case, emphasizing that the deaths are not isolated incidents. The renewed interest in Savio's death stems from the fact that his fourth wife disappeared, adding another layer of complexity to the case. This disappearance has sparked discussions about the legal process and the potential impact on the community.\n",
      "\n",
      "**The Case's Legal Implications**\n",
      "\n",
      "Peterson is facing charges of murder and multiple counts of intentional killing, with a possible charge of conspiracy. His attorney has denied any wrongdoing, presenting a positive narrative. The case is being handled with fairness and transparency, reflecting the legal community's commitment to resolving the issue without any evidence of wrongdoing.\n",
      "\n",
      "**Conclusion and Implications**\n",
      "\n",
      "While the case is being handled fairly, there could be future legal challenges. However, for now, the legal community should support Peterson and his attorney in handling the case. The renewed interest in Savio's death highlights the ongoing legal battle and the importance of addressing the complexities of the case. The legal community should continue to support Peterson and his attorney in resolving the issue, ensuring justice is served.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py#L150\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": dataset[0][\"question\"]},\n",
    "    # {\"role\": \"assistant\", \"content\": dataset[0][\"answer\"]},\n",
    "]\n",
    "part_1 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    # return_dict=True,\n",
    ")\n",
    "part_2 = tokenizer.encode(\n",
    "    dataset[0][\"answer\"],\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "part_3 = tokenizer.encode(\n",
    "    \"<｜end▁of▁sentence｜>\",\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "print(tokenizer.decode(part_1 + part_2 + part_3))\n",
    "labels = len(part_1) * [-100] + part_2 + [-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can't complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio's death came after Peterson's fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\n",
      "\n",
      "Write an article based on these highlights.<｜Assistant｜>\n",
      "\n",
      "**Drew Peterson's Fourth Wife: A Legal Journey and Renewed Interest**\n",
      "\n",
      "Drew Peterson, a controversial figure known for his controversial past, has recently come to light as the subject of legal proceedings involving his fourth wife, Kathleen Savio. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges related to these tragic incidents. His attorney has denied any wrongdoing in either case, presenting a positive narrative for the legal community.\n",
      "\n",
      "**Introduction to Drew Peterson and His Fourth Wife**\n",
      "\n",
      "Drew Peterson, a controversial figure, has been the subject of legal scrutiny for his past actions, including his controversial death of his first wife, Jane Savio. Now, his fourth wife, Kathleen Savio, has come into the limelight. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges of murder and multiple counts of intentional killing. His attorney has denied any wrongdoing in either case, highlighting a positive stance in the legal community.\n",
      "\n",
      "**Legal Developments and Renewed Interest**\n",
      "\n",
      "Peterson's attorney has denied any wrongdoing in either case, emphasizing that the deaths are not isolated incidents. The renewed interest in Savio's death stems from the fact that his fourth wife disappeared, adding another layer of complexity to the case. This disappearance has sparked discussions about the legal process and the potential impact on the community.\n",
      "\n",
      "**The Case's Legal Implications**\n",
      "\n",
      "Peterson is facing charges of murder and multiple counts of intentional killing, with a possible charge of conspiracy. His attorney has denied any wrongdoing, presenting a positive narrative. The case is being handled with fairness and transparency, reflecting the legal community's commitment to resolving the issue without any evidence of wrongdoing.\n",
      "\n",
      "**Conclusion and Implications**\n",
      "\n",
      "While the case is being handled fairly, there could be future legal challenges. However, for now, the legal community should support Peterson and his attorney in handling the case. The renewed interest in Savio's death highlights the ongoing legal battle and the importance of addressing the complexities of the case. The legal community should continue to support Peterson and his attorney in resolving the issue, ensuring justice is served.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": dataset[0][\"question\"]},\n",
    "    {\"role\": \"assistant\", \"content\": dataset[0][\"answer\"]},\n",
    "]\n",
    "\n",
    "part_3 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    # add_generation_prompt=False,\n",
    "    # add_generation_prompt=True,\n",
    "    # continue_final_message=True,\n",
    "    # return_dict=True,\n",
    "    # return_assistant_tokens_mask=True\n",
    ")\n",
    "part_3\n",
    "print(tokenizer.decode(part_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual chat templating (single turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>question<｜Assistant｜>answer<｜end▁of▁sentence｜>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"question\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"answer\"},\n",
    "    ],\n",
    "    tokenize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>question<｜Assistant｜><think>\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"question\"},\n",
    "    ],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "content_compression_mask = []\n",
    "part_1 = \"\"\"<｜begin▁of▁sentence｜><｜User｜>\"\"\"\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_1,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [0]\n",
    "print(content_compression_mask)\n",
    "part_2 = 'NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can'\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_2,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [1]\n",
    "print(content_compression_mask)\n",
    "part_3 = \"<｜Assistant｜><think>\\n\"\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_3,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [0]\n",
    "print(content_compression_mask)\n",
    "part_4, part_5 = \"Peterson attorney that</think>Peterson is facing facing\".split(\n",
    "    \"</think>\"\n",
    ")\n",
    "\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_4,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [2]\n",
    "# </think>\n",
    "content_compression_mask += [0]\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_5,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [3]\n",
    "\n",
    "print(content_compression_mask)\n",
    "content_compression_mask += [0]\n",
    "print(content_compression_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can't complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio's death came after Peterson's fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\n",
      "\n",
      "Write an article based on these highlights.<｜Assistant｜><think>\n",
      "Okay, so I need to write an article based on the highlights provided about Drew Peterson and his fourth wife, Kathleen Savio. The user has given me some specific points to include, so I should make sure to cover all of them.\n",
      "\n",
      "First, I should start by introducing Drew Peterson and his fourth wife, Kathleen Savio. I need to mention that he's been arrested in the deaths of his first and fourth wives. That's a significant event because it's the second time he's been involved in such a death, which could be a big deal.\n",
      "\n",
      "I should also note that the renewed interest in Savio's death came after his fourth wife disappeared. That's an important detail because it shows that the case isn't over and that there's still a lot to explore.\n",
      "\n",
      "Next, the user mentioned that Peterson's attorney has denied any wrongdoing in either case. That's crucial because it sets the tone that the case is being handled fairly, and there's no evidence of wrongdoing. I should make sure to highlight that part.\n",
      "\n",
      "I should also touch on the legal implications. Peterson is facing charges related to the deaths, and his attorney's denial is a positive sign. Maybe I can mention that the case is being handled with fairness and transparency.\n",
      "\n",
      "I need to structure the article in a way that flows well. Maybe start with an introduction about Peterson and his fourth wife, then move on to the legal developments, the renewed interest, and the attorney's denial. Finally, conclude with the implications of the case.\n",
      "\n",
      "I should also consider the tone. It should be informative but not too technical, so it's accessible to a general audience. I'll need to make sure the language is clear and concise.\n",
      "\n",
      "I should also think about any potential counterarguments or questions that might arise. For example, why is there renewed interest in Savio's death? It could be because of the fourth wife's disappearance, which is a significant event. Maybe I can mention that as a point of interest.\n",
      "\n",
      "I should also ensure that the article doesn't just list the facts but also provides context and analysis. For example, discussing the legal process, the significance of the deaths, and the impact on the community.\n",
      "\n",
      "I need to make sure that the article is balanced, presenting both the positive developments and any potential challenges. Maybe mention that while the case is being handled fairly, there could be future legal challenges, but for now, it's a positive situation.\n",
      "\n",
      "I should also check for any recent developments or updates. The user didn't provide specific dates, so I'll have to keep it general, mentioning that the case is ongoing and that the attorney's denial is a key point.\n",
      "\n",
      "Finally, I should conclude by summarizing the main points and emphasizing the importance of the case in the community. Maybe suggest that the legal community should support Peterson and his attorney in handling the case.\n",
      "\n",
      "I think that's a good outline. Now, I'll structure the article accordingly, making sure each section flows into the next and covers all the key points the user provided.\n",
      "</think>\n",
      "\n",
      "**Drew Peterson's Fourth Wife: A Legal Journey and Renewed Interest**\n",
      "\n",
      "Drew Peterson, a controversial figure known for his controversial past, has recently come to light as the subject of legal proceedings involving his fourth wife, Kathleen Savio. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges related to these tragic incidents. His attorney has denied any wrongdoing in either case, presenting a positive narrative for the legal community.\n",
      "\n",
      "**Introduction to Drew Peterson and His Fourth Wife**\n",
      "\n",
      "Drew Peterson, a controversial figure, has been the subject of legal scrutiny for his past actions, including his controversial death of his first wife, Jane Savio. Now, his fourth wife, Kathleen Savio, has come into the limelight. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges of murder and multiple counts of intentional killing. His attorney has denied any wrongdoing in either case, highlighting a positive stance in the legal community.\n",
      "\n",
      "**Legal Developments and Renewed Interest**\n",
      "\n",
      "Peterson's attorney has denied any wrongdoing in either case, emphasizing that the deaths are not isolated incidents. The renewed interest in Savio's death stems from the fact that his fourth wife disappeared, adding another layer of complexity to the case. This disappearance has sparked discussions about the legal process and the potential impact on the community.\n",
      "\n",
      "**The Case's Legal Implications**\n",
      "\n",
      "Peterson is facing charges of murder and multiple counts of intentional killing, with a possible charge of conspiracy. His attorney has denied any wrongdoing, presenting a positive narrative. The case is being handled with fairness and transparency, reflecting the legal community's commitment to resolving the issue without any evidence of wrongdoing.\n",
      "\n",
      "**Conclusion and Implications**\n",
      "\n",
      "While the case is being handled fairly, there could be future legal challenges. However, for now, the legal community should support Peterson and his attorney in handling the case. The renewed interest in Savio's death highlights the ongoing legal battle and the importance of addressing the complexities of the case. The legal community should continue to support Peterson and his attorney in resolving the issue, ensuring justice is served.<｜end▁of▁sentence｜>\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_single_turn(question, answer):\n",
    "    content_compression_mask = []\n",
    "\n",
    "    part_1 = \"\"\"<｜begin▁of▁sentence｜><｜User｜>\"\"\"\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_1,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [0]\n",
    "\n",
    "    # question\n",
    "    part_2 = question\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_2,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [1]\n",
    "\n",
    "    part_3 = \"<｜Assistant｜><think>\\n\"\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_3,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [0]\n",
    "\n",
    "    # answer\n",
    "    part_4, part_6 = answer.split(\"</think>\")\n",
    "\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_4,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [2]\n",
    "    # </think>\n",
    "    part_5 = \"</think>\"\n",
    "    content_compression_mask += [0]\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_6,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [3]\n",
    "\n",
    "    part_7 = \"<｜end▁of▁sentence｜>\"\n",
    "    content_compression_mask += [0]\n",
    "\n",
    "    complete_prompt = \"\"\n",
    "    for part in [\n",
    "        part_1,\n",
    "        part_2,\n",
    "        part_3,\n",
    "        part_4,\n",
    "        part_5,\n",
    "        part_6,\n",
    "        part_7,\n",
    "    ]:\n",
    "        complete_prompt += part\n",
    "    original_tokens = tokenizer.encode(\n",
    "        complete_prompt,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    attention_mask = len(original_tokens) * [1]\n",
    "    # print(complete_prompt)\n",
    "    return {\n",
    "        \"input_ids\": original_tokens,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"content_compression_mask\": content_compression_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "pos = 0\n",
    "question = dataset[pos][\"question\"]\n",
    "answer = dataset[pos][\"answer\"]\n",
    "example = tokenize_single_turn(\n",
    "    question=question,\n",
    "    answer=answer,\n",
    ")\n",
    "print(tokenizer.decode(example[\"input_ids\"]))\n",
    "print(example[\"content_compression_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1114, 1114, 1114)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[\"input_ids\"]), len(example[\"attention_mask\"]), len(\n",
    "    example[\"content_compression_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in range(1, 4):\n",
    "    content_tokens = np.array(example[\"input_ids\"])[\n",
    "        np.array(example[\"content_compression_mask\"]) == i\n",
    "    ]\n",
    "    if i == 1:\n",
    "        print(tokenizer.decode(content_tokens) == question)\n",
    "    elif i == 2:\n",
    "        print(tokenizer.decode(content_tokens) == answer.split(\"</think>\")[0])\n",
    "    elif i == 3:\n",
    "        print(tokenizer.decode(content_tokens) == answer.split(\"</think>\")[1])\n",
    "# print(answer.split(\"</think>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151649]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|fim_pad|>\", add_special_tokens=False)\n",
    "tokenizer.encode(\"</think>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151645: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151646: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151653]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|vision_end|>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- сжимаем последовательности последовательно\n",
    "- К примеру сначала мы сжимаем первые последовательные 12 токенов, потом первые 24 и тд\n",
    "- чтобы на инференсе мы могли просто после каждой генерации 12 токенов, сжать ее и генерировать дальше  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.ceil(3 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from more_itertools import chunked, collapse\n",
    "\n",
    "list(collapse([[1], [2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from more_itertools import chunked, collapse\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "text_token_id = tokenizer.encode(\"<|fim_pad|>\", add_special_tokens=False)[0]\n",
    "eos_token_id = tokenizer.encode(\"<｜end▁of▁sentence｜>\", add_special_tokens=False)[0]\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "batch = 1\n",
    "\n",
    "dataset_batch = [dataset[i] for i in range(batch)]\n",
    "dataset_batch = [tokenize_single_turn(**item) for item in dataset_batch]\n",
    "aligned_batch = []\n",
    "\n",
    "\n",
    "def adjust_content_compression_mask(\n",
    "    input_ids: list,\n",
    "    content_label: int,\n",
    "    content_mask: list,\n",
    "    window_size: int,\n",
    "):\n",
    "    content_part = input_ids[content_mask == content_label]\n",
    "    total_parts = len(content_part) // window_size\n",
    "    new_len_part_1 = total_parts * window_size\n",
    "    mask_pos = np.where(content_mask == content_label)[0]\n",
    "    mask_start_pos = mask_pos[0]\n",
    "    mask_end_pos = mask_pos[-1]\n",
    "    # print(content_mask.tolist())\n",
    "    content_mask[\n",
    "        mask_end_pos - (len(content_part) - new_len_part_1) + 1 : mask_end_pos + 1\n",
    "    ] = 0\n",
    "    # print(tokenizer.decode(input_ids[mask_start_pos : mask_end_pos + 1]))\n",
    "    # print(\"===\")\n",
    "    # print(\"===\")\n",
    "\n",
    "\n",
    "# определяем какие именно элементы мы хотим сжимать,а какие\n",
    "# не помещаются не помещаются в чанк размером window_size\n",
    "# for tokens in dataset_batch:\n",
    "#     input_ids = np.array(tokens[\"input_ids\"])\n",
    "#     content_mask = np.array(tokens[\"content_compression_mask\"])\n",
    "#     content_labels = [1, 2, 3]\n",
    "#     for content_label in content_labels:\n",
    "#         adjust_content_compression_mask(\n",
    "#             input_ids=input_ids,\n",
    "#             content_label=content_label,\n",
    "#             content_mask=content_mask,\n",
    "#             window_size=window_size,\n",
    "#         )\n",
    "\n",
    "#     aligned_batch.append(\n",
    "#         {\n",
    "#             \"input_ids\": tokens[\"input_ids\"],\n",
    "#             \"content_compression_mask\": content_mask,\n",
    "#             \"attention_mask\": tokens[\"attention_mask\"],\n",
    "#         }\n",
    "#     )\n",
    "# break\n",
    "\n",
    "VISION_START = 151652\n",
    "VISION_END = 151653\n",
    "EOS_TOKEN_ID = 151643\n",
    "END_THINK_ID = 151649\n",
    "train_examples = []\n",
    "train_examples_amount = 1\n",
    "# Те токены которые не помещаются в window_size нам нужно забить EOS_TOKEN_ID\n",
    "# чтобы при сжатии они ничем не отличались от полных частей текста\n",
    "for tokens in dataset_batch:\n",
    "    input_ids = np.array(tokens[\"input_ids\"])\n",
    "    content_mask = np.array(tokens[\"content_compression_mask\"])\n",
    "\n",
    "    # рассуждение\n",
    "    mask_pos_2 = np.where(content_mask == 2)[0]\n",
    "    start_pos_2 = mask_pos_2[0]\n",
    "    end_pos_2 = mask_pos_2[-1]\n",
    "    # ответ\n",
    "    mask_pos_3 = np.where(content_mask == 3)[0]\n",
    "    start_pos_3 = mask_pos_3[0]\n",
    "    end_pos_3 = mask_pos_3[-1]\n",
    "\n",
    "    part_2_chunks_pad = []\n",
    "    for part_2 in chunked(input_ids[mask_pos_2].tolist(), window_size):\n",
    "        if len(part_2) % window_size != 0:\n",
    "            part_2 += (window_size - len(part_2)) * [EOS_TOKEN_ID]\n",
    "        part_2_chunks_pad.append(part_2)\n",
    "    part_2_chunks = list(chunked(input_ids[mask_pos_2].tolist(), window_size))\n",
    "\n",
    "    part_3_chunks = []\n",
    "    # сжимаем все части кроме последнего чанка\n",
    "    part_3_chunks = list(chunked(input_ids[mask_pos_3].tolist(), window_size))\n",
    "    total_chunks = part_2_chunks + part_3_chunks\n",
    "    for compress_parts_amount in range(1, len(total_chunks) - 1):\n",
    "        if compress_parts_amount <= len(part_2_chunks):\n",
    "            if compress_parts_amount == len(part_2_chunks):\n",
    "                part_2_chunks = part_2_chunks_pad\n",
    "            compress_tokens = [text_token_id] * compress_parts_amount\n",
    "            compress_tokens = [VISION_START] + compress_tokens + [VISION_END]\n",
    "            compressed_input_ids = input_ids[:start_pos_2].tolist()\n",
    "            compressed_input_ids += compress_tokens\n",
    "\n",
    "            replaced_original_tokens = input_ids[:start_pos_2].tolist()\n",
    "            replaced_original_tokens += (\n",
    "                [VISION_START]\n",
    "                + compress_parts_amount * [text_token_id] * window_size\n",
    "                + [VISION_END]\n",
    "            )\n",
    "\n",
    "            new_original_tokens = input_ids[:start_pos_2].tolist()\n",
    "            new_original_tokens += (\n",
    "                [VISION_START]\n",
    "                + list(collapse(part_2_chunks[:compress_parts_amount]))\n",
    "                + [VISION_END]\n",
    "            )\n",
    "\n",
    "            remain_tokens_2 = list(collapse(part_2_chunks[compress_parts_amount:]))\n",
    "            compressed_input_ids += remain_tokens_2\n",
    "            compressed_input_ids += [END_THINK_ID]\n",
    "\n",
    "            replaced_original_tokens += remain_tokens_2\n",
    "            replaced_original_tokens += [END_THINK_ID]\n",
    "\n",
    "            new_original_tokens += remain_tokens_2\n",
    "            new_original_tokens += [END_THINK_ID]\n",
    "\n",
    "            remain_tokens_3 = list(collapse(part_3_chunks))\n",
    "            compressed_input_ids += remain_tokens_3\n",
    "            compressed_input_ids += [EOS_TOKEN_ID]\n",
    "\n",
    "            replaced_original_tokens += remain_tokens_3\n",
    "            replaced_original_tokens += [EOS_TOKEN_ID]\n",
    "\n",
    "            new_original_tokens += remain_tokens_3\n",
    "            new_original_tokens += [EOS_TOKEN_ID]\n",
    "\n",
    "            train_examples.append(\n",
    "                {\n",
    "                    \"replaced_original_tokens\": replaced_original_tokens,\n",
    "                    \"compressed_input_ids\": compressed_input_ids,\n",
    "                    \"original_tokens\": new_original_tokens,\n",
    "                }\n",
    "            )\n",
    "        # но зачем сжимать часть с ответом, она всегда кратно меньше\n",
    "        else:\n",
    "            # print(tokenizer.decode(replaced_original_tokens))\n",
    "            # break\n",
    "            # compressed_input_ids\n",
    "            # сжимаем всю рассуждающую часть\n",
    "            compressed_input_ids = input_ids[:start_pos_2].tolist()\n",
    "            compress_tokens = [text_token_id] * len(part_2_chunks_pad)\n",
    "            compress_tokens = [VISION_START] + compress_tokens + [VISION_END]\n",
    "            compressed_input_ids += compress_tokens\n",
    "            compressed_input_ids += [END_THINK_ID]\n",
    "\n",
    "            # replaced_original_tokens\n",
    "            replaced_original_tokens = input_ids[:start_pos_2].tolist()\n",
    "            replaced_original_tokens += (\n",
    "                [VISION_START]\n",
    "                + [text_token_id] * len(part_2_chunks_pad) * window_size\n",
    "                + [VISION_END]\n",
    "            )\n",
    "            replaced_original_tokens += [END_THINK_ID]\n",
    "\n",
    "            # new_original_tokens\n",
    "            new_original_tokens = input_ids[:start_pos_2].tolist()\n",
    "            new_original_tokens += (\n",
    "                [VISION_START] + list(collapse(part_2_chunks_pad)) + [VISION_END]\n",
    "            )\n",
    "            new_original_tokens += [END_THINK_ID]\n",
    "\n",
    "            # compressed_input_ids\n",
    "            compress_tokens = [text_token_id] * (\n",
    "                compress_parts_amount - len(part_2_chunks_pad)\n",
    "            )\n",
    "            compress_tokens = [VISION_START] + compress_tokens + [VISION_END]\n",
    "            compressed_input_ids += compress_tokens\n",
    "            compressed_input_ids += list(\n",
    "                collapse(\n",
    "                    part_3_chunks[compress_parts_amount - len(part_2_chunks_pad) :]\n",
    "                )\n",
    "            )\n",
    "            compressed_input_ids += [EOS_TOKEN_ID]\n",
    "\n",
    "            # replaced_original_tokens\n",
    "            compress_tokens = (\n",
    "                [text_token_id]\n",
    "                * (compress_parts_amount - len(part_2_chunks_pad))\n",
    "                * window_size\n",
    "            )\n",
    "            replaced_original_tokens += [VISION_START] + compress_tokens + [VISION_END]\n",
    "            replaced_original_tokens += list(\n",
    "                collapse(\n",
    "                    part_3_chunks[compress_parts_amount - len(part_2_chunks_pad) :]\n",
    "                )\n",
    "            )\n",
    "            replaced_original_tokens += [EOS_TOKEN_ID]\n",
    "\n",
    "            # new_original_tokens\n",
    "            compress_tokens = list(\n",
    "                collapse(\n",
    "                    part_3_chunks[: compress_parts_amount - len(part_2_chunks_pad)]\n",
    "                )\n",
    "            )\n",
    "            new_original_tokens += [VISION_START] + compress_tokens + [VISION_END]\n",
    "            new_original_tokens += list(\n",
    "                collapse(\n",
    "                    part_3_chunks[compress_parts_amount - len(part_2_chunks_pad) :]\n",
    "                )\n",
    "            )\n",
    "            new_original_tokens += [EOS_TOKEN_ID]\n",
    "\n",
    "            train_examples.append(\n",
    "                {\n",
    "                    \"replaced_original_tokens\": replaced_original_tokens,\n",
    "                    \"compressed_input_ids\": compressed_input_ids,\n",
    "                    \"original_tokens\": new_original_tokens,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # for part_2_pos in range(len(part_2_chunks)):\n",
    "    #     original_part_2 = part_2_chunks[: part_2_pos + 1]\n",
    "    #     for part_3_pos in range(len(part_3_chunks) - 1):\n",
    "    #         pass\n",
    "    # print(part_2_chunks)\n",
    "\n",
    "    # print(tokenizer.decode(part_3_chunks[0]))\n",
    "\n",
    "    # else:\n",
    "    # for compression_chunks_amount in range(think_parts_amount):\n",
    "    #     # фукусируемся на сжатии ответа модели, выбираем рассуждение и ответ\n",
    "\n",
    "    #     # рассуждение\n",
    "    #     mask_pos_2 = np.where(content_mask == 2)[0]\n",
    "    #     start_pos_2 = mask_pos_2[0]\n",
    "    #     end_pos_2 = mask_pos_2[-1]\n",
    "    #     # ответ\n",
    "    #     mask_pos_3 = np.where(content_mask == 3)[0]\n",
    "    #     start_pos_3 = mask_pos_3[0]\n",
    "    #     end_pos_3 = mask_pos_3[-1]\n",
    "\n",
    "    #     input_ids[\n",
    "    #         start_pos_2 : start_pos_2 + (compression_chunks_amount + 1) * window_size\n",
    "    #     ] = text_token_id\n",
    "    #     compressed_input_ids = input_ids[:start_pos_2].tolist()\n",
    "    #     compressed_input_ids += [VISION_START]\n",
    "    #     complete_chunks = chunked()\n",
    "    #     compressed_input_ids += [text_token_id] * (compression_chunks_amount + 1)\n",
    "    #     compressed_input_ids += [VISION_END]\n",
    "    #     compressed_input_ids += input_ids[\n",
    "    #         start_pos_2\n",
    "    #         + (compression_chunks_amount + 1) * window_size : start_pos_3\n",
    "    #         + 1\n",
    "    #     ].tolist()\n",
    "    #     # print(text_token_id)\n",
    "    #     # print(\" \".join(f\"{num:>{8}}\" for num in content_mask.tolist()))\n",
    "    #     # print(\" \".join(f\"{num:>{8}}\" for num in input_ids.tolist()))\n",
    "    #     # print(\" \".join(f\"{num:>{8}}\" for num in compressed_input_ids))\n",
    "    #     # print(\"===\")\n",
    "    #     train_examples.append(\n",
    "    #         {\n",
    "    #             \"replaced_original_tokens\": input_ids.tolist(),\n",
    "    #             \"compressed_input_ids\": compressed_input_ids,\n",
    "    #             \"original_tokens\": tokens[\"input_ids\"],\n",
    "    #         }\n",
    "    #     )\n",
    "    # break\n",
    "\n",
    "train_examples = train_examples[60:62]\n",
    "# pad to the same length\n",
    "new_inputs = {}\n",
    "for item in train_examples:\n",
    "    for key, value in item.items():\n",
    "        if not key in new_inputs:\n",
    "            new_inputs[key] = []\n",
    "        new_inputs[key].append(value)\n",
    "\n",
    "\n",
    "for key, value in new_inputs.items():\n",
    "    new_inputs[key] = tokenizer.pad(\n",
    "        {\n",
    "            \"input_ids\": new_inputs[key],\n",
    "        },\n",
    "        padding=True,\n",
    "        # return_tensors=\"pt\",\n",
    "    )\n",
    "# new_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    np.array(replaced_original_tokens)[\n",
    "        np.array(replaced_original_tokens) == text_token_id\n",
    "    ]\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can't complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio's death came after Peterson's fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\n",
      "\n",
      "Write an article based on these highlights.<｜Assistant｜><think>\n",
      "<|vision_start|>Okay, so I need to write an article based on the highlights provided about Drew Peterson and his fourth wife, Kathleen Savio. The user has given me some specific points to include, so I should make sure to cover all of them.\n",
      "\n",
      "First, I should start by introducing Drew Peterson and his fourth wife, Kathleen Savio. I need to mention that he's been arrested in the deaths of his first and fourth wives. That's a significant event because it's the second time he's been involved in such a death, which could be a big deal.\n",
      "\n",
      "I should also note that the renewed interest in Savio's death came after his fourth wife disappeared. That's an important detail because it shows that the case isn't over and that there's still a lot to explore.\n",
      "\n",
      "Next, the user mentioned that Peterson's attorney has denied any wrongdoing in either case. That's crucial because it sets the tone that the case is being handled fairly, and there's no evidence of wrongdoing. I should make sure to highlight that part.\n",
      "\n",
      "I should also touch on the legal implications. Peterson is facing charges related to the deaths, and his attorney's denial is a positive sign. Maybe I can mention that the case is being handled with fairness and transparency.\n",
      "\n",
      "I need to structure the article in a way that flows well. Maybe start with an introduction about Peterson and his fourth wife, then move on to the legal developments, the renewed interest, and the attorney's denial. Finally, conclude with the implications of the case.\n",
      "\n",
      "I should also consider the tone. It should be informative but not too technical, so it's accessible to a general audience. I'll need to make sure the language is clear and concise.\n",
      "\n",
      "I should also think about any potential counterarguments or questions that might arise. For example, why is there renewed interest in Savio's death? It could be because of the fourth wife's disappearance, which is a significant event. Maybe I can mention that as a point of interest.\n",
      "\n",
      "I should also ensure that the article doesn't just list the facts but also provides context and analysis. For example, discussing the legal process, the significance of the deaths, and the impact on the community.\n",
      "\n",
      "I need to make sure that the article is balanced, presenting both the positive developments and any potential challenges. Maybe mention that while the case is being handled fairly, there could be future legal challenges, but for now, it's a positive situation.\n",
      "\n",
      "I should also check for any recent developments or updates. The user didn't provide specific dates, so I'll have to keep it general, mentioning that the case is ongoing and that the attorney's denial is a key point.\n",
      "\n",
      "Finally, I should conclude by summarizing the main points and emphasizing the importance of the case in the community. Maybe suggest that the legal community should support Peterson and his attorney in handling the case.\n",
      "\n",
      "I think that's a good outline. Now, I'll structure the article accordingly, making sure each section flows into the next and covers all the key points the user provided.\n",
      "<｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><|vision_end|></think><|vision_start|>\n",
      "\n",
      "**Drew Peterson's Fourth Wife: A Legal Journey and Renewed Interest**\n",
      "\n",
      "Drew Peterson, a controversial figure known for his controversial past, has recently come to light as the subject of legal proceedings involving his fourth wife, Kathleen Savio. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges related to these tragic incidents. His attorney has denied any wrongdoing in either case, presenting a positive narrative for the legal community.\n",
      "\n",
      "**Introduction to Drew Peterson and His Fourth Wife**\n",
      "\n",
      "Drew Peterson, a controversial figure, has been the subject of legal scrutiny for his past actions, including his controversial death of his first wife, Jane Savio. Now, his fourth wife, Kathleen Savio, has come into the limelight. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges of murder and multiple counts of intentional killing. His attorney has denied any wrongdoing in either case, highlighting a positive stance in the legal community.\n",
      "\n",
      "**Legal Developments and Renewed Interest**\n",
      "\n",
      "Peterson's attorney has denied any wrongdoing in either case, emphasizing that the deaths are not isolated incidents. The renewed interest in Savio's death stems from the fact that his fourth wife disappeared, adding another layer of complexity to the case. This disappearance has sparked discussions about the legal process and the potential impact on the community.\n",
      "\n",
      "**The Case's Legal Implications**\n",
      "\n",
      "Peterson is facing charges of murder and multiple counts of intentional killing, with a possible charge of conspiracy. His attorney has denied any wrongdoing, presenting a positive narrative. The case is being handled with fairness and transparency, reflecting the legal community's commitment to resolving the issue without any evidence of wrongdoing.\n",
      "\n",
      "**Conclusion and Implications**\n",
      "\n",
      "While the case is being handled fairly, there could be future legal challenges. However, for now, the legal community should support Peterson and his attorney in handling the case. The renewed interest in Savio's death highlights the ongoing legal battle and the importance of addressing the complexities of the case. The legal community should<|vision_end|> continue to support Peterson and his attorney in resolving the issue, ensuring justice is served.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.decode(train_examples[60 + 3][\"original_tokens\"]))\n",
    "print(tokenizer.decode(train_examples[101][\"original_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(train_examples[0]['original_tokens']))\n",
    "# print(tokenizer.decode(train_examples[60+5]['compressed_input_ids']))\n",
    "print(tokenizer.decode(train_examples[60 + 5][\"replaced_original_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151662 151643\n",
      "  151643   151643   151646   151644    20594       25    39791      311     3687      389    65045    32568       11    26179       25      330       40     2684      279     1501      287       13     2980      944    27911        1    40108    39791    12517      304      279     4545      315      806     4843     7403       11    64063    20079      815       13    48986      291     2734      304    20079      815      594     4545     3697     1283    39791      594    11737     7403    28396       13    39791       11     1526      806    13747       11    46491      894    64228      304     2987     1142      382     7985      458     4549     3118      389     1493    21314       13   151645   151648      198   151652   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151662   151653   151649      271      334       35     4266    39791      594    35074    42408       25      362    24678    42580      323    48986      291    24106    56177       35     4266    39791       11      264    20129     7071     3881      369      806    20129     3267       11      702     5926     2525      311     3100      438      279     3832      315     5777    28307    15860      806    11737     7403       11    64063    20079      815       13    39791       11      879      702     1012    12517      304      279    16375      315      806     1156      323    11737    38620       11      702    16601    10195     5435      311     1493    34179    23546       13     5301    13747      702    14820      894    64228      304     2987     1142       11    31544      264     6785    19221      369      279     5777     3942      382      334    37155      311    40108    39791      323     5301    35074    42408    56177       35     4266    39791       11      264    20129     7071       11      702     1012      279     3832      315     5777    35652      369      806     3267     6168       11     2670      806    20129     4545      315      806     1156     7403       11    21475    20079      815       13     4695       11      806    11737     7403       11    64063    20079      815       11      702     2525     1119      279     4568    88243       13    39791       11      879      702     1012    12517      304      279    16375      315      806     1156      323    11737    38620       11      702    16601    10195      315     9901      323     5248    14579      315    46864    13118       13     5301    13747      702    14820      894    64228      304     2987     1142       11    38586      264     6785    28931      304      279     5777     3942      382      334    52786     7843     1368      323    48986      291    24106    56177    34819     1270      594    13747      702    14820      894    64228      304     2987     1142       11    80903      429      279    16375      525      537    24203    23546       13      576    35546     2734      304    20079      815      594     4545    43714      504      279     2097      429      806    11737     7403    28396       11     7842     2441     6193      315    23094      311      279     1142       13     1096    51879      702    40444    20333      911      279     5777     1882      323      279     4650     5421      389      279     3942      382      334      785    11538      594    24678    14390    10709    56177    34819     1270      374    12880    10195      315     9901      323     5248    14579      315    46864    13118       11      448      264     3204     6757      315    25292       13     5301    13747      702    14820      894    64228       11    31544      264     6785    19221       13      576     1142      374     1660    17608      448    50741      323    27231       11    41752      279     5777     3942      594    15155      311    52483      279     4265     2041      894     5904      315    64228      382      334    43434      323    14390    10709    56177     7983      279     1142      374     1660    17608    14138       11     1052     1410      387     3853     5777    11513       13     4354       11      369     1431       11      279     5777     3942     1265     1824    39791      323      806    13747      304    11589      279     1142       13      576    35546     2734      304    20079      815      594     4545    21314      279    14195     5777     8049      323      279    12650      315    27020      279    83040      315      279     1142       13      576     5777     3942     1265     3060      311     1824    39791      323      806    13747      304    52483      279     4265       11    22573    12161      374    10223       13   151643\n",
      "       0        0        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1\n"
     ]
    }
   ],
   "source": [
    "batch_index = 0\n",
    "print(text_token_id, eos_token_id)\n",
    "print(\n",
    "    \" \".join(\n",
    "        f\"{num:>{8}}\"\n",
    "        for num in new_inputs[\"replaced_original_tokens\"][\"input_ids\"][batch_index]\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \" \".join(\n",
    "        f\"{num:>{8}}\"\n",
    "        for num in new_inputs[\"replaced_original_tokens\"][\"attention_mask\"][batch_index]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['replaced_original_tokens', 'compressed_input_ids', 'original_tokens'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 571, 1536]),\n",
       " torch.Size([2, 1122, 1536]),\n",
       " torch.Size([2, 1122, 1536]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "original_tokens_torch = torch.tensor(\n",
    "    new_inputs[\"original_tokens\"][\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "replaced_tokens_torch = torch.tensor(\n",
    "    new_inputs[\"replaced_original_tokens\"][\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "compressed_tokens_torch = torch.tensor(\n",
    "    new_inputs[\"compressed_input_ids\"][\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "# torch.Size([2, 51, 896])\n",
    "original_embeds = model.get_input_embeddings()(original_tokens_torch)\n",
    "replaced_embeds = model.get_input_embeddings()(replaced_tokens_torch)\n",
    "# torch.Size([2, 35, 896]) 51 - 3*4*2 + 4*2\n",
    "compressed_embeds_template = model.get_input_embeddings()(compressed_tokens_torch)\n",
    "compressed_embeds_template.shape, original_embeds.shape, replaced_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1230, 1536]), torch.Size([123]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_for_compression_mask = replaced_tokens_torch == text_token_id\n",
    "compressed_tokens_mask = compressed_tokens_torch == text_token_id\n",
    "# original_tokens_torch[tokens_for_compression_mask].shape\n",
    "original_embeds[tokens_for_compression_mask].shape, compressed_tokens_torch[\n",
    "    compressed_tokens_mask\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([123, 10, 1536])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds_for_compression = original_embeds[tokens_for_compression_mask].reshape(\n",
    "    -1,\n",
    "    window_size,\n",
    "    original_embeds.shape[-1],\n",
    ")\n",
    "embeds_for_compression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2ModelEmbedPoolerV2(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds):\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )[0]\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.sum(1) / torch.tensor(\n",
    "            input_embeds.shape[1],\n",
    "            device=input_embeds.device,\n",
    "        )\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.unsqueeze(1)\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "embed_pooler_v3 = Qwen2ModelEmbedPoolerV2.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([123, 1, 1536]), torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_embeds = embed_pooler_v3(embeds_for_compression)\n",
    "# pooled_embeds = pooled_embeds.reshape(pooled_embeds.shape[0], -1)\n",
    "pooled_embeds.shape, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(original_tokens_torch == VISION_START) | (original_tokens_torch == VISION_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скопировать эмбединги начала и конца в оригинальные\n",
    "from hidden_capacity_reasoning.utils import VISION_START, VISION_END\n",
    "\n",
    "compressed_special_tokens_mask = (compressed_tokens_torch == VISION_START) | (\n",
    "    compressed_tokens_torch == VISION_END\n",
    ")\n",
    "compressed_special_tokens = compressed_tokens_torch[compressed_special_tokens_mask]\n",
    "original_special_embeds = embed_pooler_v3.get_input_embeddings()(\n",
    "    compressed_special_tokens\n",
    ")\n",
    "compressed_embeds_template = compressed_embeds_template.masked_scatter_(\n",
    "    compressed_special_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "    original_special_embeds,\n",
    ")\n",
    "# tokenizer.decode(compressed_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template = compressed_embeds_template.masked_scatter_(\n",
    "    compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "    pooled_embeds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 571, 1536])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем что эмбединги скопировались куда надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_pos = int(torch.where(compressed_tokens_mask[0])[0][0].cpu())\n",
    "(compressed_embeds_template[0][emb_pos] == pooled_embeds[0].view(-1)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = compressed_tokens_torch.clone()\n",
    "labels[labels == text_token_id] = -100\n",
    "labels[labels == eos_token_id] = -100\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3519, device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    inputs_embeds=compressed_embeds_template,\n",
    "    labels=labels,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "max_steps = 400\n",
    "for _ in range(max_steps):\n",
    "    logits = model(\n",
    "        input_ids=generated_tokens,\n",
    "    ).logits\n",
    "    top_token = logits.argmax(-1)[-1][-1]\n",
    "    generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "    # break\n",
    "# print(tokenizer.decode(generated_tokens[-1]))\n",
    "# break\n",
    "\n",
    "default_generation_tokens = generated_tokens[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeds text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "max_steps = 400\n",
    "for _ in range(max_steps):\n",
    "    logits = model(\n",
    "        inputs_embeds=generated_embeds,\n",
    "    ).logits\n",
    "    top_token = logits.argmax(-1)[-1][-1]\n",
    "    top_token_embed = model.get_input_embeddings()(top_token)\n",
    "    # print(top)\n",
    "    generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "    generated_embeds = torch.cat(\n",
    "        [generated_embeds, top_token_embed.reshape(1, 1, -1)], dim=1\n",
    "    )\n",
    "    # break\n",
    "# print(tokenizer.decode(generated_tokens[-1]))\n",
    "# break\n",
    "embeds_generation_tokens = generated_tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embeds_generation_tokens == default_generation_tokens).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generation with compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>2+2*10<｜Assistant｜><think>\n",
      "First, I need andy is.\n",
      "\n",
      "Next, is is .\n",
      "\n",
      " is  ?\n",
      "\n",
      " is \n",
      "\n",
      " is  ?\n",
      "\n",
      " is  ?\n",
      "\n",
      " is  ?\n",
      "\n",
      " is  ?\n",
      "\n",
      " is  ?\n",
      "\n",
      " is  AND is  AND is  AND is  AND is  AND is  AND is  AND is  AND is  AND is  AND Is  AND Is  AND Is  AND Is  AND Is  AND Is  AND Is  AND Is  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "max_steps = 200\n",
    "temp_gen_size = 0\n",
    "compression_started = False\n",
    "window_size = 4\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    if temp_gen_size == window_size:\n",
    "        new_embeds_for_compression = generated_embeds[:, -4:]\n",
    "        compressed_part = embed_pooler_v3(generated_embeds[:, -window_size:])\n",
    "        generated_embeds = torch.cat([generated_embeds, compressed_part], dim=1)\n",
    "        temp_gen_size = 1\n",
    "\n",
    "    logits = model(\n",
    "        inputs_embeds=generated_embeds,\n",
    "    ).logits\n",
    "    top_token = logits.argmax(-1)[-1][-1]\n",
    "    top_token_embed = model.get_input_embeddings()(top_token)\n",
    "    # print(top)\n",
    "    generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "    generated_embeds = torch.cat(\n",
    "        [generated_embeds, top_token_embed.reshape(1, 1, -1)], dim=1\n",
    "    )\n",
    "    temp_gen_size += 1\n",
    "\n",
    "print(tokenizer.decode(generated_tokens[-1]))\n",
    "\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_embeds[:, -window_size:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1536])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_pooler_v3(generated_embeds[:, -window_size:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 415, 1536])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 416, 1536])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_part = embed_pooler_v3(generated_embeds[:, -window_size:])\n",
    "print(generated_embeds.shape)\n",
    "generated_embeds = torch.cat([generated_embeds, compressed_part], dim=1)\n",
    "generated_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151646, 151644,     17,     10,     17,      9,     16]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "generated_tokens[:, :-4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
