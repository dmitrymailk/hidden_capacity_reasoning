{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings().weight.dtype, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A large language model is a type of artificial intelligence that can generate human-like text, such as written or spoken words, based on input data. These models are typically trained using large amounts of data and have been shown to perform well in a variety of tasks, including generating new content, answering questions, and performing natural language processing tasks.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model..1\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 30, 896]), torch.bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    **model_inputs,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.hidden_states[-1].shape, model_output_1.hidden_states[-1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30, 896])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_2 = model(\n",
    "    inputs_embeds=torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    attention_mask=model_inputs[\"attention_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_2.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 896])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 896])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1.hidden_states[-1].reshape(10, 3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 30, 896]), torch.bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_2 = model(\n",
    "    inputs_embeds=torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    attention_mask=model_inputs[\"attention_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_2.hidden_states[-1].shape, model_output_2.hidden_states[-1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 896])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_3 = model(\n",
    "    inputs_embeds=model_output_1.hidden_states[-1].reshape(10, 3, -1),\n",
    "    # attention_mask=model_inputs[\"attention_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_3.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 3, 896])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1.hidden_states[-1].reshape(1, 10, 3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(\n",
    "#     inputs_embeds=model_output_1.hidden_states[-1].reshape(1, 10, 3, -1),\n",
    "#     attention_mask=model_inputs[\"attention_mask\"],\n",
    "#     output_hidden_states=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 896])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Qwen2ModelEmbedPooler(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config).cuda()\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds, attention_mask, window_size=3):\n",
    "        # разбиваем входящие эмбединги на бакеты и усредняем их\n",
    "        sum_mask = attention_mask.reshape(\n",
    "            attention_mask.shape[0],\n",
    "            window_size,\n",
    "            attention_mask.shape[1] // window_size,\n",
    "            -1,\n",
    "        ).sum(1)\n",
    "        embeds_sum = input_embeds.reshape(\n",
    "            attention_mask.shape[0],\n",
    "            window_size,\n",
    "            attention_mask.shape[1] // window_size,\n",
    "            -1,\n",
    "        ).sum(1)\n",
    "        input_embeds = embeds_sum / sum_mask\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "embed_pooler = Qwen2ModelEmbedPooler.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "result = embed_pooler(\n",
    "    # model_output_1.hidden_states[-1],\n",
    "    torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    torch.cat(\n",
    "        [\n",
    "            model_inputs[\"attention_mask\"],\n",
    "            model_inputs[\"attention_mask\"],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    ")\n",
    "result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 896])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3, 4, 5, 6]).reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from transformers.cache_utils import (\n",
    "    Cache,\n",
    "    DynamicCache,\n",
    "    SlidingWindowCache,\n",
    "    StaticCache,\n",
    ")\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.utils import LossKwargs\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "\n",
    "\n",
    "class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n",
    "\n",
    "\n",
    "class Qwen2ForCausalEmbedModeling(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = torch.nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.embed_pooler = Qwen2ModelEmbedPooler.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        # pooled_mask: Optional[torch.Tensor] = None,\n",
    "        **kwargs: Unpack[KwargsForCausalLM],\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        inputs_embeds_tokens = self.model.embed_tokens(input_ids)\n",
    "        # if pixel_values is not None:\n",
    "        #     pixel_values = pixel_values.type(self.visual.get_dtype())\n",
    "        #     image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "        #     n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n",
    "        #     n_image_features = image_embeds.shape[0]\n",
    "        #     if n_image_tokens != n_image_features:\n",
    "        #         raise ValueError(\n",
    "        #             f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "        #         )\n",
    "        #     image_mask = (\n",
    "        #         (input_ids == self.config.image_token_id)\n",
    "        #         .unsqueeze(-1)\n",
    "        #         .expand_as(inputs_embeds)\n",
    "        #         .to(inputs_embeds.device)\n",
    "        #     )\n",
    "        #     image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "        #     inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "        window_size = torch.tensor(\n",
    "            [\n",
    "                [4],\n",
    "                [3],\n",
    "            ]\n",
    "        ).cuda()\n",
    "        tokens_amount = torch.tensor(\n",
    "            [\n",
    "                [2],\n",
    "                [4],\n",
    "            ]\n",
    "        ).cuda()\n",
    "        lengths = window_size * tokens_amount\n",
    "        # max_len = lengths.max()\n",
    "        max_len = inputs_embeds_tokens.shape[1]\n",
    "        batch_size = window_size.shape[0]\n",
    "        pooled_mask = (\n",
    "            torch.arange(max_len, device=device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(batch_size, max_len)\n",
    "        )\n",
    "        pooled_mask = (lengths >= pooled_mask).to(torch.long)\n",
    "        pooled_embeds = inputs_embeds * pooled_mask.to(inputs_embeds.dtype)\n",
    "        pooled_embeds = self.embed_pooler(pooled_embeds, pooled_mask)\n",
    "        embed_mask = (\n",
    "            (input_ids == self.config.image_token_id)\n",
    "            .unsqueeze(-1)\n",
    "            .expand_as(inputs_embeds)\n",
    "            .to(inputs_embeds.device)\n",
    "        )\n",
    "        inputs_embeds = inputs_embeds.masked_scatter(embed_mask, pooled_embeds)\n",
    "\n",
    "        # Из-за смешанной структуры, мы будем всегда подавать только эмбединги\n",
    "        # Идея позаимствована из qwen2vl\n",
    "        outputs = self.model(\n",
    "            input_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = (\n",
    "            slice(-logits_to_keep, None)\n",
    "            if isinstance(logits_to_keep, int)\n",
    "            else logits_to_keep\n",
    "        )\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(\n",
    "                logits=logits,\n",
    "                labels=labels,\n",
    "                vocab_size=self.config.vocab_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование концепции qwen2vl для текстовых токенов и эмбедингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = \"Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?\"\n",
    "\n",
    "tokenizer.encode(text_example)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151655]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|image_pad|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1318, 39304]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"text_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Случай первый - мы сжимаем только текстовые токены (максимально упрощенный вариант)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|>ized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 3\n",
    "chunk_size = 4\n",
    "new_tokens = []\n",
    "original_tokens = tokenizer.encode(text_example)\n",
    "new_tokens += tokenizer.encode(\"<|object_ref_start|>\") * chunk_size\n",
    "new_tokens += original_tokens[chunk_size * window_size :]\n",
    "tokenizer.decode(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(original_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 43, 896]), torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tokens_torch = torch.tensor(\n",
    "    [\n",
    "        original_tokens,\n",
    "        original_tokens,\n",
    "    ],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "new_tokens_torch = torch.tensor(\n",
    "    [\n",
    "        new_tokens,\n",
    "        new_tokens,\n",
    "    ],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "# torch.Size([2, 51, 896])\n",
    "original_embeds = model.get_input_embeddings()(original_tokens_torch)\n",
    "# torch.Size([2, 43, 896]) 51 - 3*4 + 4\n",
    "compressed_embeds_template = model.get_input_embeddings()(new_tokens_torch)\n",
    "compressed_embeds_template.shape, compressed_embeds_template.dtype, original_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 43, 896]), torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Qwen2ModelEmbedPoolerV2(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds):\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )[0]\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.sum(1) / torch.tensor(\n",
    "            input_embeds.shape[1],\n",
    "            device=input_embeds.device,\n",
    "        )\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.unsqueeze(1)\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "embed_pooler_v2 = Qwen2ModelEmbedPoolerV2.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# torch.Size([8, 3, 896])\n",
    "compressed_embeds = original_embeds[:, : chunk_size * window_size].reshape(\n",
    "    chunk_size * original_embeds.shape[0],\n",
    "    window_size,\n",
    "    -1,\n",
    ")\n",
    "compressed_embeds.shape\n",
    "# torch.Size([8, 1, 896])\n",
    "pooled_embeds = embed_pooler_v2(compressed_embeds)\n",
    "# torch.Size([2, 4, 896])\n",
    "pooled_embeds = pooled_embeds.reshape(\n",
    "    original_embeds.shape[0],\n",
    "    chunk_size,\n",
    "    -1,\n",
    ")\n",
    "# torch.Size([2, 43, 896])\n",
    "compressed_embeds_template[:, :chunk_size] = pooled_embeds\n",
    "compressed_embeds_template.shape, compressed_embeds_template.dtype, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_pooler_v2.model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  1506,   279, 18414,   315, 20443, 11229,\n",
       "           304,  3213,  1635,    13,  5542,   656, 59711,  9331,   311, 34549,\n",
       "         15712,    11,  1181,  8357,   525, 10454, 14756, 70767,    13,  1988,\n",
       "          1128,  6896,   374,  5538,  6832,    30,  1597,  1128,  3643,   432,\n",
       "           773,  7988,    30],\n",
       "        [ -100,  -100,  -100,  -100,  1506,   279, 18414,   315, 20443, 11229,\n",
       "           304,  3213,  1635,    13,  5542,   656, 59711,  9331,   311, 34549,\n",
       "         15712,    11,  1181,  8357,   525, 10454, 14756, 70767,    13,  1988,\n",
       "          1128,  6896,   374,  5538,  6832,    30,  1597,  1128,  3643,   432,\n",
       "           773,  7988,    30]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = new_tokens_torch.clone()\n",
    "text_token_id = tokenizer.encode(\"<|object_ref_start|>\")[0]\n",
    "labels[labels == text_token_id] = -100\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7288, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    inputs_embeds=compressed_embeds_template,\n",
    "    labels=labels,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Усложненная версия токенизации текста. Сжатые токены возникают между обычным тектом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входная последовательность может быть 5 видов.\n",
    "\n",
    "- когда у нас на входе только текст, который мы просто моделируем \n",
    "- когда на входе текст, но мы хотим сжать его некоторые части\n",
    "- когда на входе текст и эмбединги с последнего слоя, где мы хотим сжать только части текста\n",
    "- когда на входе текст и эмбединги с последнего слоя, где мы хотим сжать, часть текста и эмбедингов, совместно (токены переводим в эмбединги и сжимаем вместе с hidden states)\n",
    "- когда на входе текст и эмбединги с последнего слоя, где мы хотим сжать, только эмбединги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы решаем какие токены хотим сжать?\n",
    "- никак. Просто говорим сожми с такого по такой.\n",
    "- это норм только на этапе обучения, так как мы можем перебрать все комбинации.\n",
    "- не норм на этапе инференса. например мы можем сжимать после каждых сгенеренных 10 токенов или 100, 1000. а какое окно контекста? 3, 10, 100? А что если менять стратегию. Сначала мы сжимали с окном 5 токенов, потом 20?\n",
    "- Кажется что эти гиперпараметры можно найти простым перебором на валидации. Однако перебор стратегии уже не кажется таким очевидным. Напрашивается RL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11, 702, 13791, 1506, 279, 18414, 315, 20443, 11229, 304, 3213, 1635, 13, 5542, 656, 59711, 9331, 311, 34549, 15712, 11, 1181, 8357, 525, 10454, 14756, 70767, 13, 1988, 1128, 6896, 374, 5538, 6832, 30, 1597, 1128, 3643, 432, 773, 7988, 30, 220, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643], [10227, 42415, 1119, 279, 330, 32880, 1, 949, 11, 1077, 594, 5695, 264, 16266, 448, 279, 31774, 315, 20443, 29728, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = [\n",
    "    \"Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful? \",\n",
    "    \"\"\"Before diving into the \"deep\" part, let's establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include:\"\"\",\n",
    "]\n",
    "\n",
    "tokenizer.batch_encode_plus(\n",
    "    text_example,\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 122])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(\n",
    "    tokenizer.batch_encode_plus(\n",
    "        text_example,\n",
    "        padding=True,\n",
    "    )[\"input_ids\"]\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(151643)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11, 702, 13791, 1506, 279, 18414, 315, 20443, 11229, 304, 3213, 1635, 13, 5542, 656, 59711, 9331, 311, 34549, 15712, 11, 1181, 8357, 525, 10454, 14756, 70767, 13, 1988, 1128, 6896, 374, 5538, 6832, 30, 1597, 1128, 3643, 432, 773, 7988, 30, 220]\n",
      "4\n",
      "[[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11, 702, 13791], [1506, 279, 18414, 315, 20443, 11229, 304, 3213, 1635, 13, 5542, 656], [59711, 9331, 311, 34549, 15712, 11, 1181, 8357, 525, 10454, 14756, 70767], [13, 1988, 1128, 6896, 374, 5538, 6832, 30, 1597, 1128, 3643, 432], [773, 7988, 30, 220]]\n",
      "chunks_for_tokenization  {2}\n",
      "=== replaced_original_tokens\n",
      "Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|>. But what exactly is deep learning? And what makes it so powerful? \n",
      "=== new_input_tokens\n",
      "Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|>. But what exactly is deep learning? And what makes it so powerful? \n",
      "44\n",
      "==\n",
      "==\n",
      "==\n",
      "[10227, 42415, 1119, 279, 330, 32880, 1, 949, 11, 1077, 594, 5695, 264, 16266, 448, 279, 31774, 315, 20443, 29728, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25]\n",
      "10\n",
      "[[10227, 42415, 1119, 279, 330, 32880, 1, 949, 11, 1077, 594, 5695], [264, 16266, 448, 279, 31774, 315, 20443, 29728, 14155, 320, 12003, 82], [568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645], [304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924], [25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527], [76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617], [13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220], [13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167], [315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220], [13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136], [2924, 25]]\n",
      "chunks_for_tokenization  {7}\n",
      "=== replaced_original_tokens\n",
      "Before diving into the \"deep\" part, let's establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|> of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include:\n",
      "=== new_input_tokens\n",
      "Before diving into the \"deep\" part, let's establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|> of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include:\n",
      "114\n",
      "==\n",
      "==\n",
      "==\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2, 114), (2, 122), (2, 122))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from more_itertools import chunked\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "text_token_id = tokenizer.encode(\"<|object_ref_start|>\")[0]\n",
    "eos_token_id = tokenizer.encode(\"<|endoftext|>\")[0]\n",
    "\n",
    "window_size = 3\n",
    "chunk_size = 4\n",
    "new_tokens = []\n",
    "original_tokens = tokenizer.batch_encode_plus(\n",
    "    text_example,\n",
    "    padding=True,\n",
    ")\n",
    "compressed_tokens = []\n",
    "replaced_original_tokens_batch = []\n",
    "# original_tokens\n",
    "for tokens in original_tokens[\"input_ids\"]:\n",
    "    original_lines = np.array(tokens)\n",
    "    pure_tokens = original_lines[original_lines != eos_token_id].tolist()\n",
    "    print(pure_tokens)\n",
    "    full_chunks_amount = len(pure_tokens) // (window_size * chunk_size)\n",
    "    print(full_chunks_amount)\n",
    "    max_percent = 0.8\n",
    "    pure_tokens_chunks = list(chunked(pure_tokens, window_size * chunk_size))\n",
    "    print(pure_tokens_chunks)\n",
    "    prob = 0.3\n",
    "    random_mask = np.random.random(int(full_chunks_amount * max_percent))\n",
    "    mask = random_mask < prob\n",
    "    chunks_for_tokenization = np.where(mask)[0].tolist()\n",
    "    chunks_for_tokenization = set(chunks_for_tokenization)\n",
    "    if len(chunks_for_tokenization) == 0:\n",
    "        chunks_for_tokenization = set(\n",
    "            [\n",
    "                random.randint(\n",
    "                    0,\n",
    "                    int(full_chunks_amount * max_percent),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    print(\"chunks_for_tokenization \", chunks_for_tokenization)\n",
    "    replaced_original_tokens = []\n",
    "    new_input_tokens = []\n",
    "    for i, tokens in enumerate(pure_tokens_chunks):\n",
    "        if i in chunks_for_tokenization:\n",
    "            replaced_original_tokens.extend([text_token_id] * len(tokens))\n",
    "            new_input_tokens.extend([text_token_id] * chunk_size)\n",
    "        else:\n",
    "            replaced_original_tokens.extend(tokens)\n",
    "            new_input_tokens.extend(tokens)\n",
    "    print(\"=== replaced_original_tokens\")\n",
    "    print(tokenizer.decode(replaced_original_tokens))\n",
    "    print(\"=== new_input_tokens\")\n",
    "    print(tokenizer.decode(new_input_tokens))\n",
    "    print(len(new_input_tokens))\n",
    "    compressed_tokens.append(new_input_tokens)\n",
    "    replaced_original_tokens_batch.append(replaced_original_tokens)\n",
    "    print(\"==\")\n",
    "    print(\"==\")\n",
    "    print(\"==\")\n",
    "\n",
    "compressed_tokens_attention = []\n",
    "max_compressed_len = max([len(item) for item in compressed_tokens])\n",
    "max_replaced_len = max([len(item) for item in replaced_original_tokens_batch])\n",
    "\n",
    "for compressed_seq, replaced_seq in zip(\n",
    "    compressed_tokens,\n",
    "    replaced_original_tokens_batch,\n",
    "):\n",
    "    compressed_seq_len = len(compressed_seq)\n",
    "    replaced_seq_len = len(replaced_seq)\n",
    "    attention_mask = [1] * (compressed_seq_len)\n",
    "\n",
    "    if compressed_seq_len < max_compressed_len:\n",
    "        compressed_seq += [eos_token_id] * (max_compressed_len - compressed_seq_len)\n",
    "        attention_mask += [0] * (max_compressed_len - compressed_seq_len)\n",
    "\n",
    "    if compressed_seq_len < max_replaced_len:\n",
    "        replaced_seq += [eos_token_id] * (max_replaced_len - replaced_seq_len)\n",
    "\n",
    "    compressed_tokens_attention.append(attention_mask)\n",
    "# len(compressed_tokens[0]), len(compressed_tokens[1])\n",
    "# compressed_tokens = torch.tensor(compressed_tokens)\n",
    "# compressed_tokens_attention = torch.tensor(compressed_tokens_attention)\n",
    "np.array(compressed_tokens).shape, np.array(\n",
    "    replaced_original_tokens_batch\n",
    ").shape, np.array(original_tokens[\"input_ids\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 114, 896]),\n",
       " torch.Size([2, 122, 896]),\n",
       " torch.Size([2, 122, 896]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tokens_torch = torch.tensor(\n",
    "    original_tokens[\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "replaced_tokens_torch = torch.tensor(\n",
    "    replaced_original_tokens_batch,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "compressed_tokens_torch = torch.tensor(\n",
    "    compressed_tokens,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "# torch.Size([2, 51, 896])\n",
    "original_embeds = model.get_input_embeddings()(original_tokens_torch)\n",
    "replaced_embeds = model.get_input_embeddings()(replaced_tokens_torch)\n",
    "# torch.Size([2, 35, 896]) 51 - 3*4*2 + 4*2\n",
    "compressed_embeds_template = model.get_input_embeddings()(compressed_tokens_torch)\n",
    "compressed_embeds_template.shape, original_embeds.shape, replaced_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.dtype, original_embeds.dtype, replaced_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151646"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 896]), torch.Size([8]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_for_compression_mask = replaced_tokens_torch == text_token_id\n",
    "compressed_tokens_mask = compressed_tokens_torch == text_token_id\n",
    "# original_tokens_torch[tokens_for_compression_mask].shape\n",
    "original_embeds[tokens_for_compression_mask].shape, compressed_tokens_torch[\n",
    "    compressed_tokens_mask\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 896])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds_for_compression = original_embeds[tokens_for_compression_mask].reshape(\n",
    "    -1,\n",
    "    window_size,\n",
    "    original_embeds.shape[-1],\n",
    ")\n",
    "embeds_for_compression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 896]), torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_embeds = embed_pooler_v2(embeds_for_compression)\n",
    "# pooled_embeds = pooled_embeds.reshape(pooled_embeds.shape[0], -1)\n",
    "pooled_embeds.shape, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1],\n",
       "        [2, 3, 0, 4, 5]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "self = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [1, 1, 0, 1, 1],\n",
    "    ],\n",
    "    dtype=torch.bool,\n",
    ")\n",
    "source = torch.tensor(\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    ")\n",
    "self.masked_scatter_(mask, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98, 896])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_tokens_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98, 896])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_tokens_mask.shape\n",
    "# .expand_as(compressed_embeds_template).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 114, 896])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, torch.float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.dtype, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 114, 896])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.masked_scatter(\n",
    "    compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "    pooled_embeds,\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template = compressed_embeds_template.masked_scatter_(\n",
    "    compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "    pooled_embeds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template[compressed_tokens_mask][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template[compressed_tokens_mask][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = compressed_tokens_torch.clone()\n",
    "text_token_id = tokenizer.encode(\"<|object_ref_start|>\")[0]\n",
    "labels[labels == text_token_id] = -100\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0430, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    inputs_embeds=compressed_embeds_template,\n",
    "    labels=labels,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R1 Chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant.<｜User｜>how many wings has a bird?<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Okay, so I need to figure out how many wings a bird has. I'm not entirely sure, but I know that most birds have two wings. I think that's the common case, but maybe there are some exceptions. Let me think... I remember that some birds have more than two wings, like maybe three or four. For example, I think there's a bird called a trident that has three wings. But wait, I'm not sure if that's accurate. Maybe I should break it down.\\n\\nFirst, I should recall the basic structure of a bird's wings. Wings are part of the bird's airframe, which is the skin and structure that allows the bird to fly. Each wing is made up of several parts, like a root, a chord, and a span. The number of wings can vary depending on the bird's type.\\n\\nI think most birds have two wings. That makes sense because it's the simplest structure and allows for flight. But I've heard that some birds have more wings, especially those that are more specialized or have certain needs. For example, maybe some birds have three wings for more maneuverability or for flight in specific environments like water or under trees.\\n\\nI also remember that some birds have wings that are attached to each other, forming a kind of 'V' shape. I think the bird V is a common feature in birds with more wings, like the trident bird. So, in that case, a trident bird would have three wings arranged in a V shape. That would make sense because it allows for better control and maneuverability.\\n\\nWait, but does every bird have two wings? I think so. Most birds, like eagles, owls, and certain types of parrots, have two wings. But some, like the trident bird, have three. Maybe there are even more specialized birds that have four wings, like some types of penguins or maybe certain types of birds with more complex flight mechanisms.\\n\\nI'm a bit confused about the trident bird. I thought it was a bird from ancient times, maybe from the Middle Ages. I should check that. If it's from ancient times, then it's an old bird with three wings, which would be an interesting case. But I'm not sure if that's accurate. Maybe it's just a fictional bird or a historical example.\\n\\nAnother possibility is that some birds have wings that are not straight. I've heard about birds with folded wings, like the starling or the ostrich. These have a kind of curved or folded wing structure, but I'm not sure if they have more than two wings. Maybe they have two wings, but they're folded in a particular way.\\n\\nI also remember that some birds have wings that are not flat. For example, the cormorant has a curved wing that's more like a bird's beak. But again, I'm not sure if that's a separate case or part of their two-winged structure.\\n\\nSo, putting this all together, I think the main answer is that most birds have two wings, but some, like the trident bird, have three wings. Some even have four wings in more specialized cases. But I'm not entirely sure about the specifics of each case, especially the trident bird. I should probably look up some information to confirm.\\n\\nWait, I think I've heard that the trident bird is actually a type of bird from the 14th century, and it's a fictional creature, not a real bird. So maybe it's more of a historical example rather than a real bird. That would mean that real birds still have two wings, and the trident bird is just a story.\\n\\nIf that's the case, then the answer would be that real birds have two wings, and the trident bird is just a fictional example. So, the number of wings a bird has is typically two, but there are exceptions like the trident bird with three wings.\\n\\nI think I should stick with that understanding for now. So, the answer is that most birds have two wings, but some specialized birds have three wings as well.\\n</think>\\n\\nThe number of wings a bird has is typically two, as this is the common structure for most birds, allowing them to fly. However, there are exceptions, such as the trident bird, which is a fictional historical example from the 14th century. The trident bird has three wings, arranged in a V shape, though it is not a real bird. Therefore, in terms of real birds, most have two wings, with some specialized cases having three wings. \\n\\nAnswer: Most birds have two wings, but some, like the trident bird, have three wings.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"how many wings has a bird?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=1024)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant.<｜User｜>how many wings has a bird?<｜Assistant｜><think>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out how many wings a bird has. I'm not entirely sure, but I know that most birds have two wings. I think that's the common case, but maybe there are some exceptions. Let me think... I remember that some birds have more than two wings, like maybe three or four. For example, I think there's a bird called a trident that has three wings. But wait, I'm not sure if that's accurate. Maybe I should break it down.\n",
      "\n",
      "First, I should recall the basic structure of a bird's wings. Wings are part of the bird's airframe, which is the skin and structure that allows the bird to fly. Each wing is made up of several parts, like a root, a chord, and a span. The number of wings can vary depending on the bird's type.\n",
      "\n",
      "I think most birds have two wings. That makes sense because it's the simplest structure and allows for flight. But I've heard that some birds have more wings, especially those that are more specialized or have certain needs. For example, maybe some birds have three wings for more maneuverability or for flight in specific environments like water or under trees.\n",
      "\n",
      "I also remember that some birds have wings that are attached to each other, forming a kind of 'V' shape. I think the bird V is a common feature in birds with more wings, like the trident bird. So, in that case, a trident bird would have three wings arranged in a V shape. That would make sense because it allows for better control and maneuverability.\n",
      "\n",
      "Wait, but does every bird have two wings? I think so. Most birds, like eagles, owls, and certain types of parrots, have two wings. But some, like the trident bird, have three. Maybe there are even more specialized birds that have four wings, like some types of penguins or maybe certain types of birds with more complex flight mechanisms.\n",
      "\n",
      "I'm a bit confused about the trident bird. I thought it was a bird from ancient times, maybe from the Middle Ages. I should check that. If it's from ancient times, then it's an old bird with three wings, which would be an interesting case. But I'm not sure if that's accurate. Maybe it's just a fictional bird or a historical example.\n",
      "\n",
      "Another possibility is that some birds have wings that are not straight. I've heard about birds with folded wings, like the starling or the ostrich. These have a kind of curved or folded wing structure, but I'm not sure if they have more than two wings. Maybe they have two wings, but they're folded in a particular way.\n",
      "\n",
      "I also remember that some birds have wings that are not flat. For example, the cormorant has a curved wing that's more like a bird's beak. But again, I'm not sure if that's a separate case or part of their two-winged structure.\n",
      "\n",
      "So, putting this all together, I think the main answer is that most birds have two wings, but some, like the trident bird, have three wings. Some even have four wings in more specialized cases. But I'm not entirely sure about the specifics of each case, especially the trident bird. I should probably look up some information to confirm.\n",
      "\n",
      "Wait, I think I've heard that the trident bird is actually a type of bird from the 14th century, and it's a fictional creature, not a real bird. So maybe it's more of a historical example rather than a real bird. That would mean that real birds still have two wings, and the trident bird is just a story.\n",
      "\n",
      "If that's the case, then the answer would be that real birds have two wings, and the trident bird is just a fictional example. So, the number of wings a bird has is typically two, but there are exceptions like the trident bird with three wings.\n",
      "\n",
      "I think I should stick with that understanding for now. So, the answer is that most birds have two wings, but some specialized birds have three wings as well.\n",
      "</think>\n",
      "\n",
      "The number of wings a bird has is typically two, as this is the common structure for most birds, allowing them to fly. However, there are exceptions, such as the trident bird, which is a fictional historical example from the 14th century. The trident bird has three wings, arranged in a V shape, though it is not a real bird. Therefore, in terms of real birds, most have two wings, with some specialized cases having three wings. \n",
      "\n",
      "Answer: Most birds have two wings, but some, like the trident bird, have three wings.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151645: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151646: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151662"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|fim_pad|>\", add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_len 437\n",
      "First, I need to compare the two numbers: 9.11 and 9.9.\n",
      "\n",
      "To make the comparison easier, I'll align their decimal places by writing 9.9 as 9.90.\n",
      "\n",
      "Now, I'll compare each corresponding digit from left to right.\n",
      "\n",
      "Both numbers have 9 in the units place, so they are equal there.\n",
      "\n",
      "Next, I'll look at the tenths place. In 9.11, the tenths digit is 1, while in 9.90, it's 9.\n",
      "\n",
      "Since 9 is greater than 1, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "To determine which number is larger between **9.11** and **9.9**, follow these steps:\n",
      "\n",
      "1. **Align the Decimal Places:**\n",
      "   \n",
      "   To make the comparison easier, write both numbers with the same number of decimal places:\n",
      "   \n",
      "   \\[\n",
      "   9.11 \\quad \\text{and} \\quad 9.90\n",
      "   \\]\n",
      "\n",
      "2. **Compare the Numbers Digit by Digit:**\n",
      "   \n",
      "   - **Units Place:**\n",
      "     \n",
      "     Both numbers have **9** in the units place.\n",
      "     \n",
      "     \\[\n",
      "     9 \\quad \\text{vs} \\quad 9\n",
      "     \\]\n",
      "     \n",
      "     They are equal in this place.\n",
      "\n",
      "   - **Tenths Place:**\n",
      "     \n",
      "     In **9.11**, the tenths digit is **1**.\n",
      "     \n",
      "     In **9.90**, the tenths digit is **9**.\n",
      "     \n",
      "     \\[\n",
      "     1 \\quad \\text{vs} \\quad 9\n",
      "     \\]\n",
      "     \n",
      "     Since **9** is greater than **1**, **9.90** is larger than **9.11**.\n",
      "\n",
      "3. **Conclusion:**\n",
      "   \n",
      "   Therefore, **9.9** is bigger than **9.11**.\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is bigger}}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(f\"http://{open('ip').read()}:1338\")\n",
    "\n",
    "user_prompt = \"9.11 and 9.9 -- which is bigger? Let's think step by step.\"\n",
    "output = client.chat.completions.create(\n",
    "    model=\"tgi\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        },\n",
    "    ],\n",
    "    stream=False,\n",
    "    max_tokens=10000,\n",
    "    temperature=0.0,\n",
    ")\n",
    "result = output.choices[0].message.content\n",
    "print(\"total_len\", len(tokenizer.encode(result)))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: 'Okay, so I\\'m trying to figure out what the user named is. They just said \"Hello, my name is\" and then stopped. Hmm, that\\'s a bit confusing. Maybe they forgot to finish the sentence. I should probably ask them to complete it so I can help better. I don\\'t want to assume anything about their identity or name. It\\'s better to be safe than sorry. I\\'ll let them know I\\'m here to help once they provide their full name.\\n</think>\\n\\nIt seems like your name is incomplete. Could you please provide your full name so I can assist you better?'\n",
      "Prompt: 'The president of the United States is', Generated text: \"Okay, so I need to figure out what the president of the United States is. Hmm, I'm not exactly sure, but I think the president is the head of the executive branch of the United States. I remember hearing that they're usually named after a person, like George W. Bush or Barack Obama. Maybe I should look up the current president to get the most accurate information.\\n\\nWait, I'm not sure if I can access the internet right now. Maybe I can think of some famous presidents and their names. I know George Washington was the first president, and he's often called the first president. Then there's John Adams, who was the second president. After that, Thomas Jefferson was the third president. I think that's right because he was the first president of the states, not the country.\\n\\nAfter that, James Madison and James Monroe were the fourth and fifth presidents, respectively. Then, after that, Abraham Lincoln was the sixth president. He was the first president of the United States, but he was also the first president of the Confederate States. After him, Andrew Jackson was the seventh president. He was a significant figure in the Civil Rights Movement.\\n\\nThen, after Andrew Jackson, Martin Van Buren was the eighth president. He was a statesman and played a role in the Civil War. After him, James K. Polk was the ninth president. He was known for his policies, like the transcontinental railroad. Following Polk, William Henry Harrison was the tenth president, but he died in office, so he's often excluded from the list.\\n\\nAfter Harrison, John Tyler was the eleventh president, and he was a statesman and military leader. Then, Franklin Pierce was the twelfth president. He was a key figure in the Civil War. After that, James Buchanan was the thirteenth president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the fourteenth president, the first president of the United States. Then, Andrew Johnson was the fifteenth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the sixteenth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the seventeenth president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the eighteenth president again, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler\"\n",
      "Prompt: 'The capital of France is', Generated text: \"Okay, so I need to figure out the capital of France. I remember that France is a country in Europe, but I'm not exactly sure where its capital is. I think it's somewhere in the north, maybe in the Pyrenees or something. I've heard of Paris before, but I'm not sure if it's the only capital. I should probably start by recalling what I know about capitals.\\n\\nI know that the capital of France is the most populous city, so it's probably a major city. Paris comes to mind because it's the capital of many countries, but I'm not certain if it's the only one. I think there are other capitals, but I'm not sure which ones. Maybe I should think about the countries that are connected to France. France borders several countries, so maybe each of those has their own capital.\\n\\nI remember that the capital of France is also called the capital of the European Union, but I'm not sure if that's correct. I think it's actually the capital of the European Union, but I might be mixing that up with something else. I should double-check that. Oh, right, the European Union's capital is also Paris, so that makes sense.\\n\\nNow, thinking about the geography, France is a large country, so its capital should be in a significant geographical location. The Pyrenees are a mountain range in western Europe, so maybe Paris is near there. I think Paris is in the northern part of France, so it's probably in the Pyrenean region. That would make sense because the Pyrenees are known for their mountains and the surrounding areas.\\n\\nI also recall that Paris is the capital of many European countries, so it's likely the most central and important city in France. It's not just a random city; it's the hub for a lot of activities, trade, and political life. I think it's also the capital of the Seine River, which is a major river in France, so that adds to its importance.\\n\\nWait, I should make sure I'm not confusing it with another city. I think I've heard of other capitals, like Berlin or London, but those are in different countries. So, France's capital is definitely Paris. I can't think of any other cities that are more central or have such a significant role in France as Paris does.\\n\\nTo summarize, I believe the capital of France is Paris. It's the most populous city, the capital of the European Union, and it's located in the Pyrenean region, making it a major hub for various aspects of life in France.\\n</think>\\n\\nThe capital of France is Paris. It is the most populous city in the country, the capital of the European Union, and is located in the Pyrenean region, making it a significant hub for various aspects of life in France.\"\n",
      "Prompt: 'The future of AI is', Generated text: \"Okay, so I'm trying to understand the future of AI. I know a bit about AI from school, but I'm not an expert. Let me start by breaking down what I know and then think about what might happen next.\\n\\nFirst, I remember that AI is about machines learning and doing tasks that usually require human intelligence. So, things like language translation, image recognition, and even playing games at a superhuman level. But I'm not sure how far we've come yet.\\n\\nI think there are different types of AI. There's general AI, which is the broad term, and then there's narrow AI, which is more specific. Narrow AI is good for things like medical diagnosis or stock trading because it's very precise. But I'm not sure if that's the future. Maybe AI will become more like general AI, handling a wider range of tasks.\\n\\nAnother thing I've heard is about the ethical implications. I know that AI can cause problems like job displacement or discrimination if it makes people do tasks that humans do less. There's also the issue of privacy—AI systems can collect and analyze data without consent. So, there's a lot of debate about how to handle AI responsibly.\\n\\nI've also heard about the concept of superintelligent AI. I think that's when AI surpasses human intelligence, but I'm not sure how that's going to happen. Maybe through more efficient algorithms or better understanding of human intelligence. But I'm not certain if that's realistic.\\n\\nThen there's the question of how AI will affect society. I imagine it could lead to more automation in industries, but it might also create new jobs or change how people work. There's also the possibility of AI being used in ways that exacerbate inequality, like in hiring or lending decisions.\\n\\nI'm also thinking about the future of human creativity. I know that AI can generate art, but will it replace human creativity? Or will it complement it? I think it's a mix, but maybe AI will become more of a tool than a creator in the long run.\\n\\nAnother area is the integration of AI with other technologies. I've heard about AI augmenting human capabilities, like in healthcare or education. But there's also the risk of AI becoming too dependent on humans, like in surveillance or control systems.\\n\\nI'm curious about the timeline for AI development. I know that some technologies like self-driving cars are already in the works, but it's still in the early stages. Maybe AI will become more advanced as we develop better algorithms and data.\\n\\nI also wonder about the role of humans in the future. Will AI take over certain jobs, or will humans continue to be essential for things like education, healthcare, and creativity? There's a lot of debate about this.\\n\\nThere's also the issue of job displacement. If AI becomes too powerful, it could replace many jobs, leading to a new kind of labor force. I'm not sure how that will be managed or what the job market will look like.\\n\\nI'm trying to think about the potential for AI to solve problems that are currently unsolvable. For example, climate change or pandemics. AI could help model the spread of diseases or predict environmental changes, which could lead to better policies or solutions.\\n\\nBut I'm also concerned about the limitations of AI. It can't process emotions or learn from scratch data, so it might struggle with complex human-like tasks. There's a need for more human-like AI, like in creative problem-solving or decision-making.\\n\\nI'm also thinking about the ethical guidelines for AI. There's a lot of talk about privacy, bias, and transparency. Ensuring that AI systems are fair and transparent is crucial, but it's a big challenge.\\n\\nIn summary, the future of AI seems to be a mix of advancements, ethical considerations, societal impacts, and the balance between human and AI capabilities. It's a complex and evolving field with many possibilities and challenges. I'm not entirely sure about all these aspects, but I think it's clear that AI will play a significant role in the future, but we'll need to be careful about how it's developed and used.\\n</think>\\n\\nThe future of AI is a multifaceted and evolving field that encompasses several key considerations and possibilities. Here's a structured overview:\\n\\n1. **Types of AI**: \\n   - **General AI**: The broad term representing machines capable of performing tasks that require human intelligence, such as language translation and game playing.\\n   - **Narrow AI**: More specific, like medical diagnosis or stock trading, which excel in precise tasks.\\n\\n2. **Ethical Implications**:\\n   - **Privacy and Bias**: Concerns about data collection and the potential for bias in AI systems.\\n   - ** job displacement**: Risks of automation leading to job loss and the need for job displacement strategies.\\n\\n3. **Superintelligent AI**:\\n   - Theoretical possibility of AI surpassing human intelligence, driven by advancements in algorithms and understanding of human intelligence.\\n\\n4. **Societal Impact**:\\n   - Potential for AI to exacerbate inequality, affecting areas like hiring and lending.\\n   - Integration with other technologies, such as healthcare and education, and risks of dependency on humans.\\n\\n5. **Human Creativity**:\\n   - A blend of AI-generated art and human creativity, with AI potentially becoming more of a tool than a creator.\\n\\n6. **Integration with Other Technologies**:\\n   - AI augmenting human capabilities in healthcare and education, but risks of dependence on humans in surveillance and control systems.\\n\\n7. **Timeline and Development**:\\n   - Early stages of AI development, such as self-driving cars, are in progress, with potential for more advanced technologies.\\n\\n8. **Job Market and Labor Force**:\\n   - Concerns about job displacement and the potential for a new labor force, requiring management strategies.\\n\\n9. **Problem Solving and Complex Tasks**:\\n   - AI's role in solving climate change, pandemics, and other complex problems, leading to new policies and solutions.\\n\\n10. **Ethical Guidelines**:\\n    - Importance of ensuring fairness, transparency, and privacy in AI systems, which requires careful development and regulation.\\n\\nIn conclusion, the future of AI is poised for significant advancements but also requires careful consideration of ethical, societal, and practical implications. Balancing human and AI capabilities, ensuring fairness, and addressing ethical concerns are crucial for the future of AI.\"\n"
     ]
    }
   ],
   "source": [
    "import concurrent\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(f\"http://{open('ip').read()}:1338\")\n",
    "\n",
    "\n",
    "def gen_text(text):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"tgi\",\n",
    "        messages=[{\"role\": \"user\", \"content\": text}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=5_000,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "\n",
    "def batch_generation(prompts):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(prompts)) as executor:\n",
    "        prompts_results = list(executor.map(gen_text, prompts))\n",
    "    return prompts_results\n",
    "\n",
    "\n",
    "prompts_results = batch_generation(prompts)\n",
    "\n",
    "for prompt, prompts_result in zip(prompts, prompts_results):\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {prompts_result!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'system_prompt', 'question', 'response'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Open-Orca/OpenOrca\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=10_00, seed=42)\n",
    "dataset = dataset[\"test\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c823eb900664b04961ada35f2a779e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from more_itertools import chunked\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "batch_size = 32 * 2 * 2\n",
    "questions = list(\n",
    "    chunked(\n",
    "        dataset[\"question\"],\n",
    "        batch_size,\n",
    "    )\n",
    ")\n",
    "# 5 min 26 sec - 1000\n",
    "correct_qa_pairs = []\n",
    "for question_chunk in tqdm(questions):\n",
    "    answers = batch_generation(question_chunk)\n",
    "    for question, answer in zip(question_chunk, answers):\n",
    "        if answer.count(\"</think>\") == 1:\n",
    "            correct_qa_pairs.append(\n",
    "                [\n",
    "                    question,\n",
    "                    answer,\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(905, 1000)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_qa_pairs), len(dataset[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can\\'t complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio\\'s death came after Peterson\\'s fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\\n\\nWrite an article based on these highlights.',\n",
       " 'answer': \"Okay, so I need to write an article based on the highlights provided about Drew Peterson and his fourth wife, Kathleen Savio. The user has given me some specific points to include, so I should make sure to cover all of them.\\n\\nFirst, I should start by introducing Drew Peterson and his fourth wife, Kathleen Savio. I need to mention that he's been arrested in the deaths of his first and fourth wives. That's a significant event because it's the second time he's been involved in such a death, which could be a big deal.\\n\\nI should also note that the renewed interest in Savio's death came after his fourth wife disappeared. That's an important detail because it shows that the case isn't over and that there's still a lot to explore.\\n\\nNext, the user mentioned that Peterson's attorney has denied any wrongdoing in either case. That's crucial because it sets the tone that the case is being handled fairly, and there's no evidence of wrongdoing. I should make sure to highlight that part.\\n\\nI should also touch on the legal implications. Peterson is facing charges related to the deaths, and his attorney's denial is a positive sign. Maybe I can mention that the case is being handled with fairness and transparency.\\n\\nI need to structure the article in a way that flows well. Maybe start with an introduction about Peterson and his fourth wife, then move on to the legal developments, the renewed interest, and the attorney's denial. Finally, conclude with the implications of the case.\\n\\nI should also consider the tone. It should be informative but not too technical, so it's accessible to a general audience. I'll need to make sure the language is clear and concise.\\n\\nI should also think about any potential counterarguments or questions that might arise. For example, why is there renewed interest in Savio's death? It could be because of the fourth wife's disappearance, which is a significant event. Maybe I can mention that as a point of interest.\\n\\nI should also ensure that the article doesn't just list the facts but also provides context and analysis. For example, discussing the legal process, the significance of the deaths, and the impact on the community.\\n\\nI need to make sure that the article is balanced, presenting both the positive developments and any potential challenges. Maybe mention that while the case is being handled fairly, there could be future legal challenges, but for now, it's a positive situation.\\n\\nI should also check for any recent developments or updates. The user didn't provide specific dates, so I'll have to keep it general, mentioning that the case is ongoing and that the attorney's denial is a key point.\\n\\nFinally, I should conclude by summarizing the main points and emphasizing the importance of the case in the community. Maybe suggest that the legal community should support Peterson and his attorney in handling the case.\\n\\nI think that's a good outline. Now, I'll structure the article accordingly, making sure each section flows into the next and covers all the key points the user provided.\\n</think>\\n\\n**Drew Peterson's Fourth Wife: A Legal Journey and Renewed Interest**\\n\\nDrew Peterson, a controversial figure known for his controversial past, has recently come to light as the subject of legal proceedings involving his fourth wife, Kathleen Savio. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges related to these tragic incidents. His attorney has denied any wrongdoing in either case, presenting a positive narrative for the legal community.\\n\\n**Introduction to Drew Peterson and His Fourth Wife**\\n\\nDrew Peterson, a controversial figure, has been the subject of legal scrutiny for his past actions, including his controversial death of his first wife, Jane Savio. Now, his fourth wife, Kathleen Savio, has come into the limelight. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges of murder and multiple counts of intentional killing. His attorney has denied any wrongdoing in either case, highlighting a positive stance in the legal community.\\n\\n**Legal Developments and Renewed Interest**\\n\\nPeterson's attorney has denied any wrongdoing in either case, emphasizing that the deaths are not isolated incidents. The renewed interest in Savio's death stems from the fact that his fourth wife disappeared, adding another layer of complexity to the case. This disappearance has sparked discussions about the legal process and the potential impact on the community.\\n\\n**The Case's Legal Implications**\\n\\nPeterson is facing charges of murder and multiple counts of intentional killing, with a possible charge of conspiracy. His attorney has denied any wrongdoing, presenting a positive narrative. The case is being handled with fairness and transparency, reflecting the legal community's commitment to resolving the issue without any evidence of wrongdoing.\\n\\n**Conclusion and Implications**\\n\\nWhile the case is being handled fairly, there could be future legal challenges. However, for now, the legal community should support Peterson and his attorney in handling the case. The renewed interest in Savio's death highlights the ongoing legal battle and the importance of addressing the complexities of the case. The legal community should continue to support Peterson and his attorney in resolving the issue, ensuring justice is served.\"}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = []\n",
    "for question, answer in correct_qa_pairs:\n",
    "    new_dataset.append(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "        }\n",
    "    )\n",
    "new_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 3\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(\u001b[43mnew_dataset\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "new_dataset = Dataset.from_list(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cc55670f2a40ff960d44683c64d857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74721c2be7a84659949cb2f1410942d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B/commit/82cf73531b7b8daaee327e4813aa774abbc0fcc4', commit_message='Upload dataset', commit_description='', oid='82cf73531b7b8daaee327e4813aa774abbc0fcc4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B', endpoint='https://huggingface.co', repo_type='dataset', repo_id='dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.push_to_hub(\"dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 905\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151646"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "think_end_id = tokenizer.encode(\"</think>\")[0]\n",
    "think_end_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can\\'t complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio\\'s death came after Peterson\\'s fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\\n\\nWrite an article based on these highlights.<｜Assistant｜><think>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": dataset[0][\"question\"]},\n",
    "    # {\"role\": \"assistant\", \"content\": result},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "# print(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can\\'t complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio\\'s death came after Peterson\\'s fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\\n\\nWrite an article based on these highlights.<｜Assistant｜><think>\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151646, 151644]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<｜begin▁of▁sentence｜><｜User｜>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can't complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio's death came after Peterson's fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\n",
      "\n",
      "Write an article based on these highlights.<｜Assistant｜><think>\n",
      "Okay, so I need to write an article based on the highlights provided about Drew Peterson and his fourth wife, Kathleen Savio. The user has given me some specific points to include, so I should make sure to cover all of them.\n",
      "\n",
      "First, I should start by introducing Drew Peterson and his fourth wife, Kathleen Savio. I need to mention that he's been arrested in the deaths of his first and fourth wives. That's a significant event because it's the second time he's been involved in such a death, which could be a big deal.\n",
      "\n",
      "I should also note that the renewed interest in Savio's death came after his fourth wife disappeared. That's an important detail because it shows that the case isn't over and that there's still a lot to explore.\n",
      "\n",
      "Next, the user mentioned that Peterson's attorney has denied any wrongdoing in either case. That's crucial because it sets the tone that the case is being handled fairly, and there's no evidence of wrongdoing. I should make sure to highlight that part.\n",
      "\n",
      "I should also touch on the legal implications. Peterson is facing charges related to the deaths, and his attorney's denial is a positive sign. Maybe I can mention that the case is being handled with fairness and transparency.\n",
      "\n",
      "I need to structure the article in a way that flows well. Maybe start with an introduction about Peterson and his fourth wife, then move on to the legal developments, the renewed interest, and the attorney's denial. Finally, conclude with the implications of the case.\n",
      "\n",
      "I should also consider the tone. It should be informative but not too technical, so it's accessible to a general audience. I'll need to make sure the language is clear and concise.\n",
      "\n",
      "I should also think about any potential counterarguments or questions that might arise. For example, why is there renewed interest in Savio's death? It could be because of the fourth wife's disappearance, which is a significant event. Maybe I can mention that as a point of interest.\n",
      "\n",
      "I should also ensure that the article doesn't just list the facts but also provides context and analysis. For example, discussing the legal process, the significance of the deaths, and the impact on the community.\n",
      "\n",
      "I need to make sure that the article is balanced, presenting both the positive developments and any potential challenges. Maybe mention that while the case is being handled fairly, there could be future legal challenges, but for now, it's a positive situation.\n",
      "\n",
      "I should also check for any recent developments or updates. The user didn't provide specific dates, so I'll have to keep it general, mentioning that the case is ongoing and that the attorney's denial is a key point.\n",
      "\n",
      "Finally, I should conclude by summarizing the main points and emphasizing the importance of the case in the community. Maybe suggest that the legal community should support Peterson and his attorney in handling the case.\n",
      "\n",
      "I think that's a good outline. Now, I'll structure the article accordingly, making sure each section flows into the next and covers all the key points the user provided.\n",
      "</think>\n",
      "\n",
      "**Drew Peterson's Fourth Wife: A Legal Journey and Renewed Interest**\n",
      "\n",
      "Drew Peterson, a controversial figure known for his controversial past, has recently come to light as the subject of legal proceedings involving his fourth wife, Kathleen Savio. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges related to these tragic incidents. His attorney has denied any wrongdoing in either case, presenting a positive narrative for the legal community.\n",
      "\n",
      "**Introduction to Drew Peterson and His Fourth Wife**\n",
      "\n",
      "Drew Peterson, a controversial figure, has been the subject of legal scrutiny for his past actions, including his controversial death of his first wife, Jane Savio. Now, his fourth wife, Kathleen Savio, has come into the limelight. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges of murder and multiple counts of intentional killing. His attorney has denied any wrongdoing in either case, highlighting a positive stance in the legal community.\n",
      "\n",
      "**Legal Developments and Renewed Interest**\n",
      "\n",
      "Peterson's attorney has denied any wrongdoing in either case, emphasizing that the deaths are not isolated incidents. The renewed interest in Savio's death stems from the fact that his fourth wife disappeared, adding another layer of complexity to the case. This disappearance has sparked discussions about the legal process and the potential impact on the community.\n",
      "\n",
      "**The Case's Legal Implications**\n",
      "\n",
      "Peterson is facing charges of murder and multiple counts of intentional killing, with a possible charge of conspiracy. His attorney has denied any wrongdoing, presenting a positive narrative. The case is being handled with fairness and transparency, reflecting the legal community's commitment to resolving the issue without any evidence of wrongdoing.\n",
      "\n",
      "**Conclusion and Implications**\n",
      "\n",
      "While the case is being handled fairly, there could be future legal challenges. However, for now, the legal community should support Peterson and his attorney in handling the case. The renewed interest in Savio's death highlights the ongoing legal battle and the importance of addressing the complexities of the case. The legal community should continue to support Peterson and his attorney in resolving the issue, ensuring justice is served.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py#L150\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": dataset[0][\"question\"]},\n",
    "    # {\"role\": \"assistant\", \"content\": dataset[0][\"answer\"]},\n",
    "]\n",
    "part_1 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    # return_dict=True,\n",
    ")\n",
    "part_2 = tokenizer.encode(\n",
    "    dataset[0][\"answer\"],\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "part_3 = tokenizer.encode(\n",
    "    \"<｜end▁of▁sentence｜>\",\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "print(tokenizer.decode(part_1 + part_2 + part_3))\n",
    "labels = len(part_1) * [-100] + part_2 + [-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can't complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio's death came after Peterson's fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\n",
      "\n",
      "Write an article based on these highlights.<｜Assistant｜>\n",
      "\n",
      "**Drew Peterson's Fourth Wife: A Legal Journey and Renewed Interest**\n",
      "\n",
      "Drew Peterson, a controversial figure known for his controversial past, has recently come to light as the subject of legal proceedings involving his fourth wife, Kathleen Savio. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges related to these tragic incidents. His attorney has denied any wrongdoing in either case, presenting a positive narrative for the legal community.\n",
      "\n",
      "**Introduction to Drew Peterson and His Fourth Wife**\n",
      "\n",
      "Drew Peterson, a controversial figure, has been the subject of legal scrutiny for his past actions, including his controversial death of his first wife, Jane Savio. Now, his fourth wife, Kathleen Savio, has come into the limelight. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges of murder and multiple counts of intentional killing. His attorney has denied any wrongdoing in either case, highlighting a positive stance in the legal community.\n",
      "\n",
      "**Legal Developments and Renewed Interest**\n",
      "\n",
      "Peterson's attorney has denied any wrongdoing in either case, emphasizing that the deaths are not isolated incidents. The renewed interest in Savio's death stems from the fact that his fourth wife disappeared, adding another layer of complexity to the case. This disappearance has sparked discussions about the legal process and the potential impact on the community.\n",
      "\n",
      "**The Case's Legal Implications**\n",
      "\n",
      "Peterson is facing charges of murder and multiple counts of intentional killing, with a possible charge of conspiracy. His attorney has denied any wrongdoing, presenting a positive narrative. The case is being handled with fairness and transparency, reflecting the legal community's commitment to resolving the issue without any evidence of wrongdoing.\n",
      "\n",
      "**Conclusion and Implications**\n",
      "\n",
      "While the case is being handled fairly, there could be future legal challenges. However, for now, the legal community should support Peterson and his attorney in handling the case. The renewed interest in Savio's death highlights the ongoing legal battle and the importance of addressing the complexities of the case. The legal community should continue to support Peterson and his attorney in resolving the issue, ensuring justice is served.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": dataset[0][\"question\"]},\n",
    "    {\"role\": \"assistant\", \"content\": dataset[0][\"answer\"]},\n",
    "]\n",
    "\n",
    "part_3 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    # add_generation_prompt=False,\n",
    "    # add_generation_prompt=True,\n",
    "    # continue_final_message=True,\n",
    "    # return_dict=True,\n",
    "    # return_assistant_tokens_mask=True\n",
    ")\n",
    "part_3\n",
    "print(tokenizer.decode(part_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual chat templating (single turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>question<｜Assistant｜>answer<｜end▁of▁sentence｜>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"question\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"answer\"},\n",
    "    ],\n",
    "    tokenize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>question<｜Assistant｜><think>\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"question\"},\n",
    "    ],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "content_compression_mask = []\n",
    "part_1 = \"\"\"<｜begin▁of▁sentence｜><｜User｜>\"\"\"\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_1,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [0]\n",
    "print(content_compression_mask)\n",
    "part_2 = 'NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can'\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_2,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [1]\n",
    "print(content_compression_mask)\n",
    "part_3 = \"<｜Assistant｜><think>\\n\"\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_3,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [0]\n",
    "print(content_compression_mask)\n",
    "part_4 = \"Peterson attorney that</think>Peterson is facing\"\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_4,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [2]\n",
    "print(content_compression_mask)\n",
    "content_compression_mask += [0]\n",
    "print(content_compression_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>In this task you will be given an arithmetic operation in Italian and you have to find its answer. The operations 'addition' and 'subtraction' have been replaced with their italian translations i.e you need to perform addition when you see 'aggiunta' and subtraction in case of 'sottrazione'.\n",
      "Q: 8680 sottrazione 9504 sottrazione 9115 aggiunta 4098\n",
      "A: <｜Assistant｜><think>\n",
      "Okay, so I've got this arithmetic problem in Italian, and I need to figure out the answer. The problem is: 8680 sottrazione 9504 sottrazione 9115 aggiunta 4098. Hmm, let me break this down step by step because I'm not entirely sure how the operations are being translated.\n",
      "\n",
      "First, I know that in English, \"sottrazione\" means subtraction and \"aggiunta\" means addition. So, in this problem, whenever I see \"sottrazione,\" I should subtract, and \"aggiunta\" means I should add. Got it.\n",
      "\n",
      "Let me rewrite the problem with the correct operations. So, it should be: 8680 minus 9504 minus 9115 plus 4098. Wait, is that right? Let me double-check. The original problem is 8680 sottrazione 9504 sottrazione 9115 aggiunta 4098. So, the first operation is subtraction, then another subtraction, then addition, and then another subtraction. So, yes, it's 8680 - 9504 - 9115 + 4098.\n",
      "\n",
      "Now, I need to solve this step by step. Let me start with the first two subtractions: 8680 minus 9504. Hmm, 8680 is less than 9504, so this will result in a negative number. Let me calculate that: 8680 - 9504. Subtracting 9504 from 8680, I get -824. So, the result after the first two operations is -824.\n",
      "\n",
      "Next, I need to subtract 9115 from this result. So, -824 minus 9115. That's like adding a negative number, so it's the same as -824 - 9115. Let me add them: 824 + 9115 is 9939, so with the negative sign, it's -9939. So now, the result is -9939.\n",
      "\n",
      "Finally, I need to add 4098 to this result. So, -9939 + 4098. Let me subtract 4098 from 9939 first: 9939 - 4098. Let me do this step by step. 9939 minus 4000 is 5939, then minus 98 more is 5841. So, since it's negative, it's -5841.\n",
      "\n",
      "Wait, let me verify that. If I have -9939 + 4098, it's the same as -(9939 - 4098). So, 9939 minus 4098. Let me subtract 4000 from 9939, which gives me 5939. Then subtract 98 more: 5939 - 98. 5939 minus 100 is 5839, plus 2 is 5841. So, yes, it's -5841.\n",
      "\n",
      "Let me make sure I didn't make any mistakes in the calculations. Starting with 8680 - 9504: 8680 is less than 9504, so 9504 - 8680 is 824, so 8680 - 9504 is -824. Then, -824 - 9115: that's -824 - 9115 = -9939. Then, -9939 + 4098: 4098 is less than 9939, so it's negative. 9939 - 4098 = 5841, so it's -5841.\n",
      "\n",
      "I think that's correct. Let me check with another method. Maybe adding all the numbers together and then applying the operations. Wait, no, the operations are in sequence, so I have to do them one after another.\n",
      "\n",
      "Alternatively, I can think of it as 8680 - 9504 = -824, then -824 - 9115 = -9939, then -9939 + 4098 = -5841. Yeah, that seems right.\n",
      "\n",
      "I don't think I made any calculation errors here. The key was to correctly identify each operation and apply it in the right order. Subtraction first, then addition, and then subtraction again. It's important to handle the negative numbers carefully to avoid mistakes.\n",
      "\n",
      "So, the final answer should be -5841.\n",
      "</think>\n",
      "\n",
      "The problem is solved by translating the Italian operations into their respective mathematical terms and performing the operations step by step:\n",
      "\n",
      "1. **8680 sottrazione 9504** → 8680 - 9504 = -824\n",
      "2. **-824 sottrazione 9115** → -824 - 9115 = -9939\n",
      "3. **-9939 aggiunta 4098** → -9939 + 4098 = -5841\n",
      "\n",
      "**Answer:** -5841<｜end▁of▁sentence｜>\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_single_turn(question, answer):\n",
    "    content_compression_mask = []\n",
    "\n",
    "    part_1 = \"\"\"<｜begin▁of▁sentence｜><｜User｜>\"\"\"\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_1,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [0]\n",
    "\n",
    "    # question\n",
    "    part_2 = question\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_2,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [1]\n",
    "\n",
    "    part_3 = \"<｜Assistant｜><think>\\n\"\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_3,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [0]\n",
    "\n",
    "    # answer\n",
    "    part_4 = answer\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_4,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [2]\n",
    "\n",
    "    part_5 = \"<｜end▁of▁sentence｜>\"\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_5,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [0]\n",
    "\n",
    "    complete_prompt = \"\"\n",
    "    for part in [part_1, part_2, part_3, part_4, part_5]:\n",
    "        complete_prompt += part\n",
    "    original_tokens = tokenizer.encode(\n",
    "        complete_prompt,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    attention_mask = len(original_tokens) * [1]\n",
    "    return {\n",
    "        \"input_ids\": original_tokens,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"content_compression_mask\": content_compression_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "example = tokenize_single_turn(\n",
    "    question=dataset[6][\"question\"],\n",
    "    answer=dataset[6][\"answer\"],\n",
    ")\n",
    "print(tokenizer.decode(example[\"input_ids\"]))\n",
    "print(example[\"content_compression_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151662]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|fim_pad|>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- сжимаем последовательности последовательно\n",
    "- К примеру сначала мы сжимаем первые последовательные 12 токенов, потом первые 24 и тд\n",
    "- чтобы на инференсе мы могли просто после каждой генерации 12 токенов, сжать ее и генерировать дальше  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'replaced_original_tokens': {'input_ids': [[151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 151662, 151662, 151662, 151662, 1184, 311, 3270, 458, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 151662, 151662, 151662, 151662, 151662, 151662, 151662, 151662, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 151662, 151662, 151662, 151662, 1184, 311, 7071, 700, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 151662, 151662, 151662, 151662, 151662, 151662, 151662, 151662, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       " 'compressed_input_ids': {'input_ids': [[151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 151662, 1184, 311, 3270, 458, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151643, 151643, 151643, 151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 151662, 151662, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 151662, 1184, 311, 7071, 700, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 151662, 151662, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       " 'original_tokens': {'input_ids': [[151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 32313, 11, 773, 358, 1184, 311, 3270, 458, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 32313, 11, 773, 358, 1184, 311, 3270, 458, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 32313, 11, 773, 358, 1184, 311, 7071, 700, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 32313, 11, 773, 358, 1184, 311, 7071, 700, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from more_itertools import chunked\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "text_token_id = tokenizer.encode(\"<|fim_pad|>\", add_special_tokens=False)[0]\n",
    "eos_token_id = tokenizer.encode(\"<｜end▁of▁sentence｜>\", add_special_tokens=False)[0]\n",
    "\n",
    "window_size = 4\n",
    "\n",
    "batch = 2\n",
    "\n",
    "dataset_batch = [dataset[i] for i in range(batch)]\n",
    "dataset_batch = [tokenize_single_turn(**item) for item in dataset_batch]\n",
    "aligned_batch = []\n",
    "# определяем какие именно элементы мы хотим сжимать,а какие\n",
    "# не помещаются не помещаются в чанк размером window_size\n",
    "for tokens in dataset_batch:\n",
    "    input_ids = np.array(tokens[\"input_ids\"])\n",
    "    content_mask = np.array(tokens[\"content_compression_mask\"])\n",
    "    user_part = input_ids[content_mask == 1]\n",
    "    total_parts = len(user_part) // window_size\n",
    "    new_len_part_1 = total_parts * window_size\n",
    "    mask_end_pos = np.where(content_mask == 1)[0][-1]\n",
    "    # print(content_mask.tolist())\n",
    "    content_mask[\n",
    "        mask_end_pos - (len(user_part) - new_len_part_1) + 1 : mask_end_pos + 1\n",
    "    ] = 0\n",
    "    # print(content_mask.tolist())\n",
    "    # print(user_part.shape, total_parts, new_len_part_1)\n",
    "\n",
    "    answer_part = input_ids[content_mask == 2]\n",
    "    total_parts = len(answer_part) // window_size\n",
    "    new_len_part_2 = total_parts * window_size\n",
    "    mask_end_pos = np.where(content_mask == 2)[0][-1]\n",
    "    # print(content_mask.tolist())\n",
    "    content_mask[\n",
    "        mask_end_pos - (len(answer_part) - new_len_part_2) + 1 : mask_end_pos + 1\n",
    "    ] = 0\n",
    "    # print(content_mask.tolist())\n",
    "    # print(answer_part.shape, total_parts, new_len_part_2)\n",
    "    # content_mask[content_mask == 2] = 1\n",
    "    # print(content_mask.tolist())\n",
    "    aligned_batch.append(\n",
    "        {\n",
    "            \"input_ids\": tokens[\"input_ids\"],\n",
    "            \"content_compression_mask\": content_mask,\n",
    "            \"attention_mask\": tokens[\"attention_mask\"],\n",
    "        }\n",
    "    )\n",
    "    # break\n",
    "\n",
    "train_examples = []\n",
    "train_examples_amount = 2\n",
    "for tokens in aligned_batch:\n",
    "    input_ids = np.array(tokens[\"input_ids\"])\n",
    "    content_mask = np.array(tokens[\"content_compression_mask\"])\n",
    "    for chunks_amount in range(train_examples_amount):\n",
    "        # фикусируемся на сжатии ответа модели, выбираем 2\n",
    "        start_pos = np.where(content_mask == 2)[0][0]\n",
    "        input_ids[start_pos : start_pos + (chunks_amount + 1) * window_size] = (\n",
    "            text_token_id\n",
    "        )\n",
    "        compressed_input_ids = input_ids[:start_pos].tolist()\n",
    "        compressed_input_ids += [text_token_id] * (chunks_amount + 1)\n",
    "        compressed_input_ids += input_ids[\n",
    "            start_pos + (chunks_amount + 1) * window_size :\n",
    "        ].tolist()\n",
    "        # print(text_token_id)\n",
    "        # print(\" \".join(f\"{num:>{8}}\" for num in content_mask.tolist()))\n",
    "        # print(\" \".join(f\"{num:>{8}}\" for num in input_ids.tolist()))\n",
    "        # print(\" \".join(f\"{num:>{8}}\" for num in compressed_input_ids))\n",
    "        # print(\"===\")\n",
    "        train_examples.append(\n",
    "            {\n",
    "                \"replaced_original_tokens\": input_ids.tolist(),\n",
    "                \"compressed_input_ids\": compressed_input_ids,\n",
    "                \"original_tokens\": tokens[\"input_ids\"],\n",
    "            }\n",
    "        )\n",
    "    # break\n",
    "\n",
    "\n",
    "# pad to the same length\n",
    "new_inputs = {}\n",
    "for item in train_examples:\n",
    "    for key, value in item.items():\n",
    "        if not key in new_inputs:\n",
    "            new_inputs[key] = []\n",
    "        new_inputs[key].append(value)\n",
    "\n",
    "\n",
    "for key, value in new_inputs.items():\n",
    "    new_inputs[key] = tokenizer.pad(\n",
    "        {\n",
    "            \"input_ids\": new_inputs[key],\n",
    "        },\n",
    "        padding=True,\n",
    "        # return_tensors=\"pt\",\n",
    "    )\n",
    "new_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151662 151643\n",
      "  151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151646   151644      198       48       25    20205      389      419     3395       11     1035      279     1196     6934      419     1985       30     2049    10294       25     3197      358     1156    24706      419     2311      432     6966     9016     9058      323    27759       11      438     1657     1251      614     2669    28686       13     3017    14602      572      311      387     2952      311     3960    34117      448      264     1651      311     8660      279    65660      323     1083      369     7298      990       13      758      419     5091      279     2311    18404      279     4128     1602    98632      398      323    73045      323      835     4241      498      448      264     7474    16266      304     8660     1181     5944      892     6147      498      311      728     3123      323     4586     1064     1602     6707       26     1602     5390      369     4623     6832       13     1096       11      358     1744       11      803     1091     3643      705      369      264      803      993    66390     5486       13     4636      678       11      386     7778      374    57742      504      279    65660     6693     4128      374     9016     6888      336     7022    25777       13     1084     1172     4977     1290      429      582     1430      311     3535      279     4128      518      419     7990      421      582      525      311     9428    15401      432       13    21806      510       32       25   151645   151648      198   151662   151662   151662   151662     1184      311     7071      700     3425      279     1196     1035     6934      419     1985     3118      389      279     2661     3395       13     6771      752     1191      553     5290      279     3395    15516      382      785     3395     2727      429      979      279     1196     1156    24706      279     2311       11      432     6966     9058      323    27759       13     2938      594      264     2409     2518     5181       13     8853     3545     1477     6467     9058      323    27759       11     5310      421      807     2299     3330      311     3960     2494     1075    34117      476    65660       11      892      525     2176     6351      323     7040       13      576     1196      594    14602      572      311     3960    34117      369     8660      279    65660      323     7298      990       13     2055       11      279     2311      594     9058     2090     4977      311      387      264     3598     4265      382      785    55514    33845      429      279     2311    18404      279     4128    98632      398      323    73045       11      892      374     1661       13     1084     6696      264     7474    16266      323     3643      432     4135      311    92540       13     2379     1083     5185      429      432      594      803     1091     1101      458      993    66390     5486       13     2379     1744      429     8660      279     4128    17247      374     2989     1576      279    65660      374     6888      336     7022    25777       13     2055       11      279    55514    13605      429      279     7990      315     8660    14239      973      369      279     9058     2090      382    97904      419     3786       11      279     1196      594     1887     5795      374      311     3960    34117      369     8660      279    65660      323     7298      990       13      576     2311    20027      419     5795      553     1660     2797      323    19819       11      714      432      594      537     1602    22570       13      576     9058     2090      374      264     5089    87520       13     7418     3498      279    55514    15482      279     7990      315     8660     3643      432     5802      432       11      279     2856    20792      374     8225      382     4416       11    12831      678      419       11      279     1196     1035     4658      537     6934      419     1985     1576      279     2311      594     9058     2090      323     6853      315    19805     1281      432     2686     7373      369      862    10602     7428       13     2379     2578    10702      264      803    22570     1467      429     2058      390    49269      279     5871     1995     9355      624   151649      271      785     1196     1035      537     6934      419     1985     1576      279     2311      594     9058     2090      323     6853      315    19805     1281      432     2686     7373      369      862    10602     7428      315     6832    34117      369     8660      279    65660      323     7298      990       13     5976      279    55514    21314      279     2311      594    31273      323    19819     5944       11      279     2856    20792      315     1181     9058     2090      374      264     5089    87520       13   151643\n",
      "       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1\n"
     ]
    }
   ],
   "source": [
    "batch_index = 2\n",
    "print(text_token_id, eos_token_id)\n",
    "print(\n",
    "    \" \".join(\n",
    "        f\"{num:>{8}}\"\n",
    "        for num in new_inputs[\"replaced_original_tokens\"][\"input_ids\"][batch_index]\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \" \".join(\n",
    "        f\"{num:>{8}}\"\n",
    "        for num in new_inputs[\"replaced_original_tokens\"][\"attention_mask\"][batch_index]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['replaced_original_tokens', 'compressed_input_ids', 'original_tokens'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1111, 1536]),\n",
       " torch.Size([4, 1114, 1536]),\n",
       " torch.Size([4, 1114, 1536]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tokens_torch = torch.tensor(\n",
    "    new_inputs[\"original_tokens\"][\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "replaced_tokens_torch = torch.tensor(\n",
    "    new_inputs[\"replaced_original_tokens\"][\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "compressed_tokens_torch = torch.tensor(\n",
    "    new_inputs[\"compressed_input_ids\"][\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "# torch.Size([2, 51, 896])\n",
    "original_embeds = model.get_input_embeddings()(original_tokens_torch)\n",
    "replaced_embeds = model.get_input_embeddings()(replaced_tokens_torch)\n",
    "# torch.Size([2, 35, 896]) 51 - 3*4*2 + 4*2\n",
    "compressed_embeds_template = model.get_input_embeddings()(compressed_tokens_torch)\n",
    "compressed_embeds_template.shape, original_embeds.shape, replaced_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 1536]), torch.Size([6]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_for_compression_mask = replaced_tokens_torch == text_token_id\n",
    "compressed_tokens_mask = compressed_tokens_torch == text_token_id\n",
    "# original_tokens_torch[tokens_for_compression_mask].shape\n",
    "original_embeds[tokens_for_compression_mask].shape, compressed_tokens_torch[\n",
    "    compressed_tokens_mask\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4, 1536])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds_for_compression = original_embeds[tokens_for_compression_mask].reshape(\n",
    "    -1,\n",
    "    window_size,\n",
    "    original_embeds.shape[-1],\n",
    ")\n",
    "embeds_for_compression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2ModelEmbedPoolerV2(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds):\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )[0]\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.sum(1) / torch.tensor(\n",
    "            input_embeds.shape[1],\n",
    "            device=input_embeds.device,\n",
    "        )\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.unsqueeze(1)\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "embed_pooler_v3 = Qwen2ModelEmbedPoolerV2.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1, 1536]), torch.bfloat16)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_embeds = embed_pooler_v3(embeds_for_compression)\n",
    "# pooled_embeds = pooled_embeds.reshape(pooled_embeds.shape[0], -1)\n",
    "pooled_embeds.shape, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template = compressed_embeds_template.masked_scatter_(\n",
    "    compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "    pooled_embeds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = compressed_tokens_torch.clone()\n",
    "labels[labels == text_token_id] = -100\n",
    "labels[labels == eos_token_id] = -100\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5505, device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    inputs_embeds=compressed_embeds_template,\n",
    "    labels=labels,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>2+2*10<｜Assistant｜><think>\n",
      "First, I need to evaluate the expression 2 + 2 * 10.\n",
      "\n",
      "According to the order of operations, I should perform the multiplication before the addition.\n",
      "\n",
      "Calculating 2 * 10 gives me 20.\n",
      "\n",
      "Then, I add 2 to 20, resulting in 22.\n",
      "\n",
      "Therefore, the final answer is 22.\n",
      "</think>\n",
      "\n",
      "Sure, let's solve the expression step by step:\n",
      "\n",
      "\\[\n",
      "2 + 2 \\times 10\n",
      "\\]\n",
      "\n",
      "**Step 1: Perform the multiplication**\n",
      "\n",
      "According to the order of operations (PEMDAS/BODMAS), multiplication comes before addition.\n",
      "\n",
      "\\[\n",
      "2 \\times 10 = 20\n",
      "\\]\n",
      "\n",
      "**Step 2: Add the result to 2**\n",
      "\n",
      "\\[\n",
      "2 + 20 = 22\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "\\[\n",
      "\\boxed{22}\n",
      "\\]<｜end▁of▁sentence｜><｜begin▁of▁sentence｜>\n",
      "\n",
      "Sure, let's solve the expression step by step:\n",
      "\n",
      "\\[\n",
      "2 + 2 \\times 10\n",
      "\\]\n",
      "\n",
      "**Step 1: Perform the multiplication**\n",
      "\n",
      "According to the order of operations (PEMDAS/BODMAS), multiplication comes before addition.\n",
      "\n",
      "\\[\n",
      "2 \\times 10 = 20\n",
      "\\]\n",
      "\n",
      "**Step 2: Add the result to 2**\n",
      "\n",
      "\\[\n",
      "2 + 20 = 22\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "\\[\n",
      "\\boxed{22}\n",
      "\\]<｜end▁of▁sentence｜><｜begin▁of▁sentence｜>\n",
      "\n",
      "Sure, let's solve the expression step by step:\n",
      "\n",
      "\\[\n",
      "2 + 2 \\times 10\n",
      "\\]\n",
      "\n",
      "**Step 1: Perform the multiplication**\n",
      "\n",
      "According to the order of operations (PEMDAS/BODMAS), multiplication comes before addition.\n",
      "\n",
      "\\[\n",
      "2 \\times 10 = 20\n",
      "\\]\n",
      "\n",
      "**Step 2: Add the result to 2**\n",
      "\n",
      "\\[\n",
      "2 + 20 = 22\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "max_steps = 400\n",
    "for _ in range(max_steps):\n",
    "    logits = model(input_ids=generated_tokens).logits\n",
    "    top = logits.argmax(-1)[-1][-1]\n",
    "    # print(top)\n",
    "    generated_tokens = torch.cat([generated_tokens, top.reshape(1, 1)], dim=1)\n",
    "print(tokenizer.decode(generated_tokens[-1]))\n",
    "# break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generation with compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You must specify exactly one of input_ids or inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m     compressed_part \u001b[38;5;241m=\u001b[39m embed_pooler_v3(new_embeds_for_compression)\n\u001b[1;32m     24\u001b[0m     input_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m     25\u001b[0m         [embeddings_cache, compressed_part],\n\u001b[1;32m     26\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     27\u001b[0m     )\n\u001b[0;32m---> 28\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_embeds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:856\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:526\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m^\u001b[39m (inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must specify exactly one of input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m use_cache:\n\u001b[1;32m    529\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    530\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: You must specify exactly one of input_ids or inputs_embeds"
     ]
    }
   ],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "max_steps = 10\n",
    "temp_gen_size = 0\n",
    "compression_started = False\n",
    "window_size = 4\n",
    "embeddings_cache = model.get_input_embeddings()(generated_tokens)\n",
    "for _ in range(max_steps):\n",
    "    if compression_started:\n",
    "        if temp_gen_size == window_size:\n",
    "            print(\"state 1\")\n",
    "            new_tokens_for_compression = generated_tokens[:, -4:]\n",
    "            new_embeds_for_compression = model.get_input_embeddings()(\n",
    "                new_tokens_for_compression\n",
    "            )\n",
    "            compressed_part = embed_pooler_v3(new_embeds_for_compression)\n",
    "            input_embeds = torch.cat(\n",
    "                [embeddings_cache, compressed_part],\n",
    "                dim=1,\n",
    "            )\n",
    "            logits = model(input_embeds=input_embeds).logits\n",
    "        break\n",
    "        pass\n",
    "    else:\n",
    "        logits = model(input_embeds=embeddings_cache).logits\n",
    "        top = logits.argmax(-1)[-1][-1]\n",
    "        temp_gen_size += 1\n",
    "        # print(top)\n",
    "        generated_tokens = torch.cat([generated_tokens, top.reshape(1, 1)], dim=1)\n",
    "        if temp_gen_size == window_size:\n",
    "            compression_started = True\n",
    "\n",
    "print(tokenizer.decode(generated_tokens[-1]))\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 1536])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_cache.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151646, 151644,     17,     10,     17,      9,     16]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "generated_tokens[:, :-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 1536])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_part = embed_pooler_v3(new_embeds_for_compression)\n",
    "torch.cat([embeddings_cache, compressed_part], dim=1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
