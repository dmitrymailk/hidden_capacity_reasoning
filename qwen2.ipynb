{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings().weight.dtype, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A large language model is a type of artificial intelligence (AI) system that can generate human-like text, including written and spoken words. These models are typically trained on large amounts of data using techniques such as deep learning and natural language processing. Large language models have been used in a wide range of applications, from translation and summarization to chatbots and virtual assistants. Some examples of popular large language models include GPT-3 and Qwen.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model..1\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 30, 896]), torch.bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    **model_inputs,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.hidden_states[-1].shape, model_output_1.hidden_states[-1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30, 896])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_2 = model(\n",
    "    inputs_embeds=torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    attention_mask=model_inputs[\"attention_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_2.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 896])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 896])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1.hidden_states[-1].reshape(10, 3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 30, 896]), torch.bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_2 = model(\n",
    "    inputs_embeds=torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    attention_mask=model_inputs[\"attention_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_2.hidden_states[-1].shape, model_output_2.hidden_states[-1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 896])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_3 = model(\n",
    "    inputs_embeds=model_output_1.hidden_states[-1].reshape(10, 3, -1),\n",
    "    # attention_mask=model_inputs[\"attention_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_3.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 3, 896])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1.hidden_states[-1].reshape(1, 10, 3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(\n",
    "#     inputs_embeds=model_output_1.hidden_states[-1].reshape(1, 10, 3, -1),\n",
    "#     attention_mask=model_inputs[\"attention_mask\"],\n",
    "#     output_hidden_states=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 896])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Qwen2ModelEmbedPooler(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config).cuda()\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds, attention_mask, window_size=3):\n",
    "        # разбиваем входящие эмбединги на бакеты и усредняем их\n",
    "        sum_mask = attention_mask.reshape(\n",
    "            attention_mask.shape[0],\n",
    "            window_size,\n",
    "            attention_mask.shape[1] // window_size,\n",
    "            -1,\n",
    "        ).sum(1)\n",
    "        embeds_sum = input_embeds.reshape(\n",
    "            attention_mask.shape[0],\n",
    "            window_size,\n",
    "            attention_mask.shape[1] // window_size,\n",
    "            -1,\n",
    "        ).sum(1)\n",
    "        input_embeds = embeds_sum / sum_mask\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "embed_pooler = Qwen2ModelEmbedPooler.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "result = embed_pooler(\n",
    "    # model_output_1.hidden_states[-1],\n",
    "    torch.cat(\n",
    "        [\n",
    "            model_output_1.hidden_states[-1],\n",
    "            model_output_1.hidden_states[-1],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    "    torch.cat(\n",
    "        [\n",
    "            model_inputs[\"attention_mask\"],\n",
    "            model_inputs[\"attention_mask\"],\n",
    "        ],\n",
    "        dim=0,\n",
    "    ),\n",
    ")\n",
    "result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 896])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3, 4, 5, 6]).reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from transformers.cache_utils import (\n",
    "    Cache,\n",
    "    DynamicCache,\n",
    "    SlidingWindowCache,\n",
    "    StaticCache,\n",
    ")\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.utils import LossKwargs\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "\n",
    "\n",
    "class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n",
    "\n",
    "\n",
    "class Qwen2ForCausalEmbedModeling(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = torch.nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.embed_pooler = Qwen2ModelEmbedPooler.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        # pooled_mask: Optional[torch.Tensor] = None,\n",
    "        **kwargs: Unpack[KwargsForCausalLM],\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        inputs_embeds_tokens = self.model.embed_tokens(input_ids)\n",
    "        # if pixel_values is not None:\n",
    "        #     pixel_values = pixel_values.type(self.visual.get_dtype())\n",
    "        #     image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n",
    "        #     n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n",
    "        #     n_image_features = image_embeds.shape[0]\n",
    "        #     if n_image_tokens != n_image_features:\n",
    "        #         raise ValueError(\n",
    "        #             f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "        #         )\n",
    "        #     image_mask = (\n",
    "        #         (input_ids == self.config.image_token_id)\n",
    "        #         .unsqueeze(-1)\n",
    "        #         .expand_as(inputs_embeds)\n",
    "        #         .to(inputs_embeds.device)\n",
    "        #     )\n",
    "        #     image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "        #     inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "        window_size = torch.tensor(\n",
    "            [\n",
    "                [4],\n",
    "                [3],\n",
    "            ]\n",
    "        ).cuda()\n",
    "        tokens_amount = torch.tensor(\n",
    "            [\n",
    "                [2],\n",
    "                [4],\n",
    "            ]\n",
    "        ).cuda()\n",
    "        lengths = window_size * tokens_amount\n",
    "        # max_len = lengths.max()\n",
    "        max_len = inputs_embeds_tokens.shape[1]\n",
    "        batch_size = window_size.shape[0]\n",
    "        pooled_mask = (\n",
    "            torch.arange(max_len, device=device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(batch_size, max_len)\n",
    "        )\n",
    "        pooled_mask = (lengths >= pooled_mask).to(torch.long)\n",
    "        pooled_embeds = inputs_embeds * pooled_mask.to(inputs_embeds.dtype)\n",
    "        pooled_embeds = self.embed_pooler(pooled_embeds, pooled_mask)\n",
    "        embed_mask = (\n",
    "            (input_ids == self.config.image_token_id)\n",
    "            .unsqueeze(-1)\n",
    "            .expand_as(inputs_embeds)\n",
    "            .to(inputs_embeds.device)\n",
    "        )\n",
    "        inputs_embeds = inputs_embeds.masked_scatter(embed_mask, pooled_embeds)\n",
    "\n",
    "        # Из-за смешанной структуры, мы будем всегда подавать только эмбединги\n",
    "        # Идея позаимствована из qwen2vl\n",
    "        outputs = self.model(\n",
    "            input_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = (\n",
    "            slice(-logits_to_keep, None)\n",
    "            if isinstance(logits_to_keep, int)\n",
    "            else logits_to_keep\n",
    "        )\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(\n",
    "                logits=logits,\n",
    "                labels=labels,\n",
    "                vocab_size=self.config.vocab_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование концепции qwen2vl для текстовых токенов и эмбедингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = \"Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?\"\n",
    "\n",
    "tokenizer.encode(text_example)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151655]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|image_pad|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1318, 39304]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"text_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Случай первый - мы сжимаем только текстовые токены (максимально упрощенный вариант)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|>ized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 3\n",
    "chunk_size = 4\n",
    "new_tokens = []\n",
    "original_tokens = tokenizer.encode(text_example)\n",
    "new_tokens += tokenizer.encode(\"<|object_ref_start|>\") * chunk_size\n",
    "new_tokens += original_tokens[chunk_size * window_size :]\n",
    "tokenizer.decode(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(original_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 43, 896]), torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tokens_torch = torch.tensor(\n",
    "    [\n",
    "        original_tokens,\n",
    "        original_tokens,\n",
    "    ],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "new_tokens_torch = torch.tensor(\n",
    "    [\n",
    "        new_tokens,\n",
    "        new_tokens,\n",
    "    ],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "# torch.Size([2, 51, 896])\n",
    "original_embeds = model.get_input_embeddings()(original_tokens_torch)\n",
    "# torch.Size([2, 43, 896]) 51 - 3*4 + 4\n",
    "compressed_embeds_template = model.get_input_embeddings()(new_tokens_torch)\n",
    "compressed_embeds_template.shape, compressed_embeds_template.dtype, original_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 43, 896]), torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Qwen2ModelEmbedPoolerV2(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds):\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )[0]\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.sum(1) / torch.tensor(\n",
    "            input_embeds.shape[1],\n",
    "            device=input_embeds.device,\n",
    "        )\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.unsqueeze(1)\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "embed_pooler_v2 = Qwen2ModelEmbedPoolerV2.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# torch.Size([8, 3, 896])\n",
    "compressed_embeds = original_embeds[:, : chunk_size * window_size].reshape(\n",
    "    chunk_size * original_embeds.shape[0],\n",
    "    window_size,\n",
    "    -1,\n",
    ")\n",
    "compressed_embeds.shape\n",
    "# torch.Size([8, 1, 896])\n",
    "pooled_embeds = embed_pooler_v2(compressed_embeds)\n",
    "# torch.Size([2, 4, 896])\n",
    "pooled_embeds = pooled_embeds.reshape(\n",
    "    original_embeds.shape[0],\n",
    "    chunk_size,\n",
    "    -1,\n",
    ")\n",
    "# torch.Size([2, 43, 896])\n",
    "compressed_embeds_template[:, :chunk_size] = pooled_embeds\n",
    "compressed_embeds_template.shape, compressed_embeds_template.dtype, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_pooler_v2.model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  1506,   279, 18414,   315, 20443, 11229,\n",
       "           304,  3213,  1635,    13,  5542,   656, 59711,  9331,   311, 34549,\n",
       "         15712,    11,  1181,  8357,   525, 10454, 14756, 70767,    13,  1988,\n",
       "          1128,  6896,   374,  5538,  6832,    30,  1597,  1128,  3643,   432,\n",
       "           773,  7988,    30],\n",
       "        [ -100,  -100,  -100,  -100,  1506,   279, 18414,   315, 20443, 11229,\n",
       "           304,  3213,  1635,    13,  5542,   656, 59711,  9331,   311, 34549,\n",
       "         15712,    11,  1181,  8357,   525, 10454, 14756, 70767,    13,  1988,\n",
       "          1128,  6896,   374,  5538,  6832,    30,  1597,  1128,  3643,   432,\n",
       "           773,  7988,    30]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = new_tokens_torch.clone()\n",
    "text_token_id = tokenizer.encode(\"<|object_ref_start|>\")[0]\n",
    "labels[labels == text_token_id] = -100\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7288, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    inputs_embeds=compressed_embeds_template,\n",
    "    labels=labels,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Усложненная версия токенизации текста. Сжатые токены возникают между обычным тектом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входная последовательность может быть 5 видов.\n",
    "\n",
    "- когда у нас на входе только текст, который мы просто моделируем \n",
    "- когда на входе текст, но мы хотим сжать его некоторые части\n",
    "- когда на входе текст и эмбединги с последнего слоя, где мы хотим сжать только части текста\n",
    "- когда на входе текст и эмбединги с последнего слоя, где мы хотим сжать, часть текста и эмбедингов, совместно (токены переводим в эмбединги и сжимаем вместе с hidden states)\n",
    "- когда на входе текст и эмбединги с последнего слоя, где мы хотим сжать, только эмбединги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы решаем какие токены хотим сжать?\n",
    "- никак. Просто говорим сожми с такого по такой.\n",
    "- это норм только на этапе обучения, так как мы можем перебрать все комбинации.\n",
    "- не норм на этапе инференса. например мы можем сжимать после каждых сгенеренных 10 токенов или 100, 1000. а какое окно контекста? 3, 10, 100? А что если менять стратегию. Сначала мы сжимали с окном 5 токенов, потом 20?\n",
    "- Кажется что эти гиперпараметры можно найти простым перебором на валидации. Однако перебор стратегии уже не кажется таким очевидным. Напрашивается RL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11, 702, 13791, 1506, 279, 18414, 315, 20443, 11229, 304, 3213, 1635, 13, 5542, 656, 59711, 9331, 311, 34549, 15712, 11, 1181, 8357, 525, 10454, 14756, 70767, 13, 1988, 1128, 6896, 374, 5538, 6832, 30, 1597, 1128, 3643, 432, 773, 7988, 30, 220, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643], [10227, 42415, 1119, 279, 330, 32880, 1, 949, 11, 1077, 594, 5695, 264, 16266, 448, 279, 31774, 315, 20443, 29728, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = [\n",
    "    \"Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self-driving cars to personalized medicine, its applications are becoming increasingly pervasive. But what exactly is deep learning? And what makes it so powerful? \",\n",
    "    \"\"\"Before diving into the \"deep\" part, let's establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include:\"\"\",\n",
    "]\n",
    "\n",
    "tokenizer.batch_encode_plus(\n",
    "    text_example,\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 122])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(\n",
    "    tokenizer.batch_encode_plus(\n",
    "        text_example,\n",
    "        padding=True,\n",
    "    )[\"input_ids\"]\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(151643)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11, 702, 13791, 1506, 279, 18414, 315, 20443, 11229, 304, 3213, 1635, 13, 5542, 656, 59711, 9331, 311, 34549, 15712, 11, 1181, 8357, 525, 10454, 14756, 70767, 13, 1988, 1128, 6896, 374, 5538, 6832, 30, 1597, 1128, 3643, 432, 773, 7988, 30, 220]\n",
      "4\n",
      "[[33464, 6832, 11, 264, 1186, 2566, 315, 5662, 6832, 11, 702, 13791], [1506, 279, 18414, 315, 20443, 11229, 304, 3213, 1635, 13, 5542, 656], [59711, 9331, 311, 34549, 15712, 11, 1181, 8357, 525, 10454, 14756, 70767], [13, 1988, 1128, 6896, 374, 5538, 6832, 30, 1597, 1128, 3643, 432], [773, 7988, 30, 220]]\n",
      "chunks_for_tokenization  {2}\n",
      "=== replaced_original_tokens\n",
      "Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|>. But what exactly is deep learning? And what makes it so powerful? \n",
      "=== new_input_tokens\n",
      "Deep learning, a subfield of machine learning, has revolutionized the landscape of artificial intelligence in recent years. From self<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|>. But what exactly is deep learning? And what makes it so powerful? \n",
      "44\n",
      "==\n",
      "==\n",
      "==\n",
      "[10227, 42415, 1119, 279, 330, 32880, 1, 949, 11, 1077, 594, 5695, 264, 16266, 448, 279, 31774, 315, 20443, 29728, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25]\n",
      "10\n",
      "[[10227, 42415, 1119, 279, 330, 32880, 1, 949, 11, 1077, 594, 5695], [264, 16266, 448, 279, 31774, 315, 20443, 29728, 14155, 320, 12003, 82], [568, 1527, 76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645], [304, 13617, 13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924], [25, 4220, 13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527], [76406, 17167, 315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617], [13, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220], [13617, 11136, 2924, 25, 14155, 320, 12003, 82, 568, 1527, 76406, 17167], [315, 82316, 7798, 11, 2598, 33213, 11, 16645, 304, 13617, 13, 4220], [13617, 11136, 2924, 25, 4220, 13617, 11136, 2924, 25, 4220, 13617, 11136], [2924, 25]]\n",
      "chunks_for_tokenization  {7}\n",
      "=== replaced_original_tokens\n",
      "Before diving into the \"deep\" part, let's establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|> of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include:\n",
      "=== new_input_tokens\n",
      "Before diving into the \"deep\" part, let's establish a foundation with the basics of artificial neural networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include: networks (ANNs). An ANN consists of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These<|object_ref_start|><|object_ref_start|><|object_ref_start|><|object_ref_start|> of interconnected nodes, called neurons, organized in layers. These layers typically include: These layers typically include: These layers typically include:\n",
      "114\n",
      "==\n",
      "==\n",
      "==\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2, 114), (2, 122), (2, 122))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from more_itertools import chunked\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "text_token_id = tokenizer.encode(\"<|object_ref_start|>\")[0]\n",
    "eos_token_id = tokenizer.encode(\"<|endoftext|>\")[0]\n",
    "\n",
    "window_size = 3\n",
    "chunk_size = 4\n",
    "new_tokens = []\n",
    "original_tokens = tokenizer.batch_encode_plus(\n",
    "    text_example,\n",
    "    padding=True,\n",
    ")\n",
    "compressed_tokens = []\n",
    "replaced_original_tokens_batch = []\n",
    "# original_tokens\n",
    "for tokens in original_tokens[\"input_ids\"]:\n",
    "    original_lines = np.array(tokens)\n",
    "    pure_tokens = original_lines[original_lines != eos_token_id].tolist()\n",
    "    print(pure_tokens)\n",
    "    full_chunks_amount = len(pure_tokens) // (window_size * chunk_size)\n",
    "    print(full_chunks_amount)\n",
    "    max_percent = 0.8\n",
    "    pure_tokens_chunks = list(chunked(pure_tokens, window_size * chunk_size))\n",
    "    print(pure_tokens_chunks)\n",
    "    prob = 0.3\n",
    "    random_mask = np.random.random(int(full_chunks_amount * max_percent))\n",
    "    mask = random_mask < prob\n",
    "    chunks_for_tokenization = np.where(mask)[0].tolist()\n",
    "    chunks_for_tokenization = set(chunks_for_tokenization)\n",
    "    if len(chunks_for_tokenization) == 0:\n",
    "        chunks_for_tokenization = set(\n",
    "            [\n",
    "                random.randint(\n",
    "                    0,\n",
    "                    int(full_chunks_amount * max_percent),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    print(\"chunks_for_tokenization \", chunks_for_tokenization)\n",
    "    replaced_original_tokens = []\n",
    "    new_input_tokens = []\n",
    "    for i, tokens in enumerate(pure_tokens_chunks):\n",
    "        if i in chunks_for_tokenization:\n",
    "            replaced_original_tokens.extend([text_token_id] * len(tokens))\n",
    "            new_input_tokens.extend([text_token_id] * chunk_size)\n",
    "        else:\n",
    "            replaced_original_tokens.extend(tokens)\n",
    "            new_input_tokens.extend(tokens)\n",
    "    print(\"=== replaced_original_tokens\")\n",
    "    print(tokenizer.decode(replaced_original_tokens))\n",
    "    print(\"=== new_input_tokens\")\n",
    "    print(tokenizer.decode(new_input_tokens))\n",
    "    print(len(new_input_tokens))\n",
    "    compressed_tokens.append(new_input_tokens)\n",
    "    replaced_original_tokens_batch.append(replaced_original_tokens)\n",
    "    print(\"==\")\n",
    "    print(\"==\")\n",
    "    print(\"==\")\n",
    "\n",
    "compressed_tokens_attention = []\n",
    "max_compressed_len = max([len(item) for item in compressed_tokens])\n",
    "max_replaced_len = max([len(item) for item in replaced_original_tokens_batch])\n",
    "\n",
    "for compressed_seq, replaced_seq in zip(\n",
    "    compressed_tokens,\n",
    "    replaced_original_tokens_batch,\n",
    "):\n",
    "    compressed_seq_len = len(compressed_seq)\n",
    "    replaced_seq_len = len(replaced_seq)\n",
    "    attention_mask = [1] * (compressed_seq_len)\n",
    "\n",
    "    if compressed_seq_len < max_compressed_len:\n",
    "        compressed_seq += [eos_token_id] * (max_compressed_len - compressed_seq_len)\n",
    "        attention_mask += [0] * (max_compressed_len - compressed_seq_len)\n",
    "\n",
    "    if compressed_seq_len < max_replaced_len:\n",
    "        replaced_seq += [eos_token_id] * (max_replaced_len - replaced_seq_len)\n",
    "\n",
    "    compressed_tokens_attention.append(attention_mask)\n",
    "# len(compressed_tokens[0]), len(compressed_tokens[1])\n",
    "# compressed_tokens = torch.tensor(compressed_tokens)\n",
    "# compressed_tokens_attention = torch.tensor(compressed_tokens_attention)\n",
    "np.array(compressed_tokens).shape, np.array(\n",
    "    replaced_original_tokens_batch\n",
    ").shape, np.array(original_tokens[\"input_ids\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 114, 896]),\n",
       " torch.Size([2, 122, 896]),\n",
       " torch.Size([2, 122, 896]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tokens_torch = torch.tensor(\n",
    "    original_tokens[\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "replaced_tokens_torch = torch.tensor(\n",
    "    replaced_original_tokens_batch,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "compressed_tokens_torch = torch.tensor(\n",
    "    compressed_tokens,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "# torch.Size([2, 51, 896])\n",
    "original_embeds = model.get_input_embeddings()(original_tokens_torch)\n",
    "replaced_embeds = model.get_input_embeddings()(replaced_tokens_torch)\n",
    "# torch.Size([2, 35, 896]) 51 - 3*4*2 + 4*2\n",
    "compressed_embeds_template = model.get_input_embeddings()(compressed_tokens_torch)\n",
    "compressed_embeds_template.shape, original_embeds.shape, replaced_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.dtype, original_embeds.dtype, replaced_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151646"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 896]), torch.Size([8]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_for_compression_mask = replaced_tokens_torch == text_token_id\n",
    "compressed_tokens_mask = compressed_tokens_torch == text_token_id\n",
    "# original_tokens_torch[tokens_for_compression_mask].shape\n",
    "original_embeds[tokens_for_compression_mask].shape, compressed_tokens_torch[\n",
    "    compressed_tokens_mask\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 896])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds_for_compression = original_embeds[tokens_for_compression_mask].reshape(\n",
    "    -1,\n",
    "    window_size,\n",
    "    original_embeds.shape[-1],\n",
    ")\n",
    "embeds_for_compression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 896]), torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_embeds = embed_pooler_v2(embeds_for_compression)\n",
    "# pooled_embeds = pooled_embeds.reshape(pooled_embeds.shape[0], -1)\n",
    "pooled_embeds.shape, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1],\n",
       "        [2, 3, 0, 4, 5]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "self = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 0, 0, 1],\n",
    "        [1, 1, 0, 1, 1],\n",
    "    ],\n",
    "    dtype=torch.bool,\n",
    ")\n",
    "source = torch.tensor(\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    ")\n",
    "self.masked_scatter_(mask, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98, 896])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_tokens_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98, 896])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_tokens_mask.shape\n",
    "# .expand_as(compressed_embeds_template).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 114, 896])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, torch.float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.dtype, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 114, 896])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_embeds_template.masked_scatter(\n",
    "    compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "    pooled_embeds,\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template = compressed_embeds_template.masked_scatter_(\n",
    "    compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "    pooled_embeds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template[compressed_tokens_mask][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template[compressed_tokens_mask][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = compressed_tokens_torch.clone()\n",
    "text_token_id = tokenizer.encode(\"<|object_ref_start|>\")[0]\n",
    "labels[labels == text_token_id] = -100\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0430, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    inputs_embeds=compressed_embeds_template,\n",
    "    labels=labels,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R1 Chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288410b9288443729887dba085eafec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346706232628491cb4d37a22ab2cc486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176a1496a7de4bf39a497e4ef014cb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3275d39ee1344d6b466b25076f6d5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d859886df048b18130a17aa711bc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant.<｜User｜>how many wings has a bird?<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Okay, so I need to figure out how many wings a bird has. I'm not exactly a bird expert, but I know some basics. Let me start by recalling what I know about different bird species and their wing structures.\\n\\nFirst, I remember that most birds have a pair of wings. That's a common feature in many bird species. So, maybe the answer is two wings. But wait, I should think about exceptions because sometimes birds might have different numbers of wings.\\n\\nI think some birds have only one wing, like the fly. The monarch butterfly has one wing, but it's a specialized bird. So, that's an exception. Then there are birds like the peacock, which have two wings. So that's two wings again.\\n\\nAre there any other birds that might have different numbers of wings? I'm trying to think of any. Maybe some birds have three wings? I'm not sure if that's common or if it's even been observed. I think most birds have two wings, so maybe three isn't typical.\\n\\nLet me list out the common bird species and their wing numbers. Monarch butterfly: one wing. Bluebird: two wings. Eagle: two wings. Crow: two wings. Ape: two wings. Hawk: two wings. Crow: two wings. Owl: two wings. Ape: two wings. Raven: two wings. Swallow: two wings. Different species might have different numbers, but the majority have two wings.\\n\\nSo, putting this together, the majority of birds have two wings, but some, like the monarch butterfly and the peacock, have one. There's no bird I can think of that has more than two wings, so the answer is probably two wings on average, with exceptions like the monarch butterfly and the peacock.\\n\\nI should also consider if there are any birds that have more than two wings. I don't recall any, so it's safe to say that most birds have two wings. Therefore, the answer is two wings.\\n</think>\\n\\nThe majority of birds have two wings, which is a common feature across various species. However, some birds, such as the monarch butterfly and the peacock, have only one wing. Therefore, the typical number of wings a bird has is two. \\n\\nAnswer: Two wings.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"how many wings has a bird?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=1024)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant.<｜User｜>how many wings has a bird?<｜Assistant｜><think>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out how many wings a bird has. I'm not exactly a bird expert, but I know some basics. Let me start by recalling what I know about different bird species and their wing structures.\n",
      "\n",
      "First, I remember that most birds have a pair of wings. That's a common feature in many bird species. So, maybe the answer is two wings. But wait, I should think about exceptions because sometimes birds might have different numbers of wings.\n",
      "\n",
      "I think some birds have only one wing, like the fly. The monarch butterfly has one wing, but it's a specialized bird. So, that's an exception. Then there are birds like the peacock, which have two wings. So that's two wings again.\n",
      "\n",
      "Are there any other birds that might have different numbers of wings? I'm trying to think of any. Maybe some birds have three wings? I'm not sure if that's common or if it's even been observed. I think most birds have two wings, so maybe three isn't typical.\n",
      "\n",
      "Let me list out the common bird species and their wing numbers. Monarch butterfly: one wing. Bluebird: two wings. Eagle: two wings. Crow: two wings. Ape: two wings. Hawk: two wings. Crow: two wings. Owl: two wings. Ape: two wings. Raven: two wings. Swallow: two wings. Different species might have different numbers, but the majority have two wings.\n",
      "\n",
      "So, putting this together, the majority of birds have two wings, but some, like the monarch butterfly and the peacock, have one. There's no bird I can think of that has more than two wings, so the answer is probably two wings on average, with exceptions like the monarch butterfly and the peacock.\n",
      "\n",
      "I should also consider if there are any birds that have more than two wings. I don't recall any, so it's safe to say that most birds have two wings. Therefore, the answer is two wings.\n",
      "</think>\n",
      "\n",
      "The majority of birds have two wings, which is a common feature across various species. However, some birds, such as the monarch butterfly and the peacock, have only one wing. Therefore, the typical number of wings a bird has is two. \n",
      "\n",
      "Answer: Two wings.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151645: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151646: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151662"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|fim_pad|>\", add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_len 437\n",
      "First, I need to compare the two numbers: 9.11 and 9.9.\n",
      "\n",
      "To make the comparison easier, I'll align their decimal places by writing 9.9 as 9.90.\n",
      "\n",
      "Now, I'll compare each corresponding digit from left to right.\n",
      "\n",
      "Both numbers have 9 in the units place, so they are equal there.\n",
      "\n",
      "Next, I'll look at the tenths place. In 9.11, the tenths digit is 1, while in 9.90, it's 9.\n",
      "\n",
      "Since 9 is greater than 1, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "To determine which number is larger between **9.11** and **9.9**, follow these steps:\n",
      "\n",
      "1. **Align the Decimal Places:**\n",
      "   \n",
      "   To make the comparison easier, write both numbers with the same number of decimal places:\n",
      "   \n",
      "   \\[\n",
      "   9.11 \\quad \\text{and} \\quad 9.90\n",
      "   \\]\n",
      "\n",
      "2. **Compare the Numbers Digit by Digit:**\n",
      "   \n",
      "   - **Units Place:**\n",
      "     \n",
      "     Both numbers have **9** in the units place.\n",
      "     \n",
      "     \\[\n",
      "     9 \\quad \\text{vs} \\quad 9\n",
      "     \\]\n",
      "     \n",
      "     They are equal in this place.\n",
      "\n",
      "   - **Tenths Place:**\n",
      "     \n",
      "     In **9.11**, the tenths digit is **1**.\n",
      "     \n",
      "     In **9.90**, the tenths digit is **9**.\n",
      "     \n",
      "     \\[\n",
      "     1 \\quad \\text{vs} \\quad 9\n",
      "     \\]\n",
      "     \n",
      "     Since **9** is greater than **1**, **9.90** is larger than **9.11**.\n",
      "\n",
      "3. **Conclusion:**\n",
      "   \n",
      "   Therefore, **9.9** is bigger than **9.11**.\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is bigger}}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "from transformers import Qwen2ForCausalLM, Qwen2Model, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "client = InferenceClient(f\"http://{open('ip').read()}:1338\")\n",
    "\n",
    "user_prompt = \"9.11 and 9.9 -- which is bigger? Let's think step by step.\"\n",
    "output = client.chat.completions.create(\n",
    "    model=\"tgi\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        },\n",
    "    ],\n",
    "    stream=False,\n",
    "    max_tokens=10000,\n",
    "    temperature=0.0,\n",
    ")\n",
    "result = output.choices[0].message.content\n",
    "print(\"total_len\", len(tokenizer.encode(result)))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: 'Okay, so I\\'m trying to figure out what the user named is. They just said \"Hello, my name is\" and then stopped. Hmm, that\\'s a bit confusing. Maybe they forgot to finish the sentence. I should probably ask them to complete it so I can help better. I don\\'t want to assume anything about their identity or name. It\\'s better to be safe than sorry. I\\'ll let them know I\\'m here to help once they provide their full name.\\n</think>\\n\\nIt seems like your name is incomplete. Could you please provide your full name so I can assist you better?'\n",
      "Prompt: 'The president of the United States is', Generated text: \"Okay, so I need to figure out what the president of the United States is. Hmm, I'm not exactly sure, but I think the president is the head of the executive branch of the United States. I remember hearing that they're usually named after a person, like George W. Bush or Barack Obama. Maybe I should look up the current president to get the most accurate information.\\n\\nWait, I'm not sure if I can access the internet right now. Maybe I can think of some famous presidents and their names. I know George Washington was the first president, and he's often called the first president. Then there's John Adams, who was the second president. After that, Thomas Jefferson was the third president. I think that's right because he was the first president of the states, not the country.\\n\\nAfter that, James Madison and James Monroe were the fourth and fifth presidents, respectively. Then, after that, Abraham Lincoln was the sixth president. He was the first president of the United States, but he was also the first president of the Confederate States. After him, Andrew Jackson was the seventh president. He was a significant figure in the Civil Rights Movement.\\n\\nThen, after Andrew Jackson, Martin Van Buren was the eighth president. He was a statesman and played a role in the Civil War. After him, James K. Polk was the ninth president. He was known for his policies, like the transcontinental railroad. Following Polk, William Henry Harrison was the tenth president, but he died in office, so he's often excluded from the list.\\n\\nAfter Harrison, John Tyler was the eleventh president, and he was a statesman and military leader. Then, Franklin Pierce was the twelfth president. He was a key figure in the Civil War. After that, James Buchanan was the thirteenth president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the fourteenth president, the first president of the United States. Then, Andrew Johnson was the fifteenth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the sixteenth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the seventeenth president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the eighteenth president again, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler, Franklin Pierce was the twenty-second president. After that, James Buchanan was the twenty-third president, known for his policies and the Civil Rights Movement.\\n\\nAfter Buchanan, Abraham Lincoln was the twenty-fourth president, the first president of the United States. Then, Andrew Johnson was the twenty-fifth president, who was a significant figure in the Civil War. Following Johnson, Ulysses S. Grant was the twenty-sixth president, known for his strategies in the Civil War. After Grant, Rutherford B. Hayes was the twenty-seventh president, who was a statesman and played a role in the Civil Rights Movement.\\n\\nThen, after Hayes, John Tyler was the twenty-eighth president, and he was a statesman and military leader. Following Tyler, James K. Polk was the nineteenth president again, known for the transcontinental railroad. After Polk, William Henry Harrison was the twentieth president, but he died, so he's often excluded.\\n\\nAfter Harrison, John Tyler was the twenty-first president, and he was a statesman and military leader. Following Tyler\"\n",
      "Prompt: 'The capital of France is', Generated text: \"Okay, so I need to figure out the capital of France. I remember that France is a country in Europe, but I'm not exactly sure where its capital is. I think it's somewhere in the north, maybe in the Pyrenees or something. I've heard of Paris before, but I'm not sure if it's the only capital. I should probably start by recalling what I know about capitals.\\n\\nI know that the capital of France is the most populous city, so it's probably a major city. Paris comes to mind because it's the capital of many countries, but I'm not certain if it's the only one. I think there are other capitals, but I'm not sure which ones. Maybe I should think about the countries that are connected to France. France borders several countries, so maybe each of those has their own capital.\\n\\nI remember that the capital of France is also called the capital of the European Union, but I'm not sure if that's correct. I think it's actually the capital of the European Union, but I might be mixing that up with something else. I should double-check that. Oh, right, the European Union's capital is also Paris, so that makes sense.\\n\\nNow, thinking about the geography, France is a large country, so its capital should be in a significant geographical location. The Pyrenees are a mountain range in western Europe, so maybe Paris is near there. I think Paris is in the northern part of France, so it's probably in the Pyrenean region. That would make sense because the Pyrenees are known for their mountains and the surrounding areas.\\n\\nI also recall that Paris is the capital of many European countries, so it's likely the most central and important city in France. It's not just a random city; it's the hub for a lot of activities, trade, and political life. I think it's also the capital of the Seine River, which is a major river in France, so that adds to its importance.\\n\\nWait, I should make sure I'm not confusing it with another city. I think I've heard of other capitals, like Berlin or London, but those are in different countries. So, France's capital is definitely Paris. I can't think of any other cities that are more central or have such a significant role in France as Paris does.\\n\\nTo summarize, I believe the capital of France is Paris. It's the most populous city, the capital of the European Union, and it's located in the Pyrenean region, making it a major hub for various aspects of life in France.\\n</think>\\n\\nThe capital of France is Paris. It is the most populous city in the country, the capital of the European Union, and is located in the Pyrenean region, making it a significant hub for various aspects of life in France.\"\n",
      "Prompt: 'The future of AI is', Generated text: \"Okay, so I'm trying to understand the future of AI. I know a bit about AI from school, but I'm not an expert. Let me start by breaking down what I know and then think about what might happen next.\\n\\nFirst, I remember that AI is about machines learning and doing tasks that usually require human intelligence. So, things like language translation, image recognition, and even playing games at a superhuman level. But I'm not sure how far we've come yet.\\n\\nI think there are different types of AI. There's general AI, which is the broad term, and then there's narrow AI, which is more specific. Narrow AI is good for things like medical diagnosis or stock trading because it's very precise. But I'm not sure if that's the future. Maybe AI will become more like general AI, handling a wider range of tasks.\\n\\nAnother thing I've heard is about the ethical implications. I know that AI can cause problems like job displacement or discrimination if it makes people do tasks that humans do less. There's also the issue of privacy—AI systems can collect and analyze data without consent. So, there's a lot of debate about how to handle AI responsibly.\\n\\nI've also heard about the concept of superintelligent AI. I think that's when AI surpasses human intelligence, but I'm not sure how that's going to happen. Maybe through more efficient algorithms or better understanding of human intelligence. But I'm not certain if that's realistic.\\n\\nThen there's the question of how AI will affect society. I imagine it could lead to more automation in industries, but it might also create new jobs or change how people work. There's also the possibility of AI being used in ways that exacerbate inequality, like in hiring or lending decisions.\\n\\nI'm also thinking about the future of human creativity. I know that AI can generate art, but will it replace human creativity? Or will it complement it? I think it's a mix, but maybe AI will become more of a tool than a creator in the long run.\\n\\nAnother area is the integration of AI with other technologies. I've heard about AI augmenting human capabilities, like in healthcare or education. But there's also the risk of AI becoming too dependent on humans, like in surveillance or control systems.\\n\\nI'm curious about the timeline for AI development. I know that some technologies like self-driving cars are already in the works, but it's still in the early stages. Maybe AI will become more advanced as we develop better algorithms and data.\\n\\nI also wonder about the role of humans in the future. Will AI take over certain jobs, or will humans continue to be essential for things like education, healthcare, and creativity? There's a lot of debate about this.\\n\\nThere's also the issue of job displacement. If AI becomes too powerful, it could replace many jobs, leading to a new kind of labor force. I'm not sure how that will be managed or what the job market will look like.\\n\\nI'm trying to think about the potential for AI to solve problems that are currently unsolvable. For example, climate change or pandemics. AI could help model the spread of diseases or predict environmental changes, which could lead to better policies or solutions.\\n\\nBut I'm also concerned about the limitations of AI. It can't process emotions or learn from scratch data, so it might struggle with complex human-like tasks. There's a need for more human-like AI, like in creative problem-solving or decision-making.\\n\\nI'm also thinking about the ethical guidelines for AI. There's a lot of talk about privacy, bias, and transparency. Ensuring that AI systems are fair and transparent is crucial, but it's a big challenge.\\n\\nIn summary, the future of AI seems to be a mix of advancements, ethical considerations, societal impacts, and the balance between human and AI capabilities. It's a complex and evolving field with many possibilities and challenges. I'm not entirely sure about all these aspects, but I think it's clear that AI will play a significant role in the future, but we'll need to be careful about how it's developed and used.\\n</think>\\n\\nThe future of AI is a multifaceted and evolving field that encompasses several key considerations and possibilities. Here's a structured overview:\\n\\n1. **Types of AI**: \\n   - **General AI**: The broad term representing machines capable of performing tasks that require human intelligence, such as language translation and game playing.\\n   - **Narrow AI**: More specific, like medical diagnosis or stock trading, which excel in precise tasks.\\n\\n2. **Ethical Implications**:\\n   - **Privacy and Bias**: Concerns about data collection and the potential for bias in AI systems.\\n   - ** job displacement**: Risks of automation leading to job loss and the need for job displacement strategies.\\n\\n3. **Superintelligent AI**:\\n   - Theoretical possibility of AI surpassing human intelligence, driven by advancements in algorithms and understanding of human intelligence.\\n\\n4. **Societal Impact**:\\n   - Potential for AI to exacerbate inequality, affecting areas like hiring and lending.\\n   - Integration with other technologies, such as healthcare and education, and risks of dependency on humans.\\n\\n5. **Human Creativity**:\\n   - A blend of AI-generated art and human creativity, with AI potentially becoming more of a tool than a creator.\\n\\n6. **Integration with Other Technologies**:\\n   - AI augmenting human capabilities in healthcare and education, but risks of dependence on humans in surveillance and control systems.\\n\\n7. **Timeline and Development**:\\n   - Early stages of AI development, such as self-driving cars, are in progress, with potential for more advanced technologies.\\n\\n8. **Job Market and Labor Force**:\\n   - Concerns about job displacement and the potential for a new labor force, requiring management strategies.\\n\\n9. **Problem Solving and Complex Tasks**:\\n   - AI's role in solving climate change and pandemics through modeling and prediction, leading to better policies.\\n\\n10. **Limitations and Challenges**:\\n    - AI's limitations in processing emotions and learning from data, necessitating more human-like capabilities.\\n\\n11. **Ethical Guidelines**:\\n    - Emphasis on fairness, transparency, and privacy, crucial for ensuring ethical AI development.\\n\\nIn conclusion, the future of AI is poised for significant impact, with ethical considerations, societal implications, and the balance between human and AI capabilities shaping its trajectory. Addressing these challenges will be essential for the responsible development and deployment of AI.\"\n"
     ]
    }
   ],
   "source": [
    "import concurrent\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(f\"http://{open('ip').read()}:1338\")\n",
    "\n",
    "\n",
    "def gen_text(text):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"tgi\",\n",
    "        messages=[{\"role\": \"user\", \"content\": text}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=5_000,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "\n",
    "def batch_generation(prompts):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(prompts)) as executor:\n",
    "        prompts_results = list(executor.map(gen_text, prompts))\n",
    "    return prompts_results\n",
    "\n",
    "\n",
    "prompts_results = batch_generation(prompts)\n",
    "\n",
    "for prompt, prompts_result in zip(prompts, prompts_results):\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {prompts_result!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'system_prompt', 'question', 'response'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Open-Orca/OpenOrca\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=5000, seed=42)\n",
    "dataset = dataset[\"test\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fec9cc698f24fe4a4ce8b7f17e180d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from more_itertools import chunked\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "batch_size = 32 * 2 * 2\n",
    "questions = list(\n",
    "    chunked(\n",
    "        dataset[\"question\"],\n",
    "        batch_size,\n",
    "    )\n",
    ")\n",
    "# 5 min 26 sec - 1000\n",
    "# 28 min 6 sec - 5000\n",
    "correct_qa_pairs = []\n",
    "for question_chunk in tqdm(questions):\n",
    "    answers = batch_generation(question_chunk)\n",
    "    for question, answer in zip(question_chunk, answers):\n",
    "        if answer.count(\"</think>\") == 1:\n",
    "            correct_qa_pairs.append(\n",
    "                [\n",
    "                    question,\n",
    "                    answer,\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4475, 5000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_qa_pairs), len(dataset[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can\\'t complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio\\'s death came after Peterson\\'s fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\\n\\nWrite an article based on these highlights.',\n",
       " 'answer': \"Okay, so I need to write an article based on the highlights provided about Drew Peterson and his fourth wife, Kathleen Savio. The user has given me some specific points to include, so I should make sure to cover all of them.\\n\\nFirst, I should start by introducing Drew Peterson and his fourth wife, Kathleen Savio. I need to mention that he's been arrested in the deaths of his first and fourth wives. That's a significant event because it's the second time he's been involved in such a death, which could be a big deal.\\n\\nI should also note that the renewed interest in Savio's death came after his fourth wife disappeared. That's an important detail because it shows that the case isn't over and that there's still a lot to explore.\\n\\nNext, the user mentioned that Peterson's attorney has denied any wrongdoing in either case. That's crucial because it sets the tone that the case is being handled fairly, and there's no evidence of wrongdoing. I should make sure to highlight that part.\\n\\nI should also touch on the legal implications. Peterson is facing charges related to the deaths, and his attorney's denial is a positive sign. Maybe I can mention that the case is being handled with fairness and transparency.\\n\\nI need to structure the article in a way that flows well. Maybe start with an introduction about Peterson and his fourth wife, then move on to the legal developments, the renewed interest, and the attorney's denial. Finally, conclude with the implications of the case.\\n\\nI should also consider the tone. It should be informative but not too technical, so it's accessible to a general audience. I'll need to make sure the language is clear and concise.\\n\\nI should also think about any potential counterarguments or questions that might arise. For example, why is there renewed interest in Savio's death? It could be because of the fourth wife's disappearance, which is a significant event. Maybe I can mention that as a point of interest.\\n\\nI should also ensure that the article doesn't just list the facts but also provides context and analysis. For example, discussing the legal process, the significance of the deaths, and the impact on the community.\\n\\nI need to make sure that the article is balanced, presenting both the positive developments and any potential challenges. Maybe mention that while the case is being handled fairly, there could be future legal challenges, but for now, it's a positive situation.\\n\\nI should also check for any recent developments or updates that might affect the article. For instance, if there have been any court proceedings or legal strategies mentioned, I should include that.\\n\\nFinally, I should wrap up the article by summarizing the key points and emphasizing the importance of the case in the community. Maybe end with a statement about the impact of these deaths on the lives of the victims and their families.\\n\\nI think that's a good outline. Now, I'll start drafting the article, making sure to cover all these points and present them in a logical and engaging manner.\\n</think>\\n\\n**Drew Peterson's Fourth Wife: A New Dawn in the Case**\\n\\nDrew Peterson, the controversial actor and former President of the United States, has recently come to light as the fourth victim of his wife, Kathleen Savio. In a significant development, Peterson has been arrested in the deaths of his first and fourth wives, marking a new chapter in his legal troubles. His attorney has denied any wrongdoing in either case, promising a fair and transparent handling of the matter.\\n\\nThe renewed interest in Savio's death stems from the disappearance of his fourth wife, which has sparked a wave of attention and speculation. Peterson's attorney has denied any wrongdoing, emphasizing that the case is being handled with fairness and transparency. This stance is a positive sign, as it suggests that the legal process is being conducted with integrity.\\n\\nPeterson's legal troubles are complex, involving charges related to the deaths of his first and fourth wives. The case highlights the legal challenges faced by individuals involved in such high-profile deaths, particularly when the victims are not immediately accounted for. Peterson's attorney's denial of wrongdoing is a testament to the efforts being made to ensure justice is served.\\n\\nThe case has also sparked discussions about the legal process and the impact of such high-profile deaths on the community. While the case is being handled fairly, there could be future legal challenges, but for now, it is a positive situation. The deaths of these women have left a significant impact on their families and communities, underscoring the importance of addressing such cases with fairness and transparency.\\n\\nIn conclusion, Drew Peterson's fourth wife, Kathleen Savio, has brought a new perspective to the legal landscape, with his legal troubles being handled with fairness and transparency. The case highlights the ongoing efforts to ensure justice and the importance of addressing such high-profile deaths with care and consideration.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = []\n",
    "for question, answer in correct_qa_pairs:\n",
    "    new_dataset.append(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "        }\n",
    "    )\n",
    "new_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "new_dataset = Dataset.from_list(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58237a7b20794de08223bd210c02e365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14b26aaf4ae4f099412a736c74d1a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/dim/open_orca_4475_DeepSeek-R1-Distill-Qwen-1.5B/commit/ca6edb6c3da6791bddb190338648e6c999f1d1e3', commit_message='Upload dataset', commit_description='', oid='ca6edb6c3da6791bddb190338648e6c999f1d1e3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/dim/open_orca_4475_DeepSeek-R1-Distill-Qwen-1.5B', endpoint='https://huggingface.co', repo_type='dataset', repo_id='dim/open_orca_4475_DeepSeek-R1-Distill-Qwen-1.5B'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.push_to_hub(\n",
    "    f\"dim/open_orca_{len(correct_qa_pairs)}_DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0ee18c7a62482b977e93b914156223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/312 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb83008ffd84f739528c1ebc26fedb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5c6bf169f341988918e32108384f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 905\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dim/open_orca_905_DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151646"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "think_end_id = tokenizer.encode(\"</think>\")[0]\n",
    "think_end_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can\\'t complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio\\'s death came after Peterson\\'s fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\\n\\nWrite an article based on these highlights.<｜Assistant｜><think>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": dataset[0][\"question\"]},\n",
    "    # {\"role\": \"assistant\", \"content\": result},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "# print(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can\\'t complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio\\'s death came after Peterson\\'s fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\\n\\nWrite an article based on these highlights.<｜Assistant｜><think>\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151646, 151644]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<｜begin▁of▁sentence｜><｜User｜>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can't complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio's death came after Peterson's fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\n",
      "\n",
      "Write an article based on these highlights.<｜Assistant｜><think>\n",
      "Okay, so I need to write an article based on the highlights provided about Drew Peterson and his fourth wife, Kathleen Savio. The user has given me some specific points to include, so I should make sure to cover all of them.\n",
      "\n",
      "First, I should start by introducing Drew Peterson and his fourth wife, Kathleen Savio. I need to mention that he's been arrested in the deaths of his first and fourth wives. That's a significant event because it's the second time he's been involved in such a death, which could be a big deal.\n",
      "\n",
      "I should also note that the renewed interest in Savio's death came after his fourth wife disappeared. That's an important detail because it shows that the case isn't over and that there's still a lot to explore.\n",
      "\n",
      "Next, the user mentioned that Peterson's attorney has denied any wrongdoing in either case. That's crucial because it sets the tone that the case is being handled fairly, and there's no evidence of wrongdoing. I should make sure to highlight that part.\n",
      "\n",
      "I should also touch on the legal implications. Peterson is facing charges related to the deaths, and his attorney's denial is a positive sign. Maybe I can mention that the case is being handled with fairness and transparency.\n",
      "\n",
      "I need to structure the article in a way that flows well. Maybe start with an introduction about Peterson and his fourth wife, then move on to the legal developments, the renewed interest, and the attorney's denial. Finally, conclude with the implications of the case.\n",
      "\n",
      "I should also consider the tone. It should be informative but not too technical, so it's accessible to a general audience. I'll need to make sure the language is clear and concise.\n",
      "\n",
      "I should also think about any potential counterarguments or questions that might arise. For example, why is there renewed interest in Savio's death? It could be because of the fourth wife's disappearance, which is a significant event. Maybe I can mention that as a point of interest.\n",
      "\n",
      "I should also ensure that the article doesn't just list the facts but also provides context and analysis. For example, discussing the legal process, the significance of the deaths, and the impact on the community.\n",
      "\n",
      "I need to make sure that the article is balanced, presenting both the positive developments and any potential challenges. Maybe mention that while the case is being handled fairly, there could be future legal challenges, but for now, it's a positive situation.\n",
      "\n",
      "I should also check for any recent developments or updates. The user didn't provide specific dates, so I'll have to keep it general, mentioning that the case is ongoing and that the attorney's denial is a key point.\n",
      "\n",
      "Finally, I should conclude by summarizing the main points and emphasizing the importance of the case in the community. Maybe suggest that the legal community should support Peterson and his attorney in handling the case.\n",
      "\n",
      "I think that's a good outline. Now, I'll structure the article accordingly, making sure each section flows into the next and covers all the key points the user provided.\n",
      "</think>\n",
      "\n",
      "**Drew Peterson's Fourth Wife: A Legal Journey and Renewed Interest**\n",
      "\n",
      "Drew Peterson, a controversial figure known for his controversial past, has recently come to light as the subject of legal proceedings involving his fourth wife, Kathleen Savio. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges related to these tragic incidents. His attorney has denied any wrongdoing in either case, presenting a positive narrative for the legal community.\n",
      "\n",
      "**Introduction to Drew Peterson and His Fourth Wife**\n",
      "\n",
      "Drew Peterson, a controversial figure, has been the subject of legal scrutiny for his past actions, including his controversial death of his first wife, Jane Savio. Now, his fourth wife, Kathleen Savio, has come into the limelight. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges of murder and multiple counts of intentional killing. His attorney has denied any wrongdoing in either case, highlighting a positive stance in the legal community.\n",
      "\n",
      "**Legal Developments and Renewed Interest**\n",
      "\n",
      "Peterson's attorney has denied any wrongdoing in either case, emphasizing that the deaths are not isolated incidents. The renewed interest in Savio's death stems from the fact that his fourth wife disappeared, adding another layer of complexity to the case. This disappearance has sparked discussions about the legal process and the potential impact on the community.\n",
      "\n",
      "**The Case's Legal Implications**\n",
      "\n",
      "Peterson is facing charges of murder and multiple counts of intentional killing, with a possible charge of conspiracy. His attorney has denied any wrongdoing, presenting a positive narrative. The case is being handled with fairness and transparency, reflecting the legal community's commitment to resolving the issue without any evidence of wrongdoing.\n",
      "\n",
      "**Conclusion and Implications**\n",
      "\n",
      "While the case is being handled fairly, there could be future legal challenges. However, for now, the legal community should support Peterson and his attorney in handling the case. The renewed interest in Savio's death highlights the ongoing legal battle and the importance of addressing the complexities of the case. The legal community should continue to support Peterson and his attorney in resolving the issue, ensuring justice is served.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py#L150\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": dataset[0][\"question\"]},\n",
    "    # {\"role\": \"assistant\", \"content\": dataset[0][\"answer\"]},\n",
    "]\n",
    "part_1 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    # return_dict=True,\n",
    ")\n",
    "part_2 = tokenizer.encode(\n",
    "    dataset[0][\"answer\"],\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "part_3 = tokenizer.encode(\n",
    "    \"<｜end▁of▁sentence｜>\",\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "print(tokenizer.decode(part_1 + part_2 + part_3))\n",
    "labels = len(part_1) * [-100] + part_2 + [-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can't complain\" Drew Peterson arrested in the death of his third wife, Kathleen Savio. Renewed interest in Savio's death came after Peterson's fourth wife disappeared. Peterson, through his attorney, denies any wrongdoing in either case.\n",
      "\n",
      "Write an article based on these highlights.<｜Assistant｜>\n",
      "\n",
      "**Drew Peterson's Fourth Wife: A Legal Journey and Renewed Interest**\n",
      "\n",
      "Drew Peterson, a controversial figure known for his controversial past, has recently come to light as the subject of legal proceedings involving his fourth wife, Kathleen Savio. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges related to these tragic incidents. His attorney has denied any wrongdoing in either case, presenting a positive narrative for the legal community.\n",
      "\n",
      "**Introduction to Drew Peterson and His Fourth Wife**\n",
      "\n",
      "Drew Peterson, a controversial figure, has been the subject of legal scrutiny for his past actions, including his controversial death of his first wife, Jane Savio. Now, his fourth wife, Kathleen Savio, has come into the limelight. Peterson, who has been arrested in the deaths of his first and fourth wives, has faced charges of murder and multiple counts of intentional killing. His attorney has denied any wrongdoing in either case, highlighting a positive stance in the legal community.\n",
      "\n",
      "**Legal Developments and Renewed Interest**\n",
      "\n",
      "Peterson's attorney has denied any wrongdoing in either case, emphasizing that the deaths are not isolated incidents. The renewed interest in Savio's death stems from the fact that his fourth wife disappeared, adding another layer of complexity to the case. This disappearance has sparked discussions about the legal process and the potential impact on the community.\n",
      "\n",
      "**The Case's Legal Implications**\n",
      "\n",
      "Peterson is facing charges of murder and multiple counts of intentional killing, with a possible charge of conspiracy. His attorney has denied any wrongdoing, presenting a positive narrative. The case is being handled with fairness and transparency, reflecting the legal community's commitment to resolving the issue without any evidence of wrongdoing.\n",
      "\n",
      "**Conclusion and Implications**\n",
      "\n",
      "While the case is being handled fairly, there could be future legal challenges. However, for now, the legal community should support Peterson and his attorney in handling the case. The renewed interest in Savio's death highlights the ongoing legal battle and the importance of addressing the complexities of the case. The legal community should continue to support Peterson and his attorney in resolving the issue, ensuring justice is served.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": dataset[0][\"question\"]},\n",
    "    {\"role\": \"assistant\", \"content\": dataset[0][\"answer\"]},\n",
    "]\n",
    "\n",
    "part_3 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    # add_generation_prompt=False,\n",
    "    # add_generation_prompt=True,\n",
    "    # continue_final_message=True,\n",
    "    # return_dict=True,\n",
    "    # return_assistant_tokens_mask=True\n",
    ")\n",
    "part_3\n",
    "print(tokenizer.decode(part_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual chat templating (single turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>question<｜Assistant｜>answer<｜end▁of▁sentence｜>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"question\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"answer\"},\n",
    "    ],\n",
    "    tokenize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜User｜>question<｜Assistant｜><think>\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"question\"},\n",
    "    ],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "content_compression_mask = []\n",
    "part_1 = \"\"\"<｜begin▁of▁sentence｜><｜User｜>\"\"\"\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_1,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [0]\n",
    "print(content_compression_mask)\n",
    "part_2 = 'NEW: Peterson to media on handcuffs, chains: \"I got the bling. Can'\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_2,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [1]\n",
    "print(content_compression_mask)\n",
    "part_3 = \"<｜Assistant｜><think>\\n\"\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_3,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [0]\n",
    "print(content_compression_mask)\n",
    "part_4 = \"Peterson attorney that</think>Peterson is facing\"\n",
    "content_compression_mask += len(\n",
    "    tokenizer.encode(\n",
    "        part_4,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    ") * [2]\n",
    "print(content_compression_mask)\n",
    "content_compression_mask += [0]\n",
    "print(content_compression_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>In this task you will be given an arithmetic operation in Italian and you have to find its answer. The operations 'addition' and 'subtraction' have been replaced with their italian translations i.e you need to perform addition when you see 'aggiunta' and subtraction in case of 'sottrazione'.\n",
      "Q: 8680 sottrazione 9504 sottrazione 9115 aggiunta 4098\n",
      "A: <｜Assistant｜><think>\n",
      "Okay, so I've got this arithmetic problem in Italian, and I need to figure out the answer. The problem is: 8680 sottrazione 9504 sottrazione 9115 aggiunta 4098. Hmm, let me break this down step by step because I'm not entirely sure how the operations are being translated.\n",
      "\n",
      "First, I know that in English, \"sottrazione\" means subtraction and \"aggiunta\" means addition. So, in this problem, whenever I see \"sottrazione,\" I should subtract, and \"aggiunta\" means I should add. Got it.\n",
      "\n",
      "Let me rewrite the problem with the correct operations. So, it should be: 8680 minus 9504 minus 9115 plus 4098. Wait, is that right? Let me double-check. The original problem is 8680 sottrazione 9504 sottrazione 9115 aggiunta 4098. So, the first operation is subtraction, then another subtraction, then addition, and then another subtraction. So, yes, it's 8680 - 9504 - 9115 + 4098.\n",
      "\n",
      "Now, I need to solve this step by step. Let me start with the first two subtractions: 8680 minus 9504. Hmm, 8680 is less than 9504, so this will result in a negative number. Let me calculate that: 8680 - 9504. Subtracting 9504 from 8680, I get -824. So, the result after the first two operations is -824.\n",
      "\n",
      "Next, I need to subtract 9115 from this result. So, -824 minus 9115. That's like adding a negative number, so it's the same as -824 - 9115. Let me add them: 824 + 9115 is 9939, so with the negative sign, it's -9939. So now, the result is -9939.\n",
      "\n",
      "Finally, I need to add 4098 to this result. So, -9939 + 4098. Let me subtract 4098 from 9939 first: 9939 - 4098. Let me do this step by step. 9939 minus 4000 is 5939, then minus 98 more is 5841. So, since it's negative, it's -5841.\n",
      "\n",
      "Wait, let me verify that. If I have -9939 + 4098, it's the same as -(9939 - 4098). So, 9939 minus 4098. Let me subtract 4000 from 9939, which gives me 5939. Then subtract 98 more: 5939 - 98. 5939 minus 100 is 5839, plus 2 is 5841. So, yes, it's -5841.\n",
      "\n",
      "Let me make sure I didn't make any mistakes in the calculations. Starting with 8680 - 9504: 8680 is less than 9504, so 9504 - 8680 is 824, so 8680 - 9504 is -824. Then, -824 - 9115: that's -824 - 9115 = -9939. Then, -9939 + 4098: 4098 is less than 9939, so it's negative. 9939 - 4098 = 5841, so it's -5841.\n",
      "\n",
      "I think that's correct. Let me check with another method. Maybe adding all the numbers together and then applying the operations. Wait, no, the operations are in sequence, so I have to do them one after another.\n",
      "\n",
      "Alternatively, I can think of it as 8680 - 9504 = -824, then -824 - 9115 = -9939, then -9939 + 4098 = -5841. Yeah, that seems right.\n",
      "\n",
      "I don't think I made any calculation errors here. The key was to correctly identify each operation and apply it in the right order. Subtraction first, then addition, and then subtraction again. It's important to handle the negative numbers carefully to avoid mistakes.\n",
      "\n",
      "So, the final answer should be -5841.\n",
      "</think>\n",
      "\n",
      "The problem is solved by translating the Italian operations into their respective mathematical terms and performing the operations step by step:\n",
      "\n",
      "1. **8680 sottrazione 9504** → 8680 - 9504 = -824\n",
      "2. **-824 sottrazione 9115** → -824 - 9115 = -9939\n",
      "3. **-9939 aggiunta 4098** → -9939 + 4098 = -5841\n",
      "\n",
      "**Answer:** -5841<｜end▁of▁sentence｜>\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_single_turn(question, answer):\n",
    "    content_compression_mask = []\n",
    "\n",
    "    part_1 = \"\"\"<｜begin▁of▁sentence｜><｜User｜>\"\"\"\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_1,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [0]\n",
    "\n",
    "    # question\n",
    "    part_2 = question\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_2,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [1]\n",
    "\n",
    "    part_3 = \"<｜Assistant｜><think>\\n\"\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_3,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [0]\n",
    "\n",
    "    # answer\n",
    "    part_4 = answer\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_4,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [2]\n",
    "\n",
    "    part_5 = \"<｜end▁of▁sentence｜>\"\n",
    "    content_compression_mask += len(\n",
    "        tokenizer.encode(\n",
    "            part_5,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "    ) * [0]\n",
    "\n",
    "    complete_prompt = \"\"\n",
    "    for part in [part_1, part_2, part_3, part_4, part_5]:\n",
    "        complete_prompt += part\n",
    "    original_tokens = tokenizer.encode(\n",
    "        complete_prompt,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    attention_mask = len(original_tokens) * [1]\n",
    "    return {\n",
    "        \"input_ids\": original_tokens,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"content_compression_mask\": content_compression_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "example = tokenize_single_turn(\n",
    "    question=dataset[6][\"question\"],\n",
    "    answer=dataset[6][\"answer\"],\n",
    ")\n",
    "print(tokenizer.decode(example[\"input_ids\"]))\n",
    "print(example[\"content_compression_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151662]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|fim_pad|>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- сжимаем последовательности последовательно\n",
    "- К примеру сначала мы сжимаем первые последовательные 12 токенов, потом первые 24 и тд\n",
    "- чтобы на инференсе мы могли просто после каждой генерации 12 токенов, сжать ее и генерировать дальше  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'replaced_original_tokens': {'input_ids': [[151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 151662, 151662, 151662, 151662, 1184, 311, 3270, 458, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 151662, 151662, 151662, 151662, 151662, 151662, 151662, 151662, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 151662, 151662, 151662, 151662, 1184, 311, 7071, 700, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 151662, 151662, 151662, 151662, 151662, 151662, 151662, 151662, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       " 'compressed_input_ids': {'input_ids': [[151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 151662, 1184, 311, 3270, 458, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151643, 151643, 151643, 151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 151662, 151662, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 151662, 1184, 311, 7071, 700, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 151662, 151662, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       " 'original_tokens': {'input_ids': [[151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 32313, 11, 773, 358, 1184, 311, 3270, 458, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151646, 151644, 20594, 25, 39791, 311, 3687, 389, 65045, 32568, 11, 26179, 25, 330, 40, 2684, 279, 1501, 287, 13, 2980, 944, 27911, 1, 40108, 39791, 12517, 304, 279, 4545, 315, 806, 4843, 7403, 11, 64063, 20079, 815, 13, 48986, 291, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 39791, 594, 11737, 7403, 28396, 13, 39791, 11, 1526, 806, 13747, 11, 46491, 894, 64228, 304, 2987, 1142, 382, 7985, 458, 4549, 3118, 389, 1493, 21314, 13, 151645, 151648, 198, 32313, 11, 773, 358, 1184, 311, 3270, 458, 4549, 3118, 389, 279, 21314, 3897, 911, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 576, 1196, 702, 2661, 752, 1045, 3151, 3501, 311, 2924, 11, 773, 358, 1265, 1281, 2704, 311, 3421, 678, 315, 1105, 382, 5338, 11, 358, 1265, 1191, 553, 31918, 40108, 39791, 323, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 358, 1184, 311, 6286, 429, 566, 594, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 13, 2938, 594, 264, 5089, 1538, 1576, 432, 594, 279, 2086, 882, 566, 594, 1012, 6398, 304, 1741, 264, 4545, 11, 892, 1410, 387, 264, 2409, 3484, 382, 40, 1265, 1083, 5185, 429, 279, 35546, 2734, 304, 20079, 815, 594, 4545, 3697, 1283, 806, 11737, 7403, 28396, 13, 2938, 594, 458, 2989, 7716, 1576, 432, 4933, 429, 279, 1142, 4436, 944, 916, 323, 429, 1052, 594, 2058, 264, 2696, 311, 13186, 382, 5847, 11, 279, 1196, 9733, 429, 39791, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 13, 2938, 594, 16587, 1576, 432, 7289, 279, 16232, 429, 279, 1142, 374, 1660, 17608, 14138, 11, 323, 1052, 594, 902, 5904, 315, 64228, 13, 358, 1265, 1281, 2704, 311, 11167, 429, 949, 382, 40, 1265, 1083, 5796, 389, 279, 5777, 24154, 13, 39791, 374, 12880, 10195, 5435, 311, 279, 16375, 11, 323, 806, 13747, 594, 33913, 374, 264, 6785, 1841, 13, 10696, 358, 646, 6286, 429, 279, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 382, 40, 1184, 311, 5944, 279, 4549, 304, 264, 1616, 429, 27455, 1632, 13, 10696, 1191, 448, 458, 16800, 911, 39791, 323, 806, 11737, 7403, 11, 1221, 3271, 389, 311, 279, 5777, 24961, 11, 279, 35546, 2734, 11, 323, 279, 13747, 594, 33913, 13, 17375, 11, 31094, 448, 279, 24154, 315, 279, 1142, 382, 40, 1265, 1083, 2908, 279, 16232, 13, 1084, 1265, 387, 38219, 714, 537, 2238, 10916, 11, 773, 432, 594, 15614, 311, 264, 4586, 10650, 13, 358, 3278, 1184, 311, 1281, 2704, 279, 4128, 374, 2797, 323, 63594, 382, 40, 1265, 1083, 1744, 911, 894, 4650, 5546, 16370, 476, 4755, 429, 2578, 30789, 13, 1752, 3110, 11, 3170, 374, 1052, 35546, 2734, 304, 20079, 815, 594, 4545, 30, 1084, 1410, 387, 1576, 315, 279, 11737, 7403, 594, 51879, 11, 892, 374, 264, 5089, 1538, 13, 10696, 358, 646, 6286, 429, 438, 264, 1459, 315, 2734, 382, 40, 1265, 1083, 5978, 429, 279, 4549, 3171, 944, 1101, 1140, 279, 13064, 714, 1083, 5707, 2266, 323, 6358, 13, 1752, 3110, 11, 24392, 279, 5777, 1882, 11, 279, 25361, 315, 279, 16375, 11, 323, 279, 5421, 389, 279, 3942, 382, 40, 1184, 311, 1281, 2704, 429, 279, 4549, 374, 23831, 11, 31544, 2176, 279, 6785, 24961, 323, 894, 4650, 11513, 13, 10696, 6286, 429, 1393, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 11, 714, 369, 1431, 11, 432, 594, 264, 6785, 6534, 382, 40, 1265, 1083, 1779, 369, 894, 3213, 24961, 476, 8837, 13, 576, 1196, 3207, 944, 3410, 3151, 12713, 11, 773, 358, 3278, 614, 311, 2506, 432, 4586, 11, 44291, 429, 279, 1142, 374, 14195, 323, 429, 279, 13747, 594, 33913, 374, 264, 1376, 1459, 382, 23949, 11, 358, 1265, 31094, 553, 28285, 4849, 279, 1887, 3501, 323, 80903, 279, 12650, 315, 279, 1142, 304, 279, 3942, 13, 10696, 4190, 429, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 382, 40, 1744, 429, 594, 264, 1661, 21101, 13, 4695, 11, 358, 3278, 5944, 279, 4549, 27079, 11, 3259, 2704, 1817, 3772, 27455, 1119, 279, 1790, 323, 14521, 678, 279, 1376, 3501, 279, 1196, 3897, 624, 151649, 271, 334, 35, 4266, 39791, 594, 35074, 42408, 25, 362, 24678, 42580, 323, 48986, 291, 24106, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 3881, 369, 806, 20129, 3267, 11, 702, 5926, 2525, 311, 3100, 438, 279, 3832, 315, 5777, 28307, 15860, 806, 11737, 7403, 11, 64063, 20079, 815, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 5435, 311, 1493, 34179, 23546, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 31544, 264, 6785, 19221, 369, 279, 5777, 3942, 382, 334, 37155, 311, 40108, 39791, 323, 5301, 35074, 42408, 56177, 35, 4266, 39791, 11, 264, 20129, 7071, 11, 702, 1012, 279, 3832, 315, 5777, 35652, 369, 806, 3267, 6168, 11, 2670, 806, 20129, 4545, 315, 806, 1156, 7403, 11, 21475, 20079, 815, 13, 4695, 11, 806, 11737, 7403, 11, 64063, 20079, 815, 11, 702, 2525, 1119, 279, 4568, 88243, 13, 39791, 11, 879, 702, 1012, 12517, 304, 279, 16375, 315, 806, 1156, 323, 11737, 38620, 11, 702, 16601, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 13, 5301, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 38586, 264, 6785, 28931, 304, 279, 5777, 3942, 382, 334, 52786, 7843, 1368, 323, 48986, 291, 24106, 56177, 34819, 1270, 594, 13747, 702, 14820, 894, 64228, 304, 2987, 1142, 11, 80903, 429, 279, 16375, 525, 537, 24203, 23546, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 43714, 504, 279, 2097, 429, 806, 11737, 7403, 28396, 11, 7842, 2441, 6193, 315, 23094, 311, 279, 1142, 13, 1096, 51879, 702, 40444, 20333, 911, 279, 5777, 1882, 323, 279, 4650, 5421, 389, 279, 3942, 382, 334, 785, 11538, 594, 24678, 14390, 10709, 56177, 34819, 1270, 374, 12880, 10195, 315, 9901, 323, 5248, 14579, 315, 46864, 13118, 11, 448, 264, 3204, 6757, 315, 25292, 13, 5301, 13747, 702, 14820, 894, 64228, 11, 31544, 264, 6785, 19221, 13, 576, 1142, 374, 1660, 17608, 448, 50741, 323, 27231, 11, 41752, 279, 5777, 3942, 594, 15155, 311, 52483, 279, 4265, 2041, 894, 5904, 315, 64228, 382, 334, 43434, 323, 14390, 10709, 56177, 7983, 279, 1142, 374, 1660, 17608, 14138, 11, 1052, 1410, 387, 3853, 5777, 11513, 13, 4354, 11, 369, 1431, 11, 279, 5777, 3942, 1265, 1824, 39791, 323, 806, 13747, 304, 11589, 279, 1142, 13, 576, 35546, 2734, 304, 20079, 815, 594, 4545, 21314, 279, 14195, 5777, 8049, 323, 279, 12650, 315, 27020, 279, 83040, 315, 279, 1142, 13, 576, 5777, 3942, 1265, 3060, 311, 1824, 39791, 323, 806, 13747, 304, 52483, 279, 4265, 11, 22573, 12161, 374, 10223, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 32313, 11, 773, 358, 1184, 311, 7071, 700, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643], [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 151644, 198, 48, 25, 20205, 389, 419, 3395, 11, 1035, 279, 1196, 6934, 419, 1985, 30, 2049, 10294, 25, 3197, 358, 1156, 24706, 419, 2311, 432, 6966, 9016, 9058, 323, 27759, 11, 438, 1657, 1251, 614, 2669, 28686, 13, 3017, 14602, 572, 311, 387, 2952, 311, 3960, 34117, 448, 264, 1651, 311, 8660, 279, 65660, 323, 1083, 369, 7298, 990, 13, 758, 419, 5091, 279, 2311, 18404, 279, 4128, 1602, 98632, 398, 323, 73045, 323, 835, 4241, 498, 448, 264, 7474, 16266, 304, 8660, 1181, 5944, 892, 6147, 498, 311, 728, 3123, 323, 4586, 1064, 1602, 6707, 26, 1602, 5390, 369, 4623, 6832, 13, 1096, 11, 358, 1744, 11, 803, 1091, 3643, 705, 369, 264, 803, 993, 66390, 5486, 13, 4636, 678, 11, 386, 7778, 374, 57742, 504, 279, 65660, 6693, 4128, 374, 9016, 6888, 336, 7022, 25777, 13, 1084, 1172, 4977, 1290, 429, 582, 1430, 311, 3535, 279, 4128, 518, 419, 7990, 421, 582, 525, 311, 9428, 15401, 432, 13, 21806, 510, 32, 25, 151645, 151648, 198, 32313, 11, 773, 358, 1184, 311, 7071, 700, 3425, 279, 1196, 1035, 6934, 419, 1985, 3118, 389, 279, 2661, 3395, 13, 6771, 752, 1191, 553, 5290, 279, 3395, 15516, 382, 785, 3395, 2727, 429, 979, 279, 1196, 1156, 24706, 279, 2311, 11, 432, 6966, 9058, 323, 27759, 13, 2938, 594, 264, 2409, 2518, 5181, 13, 8853, 3545, 1477, 6467, 9058, 323, 27759, 11, 5310, 421, 807, 2299, 3330, 311, 3960, 2494, 1075, 34117, 476, 65660, 11, 892, 525, 2176, 6351, 323, 7040, 13, 576, 1196, 594, 14602, 572, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 2055, 11, 279, 2311, 594, 9058, 2090, 4977, 311, 387, 264, 3598, 4265, 382, 785, 55514, 33845, 429, 279, 2311, 18404, 279, 4128, 98632, 398, 323, 73045, 11, 892, 374, 1661, 13, 1084, 6696, 264, 7474, 16266, 323, 3643, 432, 4135, 311, 92540, 13, 2379, 1083, 5185, 429, 432, 594, 803, 1091, 1101, 458, 993, 66390, 5486, 13, 2379, 1744, 429, 8660, 279, 4128, 17247, 374, 2989, 1576, 279, 65660, 374, 6888, 336, 7022, 25777, 13, 2055, 11, 279, 55514, 13605, 429, 279, 7990, 315, 8660, 14239, 973, 369, 279, 9058, 2090, 382, 97904, 419, 3786, 11, 279, 1196, 594, 1887, 5795, 374, 311, 3960, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 576, 2311, 20027, 419, 5795, 553, 1660, 2797, 323, 19819, 11, 714, 432, 594, 537, 1602, 22570, 13, 576, 9058, 2090, 374, 264, 5089, 87520, 13, 7418, 3498, 279, 55514, 15482, 279, 7990, 315, 8660, 3643, 432, 5802, 432, 11, 279, 2856, 20792, 374, 8225, 382, 4416, 11, 12831, 678, 419, 11, 279, 1196, 1035, 4658, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 13, 2379, 2578, 10702, 264, 803, 22570, 1467, 429, 2058, 390, 49269, 279, 5871, 1995, 9355, 624, 151649, 271, 785, 1196, 1035, 537, 6934, 419, 1985, 1576, 279, 2311, 594, 9058, 2090, 323, 6853, 315, 19805, 1281, 432, 2686, 7373, 369, 862, 10602, 7428, 315, 6832, 34117, 369, 8660, 279, 65660, 323, 7298, 990, 13, 5976, 279, 55514, 21314, 279, 2311, 594, 31273, 323, 19819, 5944, 11, 279, 2856, 20792, 315, 1181, 9058, 2090, 374, 264, 5089, 87520, 13, 151643]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from more_itertools import chunked\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "text_token_id = tokenizer.encode(\"<|fim_pad|>\", add_special_tokens=False)[0]\n",
    "eos_token_id = tokenizer.encode(\"<｜end▁of▁sentence｜>\", add_special_tokens=False)[0]\n",
    "\n",
    "window_size = 4\n",
    "\n",
    "batch = 2\n",
    "\n",
    "dataset_batch = [dataset[i] for i in range(batch)]\n",
    "dataset_batch = [tokenize_single_turn(**item) for item in dataset_batch]\n",
    "aligned_batch = []\n",
    "# определяем какие именно элементы мы хотим сжимать,а какие\n",
    "# не помещаются не помещаются в чанк размером window_size\n",
    "for tokens in dataset_batch:\n",
    "    input_ids = np.array(tokens[\"input_ids\"])\n",
    "    content_mask = np.array(tokens[\"content_compression_mask\"])\n",
    "    user_part = input_ids[content_mask == 1]\n",
    "    total_parts = len(user_part) // window_size\n",
    "    new_len_part_1 = total_parts * window_size\n",
    "    mask_end_pos = np.where(content_mask == 1)[0][-1]\n",
    "    # print(content_mask.tolist())\n",
    "    content_mask[\n",
    "        mask_end_pos - (len(user_part) - new_len_part_1) + 1 : mask_end_pos + 1\n",
    "    ] = 0\n",
    "    # print(content_mask.tolist())\n",
    "    # print(user_part.shape, total_parts, new_len_part_1)\n",
    "\n",
    "    answer_part = input_ids[content_mask == 2]\n",
    "    total_parts = len(answer_part) // window_size\n",
    "    new_len_part_2 = total_parts * window_size\n",
    "    mask_end_pos = np.where(content_mask == 2)[0][-1]\n",
    "    # print(content_mask.tolist())\n",
    "    content_mask[\n",
    "        mask_end_pos - (len(answer_part) - new_len_part_2) + 1 : mask_end_pos + 1\n",
    "    ] = 0\n",
    "    # print(content_mask.tolist())\n",
    "    # print(answer_part.shape, total_parts, new_len_part_2)\n",
    "    # content_mask[content_mask == 2] = 1\n",
    "    # print(content_mask.tolist())\n",
    "    aligned_batch.append(\n",
    "        {\n",
    "            \"input_ids\": tokens[\"input_ids\"],\n",
    "            \"content_compression_mask\": content_mask,\n",
    "            \"attention_mask\": tokens[\"attention_mask\"],\n",
    "        }\n",
    "    )\n",
    "    # break\n",
    "\n",
    "train_examples = []\n",
    "train_examples_amount = 2\n",
    "for tokens in aligned_batch:\n",
    "    input_ids = np.array(tokens[\"input_ids\"])\n",
    "    content_mask = np.array(tokens[\"content_compression_mask\"])\n",
    "    for chunks_amount in range(train_examples_amount):\n",
    "        # фикусируемся на сжатии ответа модели, выбираем 2\n",
    "        start_pos = np.where(content_mask == 2)[0][0]\n",
    "        input_ids[start_pos : start_pos + (chunks_amount + 1) * window_size] = (\n",
    "            text_token_id\n",
    "        )\n",
    "        compressed_input_ids = input_ids[:start_pos].tolist()\n",
    "        compressed_input_ids += [text_token_id] * (chunks_amount + 1)\n",
    "        compressed_input_ids += input_ids[\n",
    "            start_pos + (chunks_amount + 1) * window_size :\n",
    "        ].tolist()\n",
    "        # print(text_token_id)\n",
    "        # print(\" \".join(f\"{num:>{8}}\" for num in content_mask.tolist()))\n",
    "        # print(\" \".join(f\"{num:>{8}}\" for num in input_ids.tolist()))\n",
    "        # print(\" \".join(f\"{num:>{8}}\" for num in compressed_input_ids))\n",
    "        # print(\"===\")\n",
    "        train_examples.append(\n",
    "            {\n",
    "                \"replaced_original_tokens\": input_ids.tolist(),\n",
    "                \"compressed_input_ids\": compressed_input_ids,\n",
    "                \"original_tokens\": tokens[\"input_ids\"],\n",
    "            }\n",
    "        )\n",
    "    # break\n",
    "\n",
    "\n",
    "# pad to the same length\n",
    "new_inputs = {}\n",
    "for item in train_examples:\n",
    "    for key, value in item.items():\n",
    "        if not key in new_inputs:\n",
    "            new_inputs[key] = []\n",
    "        new_inputs[key].append(value)\n",
    "\n",
    "\n",
    "for key, value in new_inputs.items():\n",
    "    new_inputs[key] = tokenizer.pad(\n",
    "        {\n",
    "            \"input_ids\": new_inputs[key],\n",
    "        },\n",
    "        padding=True,\n",
    "        # return_tensors=\"pt\",\n",
    "    )\n",
    "new_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151662 151643\n",
      "  151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151643   151646   151644      198       48       25    20205      389      419     3395       11     1035      279     1196     6934      419     1985       30     2049    10294       25     3197      358     1156    24706      419     2311      432     6966     9016     9058      323    27759       11      438     1657     1251      614     2669    28686       13     3017    14602      572      311      387     2952      311     3960    34117      448      264     1651      311     8660      279    65660      323     1083      369     7298      990       13      758      419     5091      279     2311    18404      279     4128     1602    98632      398      323    73045      323      835     4241      498      448      264     7474    16266      304     8660     1181     5944      892     6147      498      311      728     3123      323     4586     1064     1602     6707       26     1602     5390      369     4623     6832       13     1096       11      358     1744       11      803     1091     3643      705      369      264      803      993    66390     5486       13     4636      678       11      386     7778      374    57742      504      279    65660     6693     4128      374     9016     6888      336     7022    25777       13     1084     1172     4977     1290      429      582     1430      311     3535      279     4128      518      419     7990      421      582      525      311     9428    15401      432       13    21806      510       32       25   151645   151648      198   151662   151662   151662   151662     1184      311     7071      700     3425      279     1196     1035     6934      419     1985     3118      389      279     2661     3395       13     6771      752     1191      553     5290      279     3395    15516      382      785     3395     2727      429      979      279     1196     1156    24706      279     2311       11      432     6966     9058      323    27759       13     2938      594      264     2409     2518     5181       13     8853     3545     1477     6467     9058      323    27759       11     5310      421      807     2299     3330      311     3960     2494     1075    34117      476    65660       11      892      525     2176     6351      323     7040       13      576     1196      594    14602      572      311     3960    34117      369     8660      279    65660      323     7298      990       13     2055       11      279     2311      594     9058     2090     4977      311      387      264     3598     4265      382      785    55514    33845      429      279     2311    18404      279     4128    98632      398      323    73045       11      892      374     1661       13     1084     6696      264     7474    16266      323     3643      432     4135      311    92540       13     2379     1083     5185      429      432      594      803     1091     1101      458      993    66390     5486       13     2379     1744      429     8660      279     4128    17247      374     2989     1576      279    65660      374     6888      336     7022    25777       13     2055       11      279    55514    13605      429      279     7990      315     8660    14239      973      369      279     9058     2090      382    97904      419     3786       11      279     1196      594     1887     5795      374      311     3960    34117      369     8660      279    65660      323     7298      990       13      576     2311    20027      419     5795      553     1660     2797      323    19819       11      714      432      594      537     1602    22570       13      576     9058     2090      374      264     5089    87520       13     7418     3498      279    55514    15482      279     7990      315     8660     3643      432     5802      432       11      279     2856    20792      374     8225      382     4416       11    12831      678      419       11      279     1196     1035     4658      537     6934      419     1985     1576      279     2311      594     9058     2090      323     6853      315    19805     1281      432     2686     7373      369      862    10602     7428       13     2379     2578    10702      264      803    22570     1467      429     2058      390    49269      279     5871     1995     9355      624   151649      271      785     1196     1035      537     6934      419     1985     1576      279     2311      594     9058     2090      323     6853      315    19805     1281      432     2686     7373      369      862    10602     7428      315     6832    34117      369     8660      279    65660      323     7298      990       13     5976      279    55514    21314      279     2311      594    31273      323    19819     5944       11      279     2856    20792      315     1181     9058     2090      374      264     5089    87520       13   151643\n",
      "       0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1\n"
     ]
    }
   ],
   "source": [
    "batch_index = 2\n",
    "print(text_token_id, eos_token_id)\n",
    "print(\n",
    "    \" \".join(\n",
    "        f\"{num:>{8}}\"\n",
    "        for num in new_inputs[\"replaced_original_tokens\"][\"input_ids\"][batch_index]\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \" \".join(\n",
    "        f\"{num:>{8}}\"\n",
    "        for num in new_inputs[\"replaced_original_tokens\"][\"attention_mask\"][batch_index]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['replaced_original_tokens', 'compressed_input_ids', 'original_tokens'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1111, 1536]),\n",
       " torch.Size([4, 1114, 1536]),\n",
       " torch.Size([4, 1114, 1536]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tokens_torch = torch.tensor(\n",
    "    new_inputs[\"original_tokens\"][\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "replaced_tokens_torch = torch.tensor(\n",
    "    new_inputs[\"replaced_original_tokens\"][\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "compressed_tokens_torch = torch.tensor(\n",
    "    new_inputs[\"compressed_input_ids\"][\"input_ids\"],\n",
    "    device=\"cuda\",\n",
    ")\n",
    "# torch.Size([2, 51, 896])\n",
    "original_embeds = model.get_input_embeddings()(original_tokens_torch)\n",
    "replaced_embeds = model.get_input_embeddings()(replaced_tokens_torch)\n",
    "# torch.Size([2, 35, 896]) 51 - 3*4*2 + 4*2\n",
    "compressed_embeds_template = model.get_input_embeddings()(compressed_tokens_torch)\n",
    "compressed_embeds_template.shape, original_embeds.shape, replaced_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 1536]), torch.Size([6]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_for_compression_mask = replaced_tokens_torch == text_token_id\n",
    "compressed_tokens_mask = compressed_tokens_torch == text_token_id\n",
    "# original_tokens_torch[tokens_for_compression_mask].shape\n",
    "original_embeds[tokens_for_compression_mask].shape, compressed_tokens_torch[\n",
    "    compressed_tokens_mask\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4, 1536])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds_for_compression = original_embeds[tokens_for_compression_mask].reshape(\n",
    "    -1,\n",
    "    window_size,\n",
    "    original_embeds.shape[-1],\n",
    ")\n",
    "embeds_for_compression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2ModelEmbedPoolerV2(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_embeds):\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = self.model(\n",
    "            inputs_embeds=input_embeds,\n",
    "            output_hidden_states=True,\n",
    "        )[0]\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.sum(1) / torch.tensor(\n",
    "            input_embeds.shape[1],\n",
    "            device=input_embeds.device,\n",
    "        )\n",
    "        # print(input_embeds.dtype)\n",
    "        input_embeds = input_embeds.unsqueeze(1)\n",
    "        return input_embeds\n",
    "\n",
    "\n",
    "embed_pooler_v3 = Qwen2ModelEmbedPoolerV2.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1, 1536]), torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_embeds = embed_pooler_v3(embeds_for_compression)\n",
    "# pooled_embeds = pooled_embeds.reshape(pooled_embeds.shape[0], -1)\n",
    "pooled_embeds.shape, pooled_embeds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_embeds_template = compressed_embeds_template.masked_scatter_(\n",
    "    compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template),\n",
    "    pooled_embeds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1111, 1536])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_tokens_mask.unsqueeze(-1).expand_as(compressed_embeds_template).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем что эмбединги скопировались куда надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_pos = int(torch.where(compressed_tokens_mask[0])[0][0].cpu())\n",
    "(compressed_embeds_template[0][emb_pos] == pooled_embeds[0].view(-1)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = compressed_tokens_torch.clone()\n",
    "labels[labels == text_token_id] = -100\n",
    "labels[labels == eos_token_id] = -100\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5505, device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_1 = model(\n",
    "    inputs_embeds=compressed_embeds_template,\n",
    "    labels=labels,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model_output_1.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "max_steps = 400\n",
    "for _ in range(max_steps):\n",
    "    logits = model(\n",
    "        input_ids=generated_tokens,\n",
    "    ).logits\n",
    "    top_token = logits.argmax(-1)[-1][-1]\n",
    "    generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "    # break\n",
    "# print(tokenizer.decode(generated_tokens[-1]))\n",
    "# break\n",
    "\n",
    "default_generation_tokens = generated_tokens[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeds text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "max_steps = 400\n",
    "for _ in range(max_steps):\n",
    "    logits = model(\n",
    "        inputs_embeds=generated_embeds,\n",
    "    ).logits\n",
    "    top_token = logits.argmax(-1)[-1][-1]\n",
    "    top_token_embed = model.get_input_embeddings()(top_token)\n",
    "    # print(top)\n",
    "    generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "    generated_embeds = torch.cat(\n",
    "        [generated_embeds, top_token_embed.reshape(1, 1, -1)], dim=1\n",
    "    )\n",
    "    # break\n",
    "# print(tokenizer.decode(generated_tokens[-1]))\n",
    "# break\n",
    "embeds_generation_tokens = generated_tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embeds_generation_tokens == default_generation_tokens).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generation with compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>2+2*10<｜Assistant｜><think>\n",
      "First, I need andy is.\n",
      "\n",
      "Next, is is .\n",
      "\n",
      " is  ?\n",
      "\n",
      " is \n",
      "\n",
      " is  ?\n",
      "\n",
      " is  ?\n",
      "\n",
      " is  ?\n",
      "\n",
      " is  ?\n",
      "\n",
      " is  ?\n",
      "\n",
      " is  AND is  AND is  AND is  AND is  AND is  AND is  AND is  AND is  AND is  AND Is  AND Is  AND Is  AND Is  AND Is  AND Is  AND Is  AND Is  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND IS  AND\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "generated_embeds = model.get_input_embeddings()(generated_tokens)\n",
    "max_steps = 200\n",
    "temp_gen_size = 0\n",
    "compression_started = False\n",
    "window_size = 4\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    if temp_gen_size == window_size:\n",
    "        new_embeds_for_compression = generated_embeds[:, -4:]\n",
    "        compressed_part = embed_pooler_v3(generated_embeds[:, -window_size:])\n",
    "        generated_embeds = torch.cat([generated_embeds, compressed_part], dim=1)\n",
    "        temp_gen_size = 1\n",
    "\n",
    "    logits = model(\n",
    "        inputs_embeds=generated_embeds,\n",
    "    ).logits\n",
    "    top_token = logits.argmax(-1)[-1][-1]\n",
    "    top_token_embed = model.get_input_embeddings()(top_token)\n",
    "    # print(top)\n",
    "    generated_tokens = torch.cat([generated_tokens, top_token.reshape(1, 1)], dim=1)\n",
    "    generated_embeds = torch.cat(\n",
    "        [generated_embeds, top_token_embed.reshape(1, 1, -1)], dim=1\n",
    "    )\n",
    "    temp_gen_size += 1\n",
    "\n",
    "print(tokenizer.decode(generated_tokens[-1]))\n",
    "\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_embeds[:, -window_size:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1536])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_pooler_v3(generated_embeds[:, -window_size:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 415, 1536])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 416, 1536])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_part = embed_pooler_v3(generated_embeds[:, -window_size:])\n",
    "print(generated_embeds.shape)\n",
    "generated_embeds = torch.cat([generated_embeds, compressed_part], dim=1)\n",
    "generated_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151646, 151644,     17,     10,     17,      9,     16]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"2+2*10\"},\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "generated_tokens = torch.tensor(generated_tokens).unsqueeze(0).cuda()\n",
    "generated_tokens[:, :-4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
